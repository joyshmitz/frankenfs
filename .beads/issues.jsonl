{"id":"bd-10t","title":"ext4 semantics: Implement inode read (ffs-inode) from disk image","description":"Goal: read an ext4 inode by inode number and return a typed struct with enough info for getattr + extent mapping.\n\nDependencies:\n- ext4 inode location math helper\n- ext4 inode parsing that respects inode_size\n- BlockDevice reads\n\nDeliverables:\n- ffs-inode: read_inode(inode_no) -> Ext4Inode (or higher-level Inode)\n- Handle inode numbers starting at 1; validate bounds.\n- Map ParseError to FfsError with context.\n\nAcceptance:\n- Fixture test reads a known inode and matches expected mode/size/flags.\n- Used by path lookup + getattr in later tasks.","status":"closed","priority":1,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:25:28.329715527Z","created_by":"ubuntu","updated_at":"2026-02-10T19:24:51.847468125Z","closed_at":"2026-02-10T19:24:51.847440593Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","semantics"],"dependencies":[{"issue_id":"bd-10t","depends_on_id":"bd-3nc","type":"blocks","created_at":"2026-02-10T03:27:33.238928160Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10t","depends_on_id":"bd-3qq","type":"blocks","created_at":"2026-02-10T03:27:33.136761424Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10t","depends_on_id":"bd-8tr","type":"blocks","created_at":"2026-02-10T03:27:33.028783440Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-126","title":"Docs: Align parser error taxonomy (ParseError vs FfsError)","description":"Problem: COMPREHENSIVE_SPEC describes some parser failures as returning FfsError::Format/Corruption, but the current design uses ffs-types::ParseError for pure parsing (ffs-ondisk) and maps to FfsError at the orchestration boundary (ffs-core / ffs-ext4 / ffs-btrfs).\n\nGoal: document the layering explicitly:\n- ffs-ondisk is pure and returns ParseError\n- ffs-core converts ParseError to FfsError for user-facing surfaces (CLI/FUSE)\n- checksum verification errors choose a stable representation (ParseError::InvalidField vs a dedicated variant)\n\nAcceptance:\n- Spec sections that mention parser error returns match the actual crate boundaries.\n- No doc requires ffs-ondisk to depend on ffs-error (unless we intentionally change architecture and implement it).","status":"closed","priority":0,"issue_type":"task","assignee":"QuietFalcon","created_at":"2026-02-10T03:13:41.416923883Z","created_by":"ubuntu","updated_at":"2026-02-10T16:46:16.485011064Z","closed_at":"2026-02-10T16:46:16.484988762Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs"],"dependencies":[{"issue_id":"bd-126","depends_on_id":"bd-2ds","type":"blocks","created_at":"2026-02-10T03:14:05.670314853Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":18,"issue_id":"bd-126","author":"QuietFalcon","text":"Updated `COMPREHENSIVE_SPEC_FOR_FRANKENFS_V1.md` to make the layering explicit: `ffs-ondisk` checksum/parse failures return `ParseError`, and `ffs-core` converts `ParseError -> FfsError` for mount/CLI/FUSE. Fixed CRC32C seed wording (csum_seed vs superblock seed), replaced the incorrect code snippet to use `ParseError` + `crc32c_append`, and removed the non-existent `ChecksumMismatch` mention in Phase 2 acceptance criteria. Also updated Errata to track only the remaining contextual mapping gap (bd-2fy).","created_at":"2026-02-10T16:46:13Z"}]}
{"id":"bd-12x","title":"Repair: Implement RaptorQ encode/decode workflow + decode proof","description":"Goal: given a set of missing/corrupted blocks in a block group, reconstruct them using repair symbols.\n\nDeliverables:\n- Use asupersync RaptorQ codec to generate symbols and attempt decode.\n- Produce a DecodeProof artifact: which symbols used, success/failure, residual error.\n- Integrate with MVCC commit seq: repair applies to a specific snapshot/version.\n\nAcceptance:\n- Fault injection fixture: corrupt N blocks, decode succeeds when redundancy is sufficient.\n- Decode failures are explainable (proof shows why insufficient).","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:24:37.815069255Z","created_by":"ubuntu","updated_at":"2026-02-11T02:23:23.278642774Z","closed_at":"2026-02-11T02:23:23.278559839Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["repair"],"dependencies":[{"issue_id":"bd-12x","depends_on_id":"bd-16f","type":"blocks","created_at":"2026-02-10T03:25:00.679065433Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12x","depends_on_id":"bd-1u7","type":"blocks","created_at":"2026-02-10T03:25:00.834803013Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-14w","title":"ffs-types: Add missing canonical newtypes (BlockSize, ByteOffset, GroupNumber, DeviceId, Generation, ...)","description":"Goal: make ffs-types the single canonical home for cross-crate identifiers and basic arithmetic invariants.\n\nWhy: without strong newtypes we will silently mix units (bytes vs blocks, logical vs physical, ext4 group vs btrfs objectid) and ship corruption bugs.\n\nDeliverables (minimum set):\n- BlockSize(u32): must be power-of-two; ext4 v1 supports 1024/2048/4096 only.\n- ByteOffset(u64): byte offsets on ByteDevice; checked add/sub/mul helpers.\n- GroupNumber(u32): ext4 block group index.\n- DeviceId(u64) or u128: stable id for multi-device future; for now single-device only.\n- Generation(u64): btrfs generation / ext4 generation boundary type.\n- LogicalBlock(BlockNumber) vs PhysicalBlock(BlockNumber) wrappers if we decide they are distinct.\n\nAcceptance:\n- All added newtypes have documented invariants + constructors that enforce them.\n- Existing code in ffs-ondisk/ffs-block/ffs-core is updated to use these types where it improves correctness.","status":"closed","priority":0,"issue_type":"task","assignee":"codex","created_at":"2026-02-10T03:14:19.015044302Z","created_by":"ubuntu","updated_at":"2026-02-10T07:07:56.412273652Z","closed_at":"2026-02-10T07:07:56.412252261Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["foundation"]}
{"id":"bd-15c","title":"EPIC: Self-Healing Durability (RaptorQ)","description":"# EPIC: Self-Healing Durability (RaptorQ)\n\n## PURPOSE\nImplement FrankenFS's second key innovation: automatic corruption recovery via fountain-coded repair symbols. Currently at 30% (3/10).\n\n## BACKGROUND\nFrom README: \"RaptorQ (RFC 6330) is a fountain code ‚Äî an erasure coding scheme that generates repair symbols from source data. Given enough symbols, you can recover any lost/corrupted source blocks.\"\n\nTraditional filesystems detect corruption (checksums) but require manual fsck to recover. FrankenFS stores repair symbols alongside data, enabling automatic recovery.\n\n## CURRENT STATE\nFrom FEATURE_PARITY.md:\n- durability policy model: ‚úÖ (Bayesian expected-loss selector)\n- asupersync config mapping: ‚úÖ (`RaptorQConfig` mapping)\n- format-aware scrub superblock validation: ‚úÖ\n\nWhat‚Äôs missing:\n- actual symbol generation\n- symbol storage + integrity\n- recovery orchestration\n- E2E corruption injection and proof\n\n## GAPS TO CLOSE\n1. Integrate asupersync RaptorQ codec\n2. Define and implement repair symbol storage format\n3. Generation pipeline (on write / on refresh)\n4. Recovery pipeline (detect -> retrieve -> decode -> repair -> verify)\n5. Background scrub integration (later)\n\n## ALIEN-ARTIFACT QUALITY BAR\nSelf-healing policy is explicitly called out in AGENTS.md:\n- Bayesian evidence updates for corruption/failure rates\n- Expected-loss decision rules for redundancy policies\n- Anytime-valid monitoring for long-running checks\n- Deterministic audit logs / evidence ledger\n\n## ACCEPTANCE CRITERIA\n1. RaptorQ symbols generated for each block group\n2. Configurable overhead (default 5%)\n3. Corruption detected -> automatic recovery attempted\n4. Recovery success/failure logged with evidence\n5. Self-healing parity reaches 70%+ (7/10)\n\n## DEPENDENCIES\n- EPIC: MVCC Core (bd-22w) - write orchestration and snapshot semantics\n- EPIC: Conformance & Quality Infrastructure (bd-2jk) - fixtures + E2E harness\n\n## RELATED SPEC SECTIONS\n- COMPREHENSIVE_SPEC_FOR_FRANKENFS_V1.md ¬ß4 (Self-Healing)\n- RFC 6330 (RaptorQ specification)\n- asupersync RaptorQ module documentation\n\n## Success Criteria\n1. A deterministic E2E script can inject bounded corruption and demonstrate recovery with an evidence ledger (`scripts/e2e/ffs_repair_recovery_smoke.sh`).\n2. Unit/integration tests cover encode/decode and symbol-store durability + crash safety.\n3. Recovery failures are graceful and explainable (clear error + evidence) rather than silent.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-12T15:01:00.315447377Z","created_by":"ubuntu","updated_at":"2026-02-12T20:58:54.389929949Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-15c","depends_on_id":"bd-22w","type":"blocks","created_at":"2026-02-12T15:04:29.249453396Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-15c","depends_on_id":"bd-2jk","type":"blocks","created_at":"2026-02-12T20:53:17.294573238Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-15c.1","title":"Integrate asupersync RaptorQ codec into ffs-repair","description":"# Integrate asupersync RaptorQ codec into ffs-repair\n\n## GOAL\nWire asupersync's RaptorQ implementation into ffs-repair for symbol generation and decoding.\n\n## CONTEXT\nasupersync (at /dp/asupersync) provides a RaptorQ codec following RFC 6330. ffs-repair has the durability policy model but lacks actual encoding/decoding.\n\n## IMPLEMENTATION APPROACH\n\n### Step 1: Understand asupersync RaptorQ API\n```rust\n// From asupersync (expected API shape)\npub struct RaptorQEncoder {\n    pub fn new(config: RaptorQConfig) -> Self;\n    pub fn encode(&self, source_blocks: &[&[u8]]) -> Vec<EncodedSymbol>;\n}\n\npub struct RaptorQDecoder {\n    pub fn new(config: RaptorQConfig) -> Self;\n    pub fn decode(&self, symbols: &[EncodedSymbol]) -> Result<Vec<Vec<u8>>>;\n}\n```\n\n### Step 2: Create ffs-repair encoding wrapper\n```rust\n// In ffs-repair/src/raptorq.rs\npub struct BlockGroupEncoder {\n    config: RaptorQConfig,\n    overhead_percent: f32,  // Default 5%\n}\n\nimpl BlockGroupEncoder {\n    pub fn encode_block_group(\n        &self,\n        cx: &Cx,\n        blocks: &[BlockBuf],\n    ) -> Result<RepairSymbols> {\n        let encoder = asupersync::RaptorQEncoder::new(self.config);\n        let source_refs: Vec<&[u8]> = blocks.iter().map(|b| b.as_ref()).collect();\n        let symbols = encoder.encode(&source_refs);\n        \n        Ok(RepairSymbols {\n            group_id: ...,\n            source_block_count: blocks.len() as u32,\n            symbols,\n        })\n    }\n}\n```\n\n### Step 3: Create ffs-repair decoding wrapper\n```rust\npub struct BlockGroupDecoder {\n    config: RaptorQConfig,\n}\n\nimpl BlockGroupDecoder {\n    pub fn recover_blocks(\n        &self,\n        cx: &Cx,\n        available_blocks: &[(BlockNumber, Option<BlockBuf>)],  // Some are corrupted (None)\n        repair_symbols: &RepairSymbols,\n    ) -> Result<Vec<(BlockNumber, BlockBuf)>> {\n        // Feed available blocks + symbols to decoder\n        // Return recovered blocks\n    }\n}\n```\n\n## FILES TO MODIFY\n- crates/ffs-repair/Cargo.toml (add asupersync dependency)\n- crates/ffs-repair/src/raptorq.rs (new)\n- crates/ffs-repair/src/lib.rs (export)\n\n## ACCEPTANCE CRITERIA\n1. Can encode a block group into repair symbols\n2. Can decode/recover corrupted blocks given enough symbols\n3. Overhead matches configured percentage\n4. Integration test with simulated corruption\n5. Performance: encoding < 100ms per block group\n\n## TESTING\n```rust\n#[test]\nfn test_encode_decode_roundtrip() {\n    let blocks: Vec<BlockBuf> = (0..128).map(|_| random_block()).collect();\n    let encoder = BlockGroupEncoder::new(RaptorQConfig::default());\n    let symbols = encoder.encode_block_group(&cx, &blocks)?;\n    \n    // Simulate corruption: lose 5% of blocks\n    let mut available: Vec<(_, Option<_>)> = blocks.iter()\n        .enumerate()\n        .map(|(i, b)| (BlockNumber(i as u64), Some(b.clone())))\n        .collect();\n    for i in (0..available.len()).step_by(20) {\n        available[i].1 = None;  // Corrupt every 20th\n    }\n    \n    let decoder = BlockGroupDecoder::new(RaptorQConfig::default());\n    let recovered = decoder.recover_blocks(&cx, &available, &symbols)?;\n    \n    // Verify all blocks recovered\n    for (bn, buf) in recovered {\n        assert_eq!(buf.as_ref(), blocks[bn.0 as usize].as_ref());\n    }\n}\n```\n\n## DEPENDENCIES\n- asupersync crate must be available at /dp/asupersync","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-12T15:01:17.711178940Z","created_by":"ubuntu","updated_at":"2026-02-12T15:01:17.711178940Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-15c.1","depends_on_id":"bd-15c","type":"parent-child","created_at":"2026-02-12T15:01:17.711178940Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-15c.2","title":"Design and implement repair symbol storage format","description":"# Design and implement repair symbol storage format\n\n## GOAL\nDefine where and how repair symbols are stored alongside filesystem data.\n\n## CONTEXT\nRaptorQ symbols need persistent storage. Options:\n1. Inline within block groups (reserved blocks)\n2. Separate sidecar file\n3. Dedicated repair region at end of image\n\n## DESIGN CONSIDERATIONS\n\n### Space overhead\n- 5% overhead means 5 repair blocks per 100 data blocks\n- For 4KB blocks, 1GB image = 12,800 repair blocks = 50MB\n\n### Access patterns\n- Symbols needed only on corruption detection\n- Symbols updated on every write to block group\n- Read: random access by block group\n- Write: sequential append (with periodic compaction)\n\n## RECOMMENDED: Sidecar file approach\n\n### Format: .ffs-repair file\n```\nHeader (4KB):\n  magic: \"FFSREPAIR\"\n  version: u32\n  block_size: u32\n  groups_count: u32\n  overhead_percent: f32\n  checksum: crc32c\n\nPer-group index (32 bytes each):\n  group_id: u32\n  symbol_count: u32\n  offset: u64\n  length: u64\n  generation: u64\n  checksum: crc32c\n\nSymbol data:\n  [raw symbol bytes, concatenated]\n```\n\n### API\n```rust\npub struct RepairStore {\n    file: File,\n    index: BTreeMap<GroupNumber, SymbolLocation>,\n}\n\nimpl RepairStore {\n    pub fn open_or_create(image_path: &Path) -> Result<Self>;\n    pub fn write_symbols(&mut self, group: GroupNumber, symbols: &[EncodedSymbol]) -> Result<()>;\n    pub fn read_symbols(&self, group: GroupNumber) -> Result<Vec<EncodedSymbol>>;\n    pub fn sync(&self) -> Result<()>;\n}\n```\n\n### File naming\n- Image: `/path/to/filesystem.img`\n- Repair: `/path/to/filesystem.img.ffs-repair`\n\n## FILES TO CREATE\n- crates/ffs-repair/src/store.rs\n\n## ACCEPTANCE CRITERIA\n1. Repair file created alongside image on first write\n2. Symbols persisted and retrievable by group\n3. Checksum validation on read\n4. Atomic updates (don't corrupt on crash)\n5. Space overhead matches configured percentage\n\n## TESTING\n```rust\n#[test]\nfn test_repair_store_persistence() {\n    let store = RepairStore::open_or_create(tmp_path)?;\n    store.write_symbols(GroupNumber(0), &symbols)?;\n    drop(store);\n    \n    let store2 = RepairStore::open_or_create(tmp_path)?;\n    let read = store2.read_symbols(GroupNumber(0))?;\n    assert_eq!(read, symbols);\n}\n```\n\n## DEPENDENCIES\n- RaptorQ integration (bd-15c.1)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-12T15:01:32.860960386Z","created_by":"ubuntu","updated_at":"2026-02-12T15:01:57.631402887Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-15c.2","depends_on_id":"bd-15c","type":"parent-child","created_at":"2026-02-12T15:01:32.860960386Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-15c.2","depends_on_id":"bd-15c.1","type":"blocks","created_at":"2026-02-12T15:01:57.631365276Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-15c.3","title":"Implement corruption recovery pipeline","description":"# Implement corruption recovery pipeline\n\n## GOAL\nWire together corruption detection, symbol retrieval, decoding, and block repair into an end-to-end recovery flow.\n\n## CONTEXT\nIndividual pieces exist:\n- Checksum verification in ffs-block\n- RaptorQ codec (bd-15c.1)\n- Symbol storage (bd-15c.2)\n\nNeed to orchestrate: detect ‚Üí retrieve ‚Üí decode ‚Üí repair ‚Üí verify\n\n## IMPLEMENTATION\n\n### Recovery flow\n```rust\npub struct RecoveryOrchestrator {\n    device: Arc<dyn BlockDevice>,\n    repair_store: RepairStore,\n    decoder: BlockGroupDecoder,\n}\n\nimpl RecoveryOrchestrator {\n    pub fn attempt_recovery(\n        &self,\n        cx: &Cx,\n        corrupted_block: BlockNumber,\n    ) -> Result<RecoveryOutcome> {\n        // 1. Determine which block group\n        let group = self.block_to_group(corrupted_block);\n        \n        // 2. Read all blocks in group, noting which are corrupted\n        let mut available = Vec::new();\n        for bn in self.blocks_in_group(group) {\n            match self.device.read_block(cx, bn) {\n                Ok(buf) if self.verify_checksum(&buf) => {\n                    available.push((bn, Some(buf)));\n                }\n                _ => {\n                    available.push((bn, None));  // Corrupted or unreadable\n                }\n            }\n        }\n        \n        // 3. Load repair symbols\n        let symbols = self.repair_store.read_symbols(group)?;\n        \n        // 4. Attempt decode\n        let recovered = self.decoder.recover_blocks(cx, &available, &symbols)?;\n        \n        // 5. Write recovered blocks back\n        for (bn, buf) in &recovered {\n            self.device.write_block(cx, *bn, buf.as_ref())?;\n        }\n        \n        // 6. Return outcome\n        Ok(RecoveryOutcome {\n            group,\n            corrupted_count: available.iter().filter(|(_, b)| b.is_none()).count(),\n            recovered_count: recovered.len(),\n            success: true,\n        })\n    }\n}\n```\n\n### Integration with read path\n```rust\n// In ffs-block or ffs-core\npub fn read_block_with_recovery(\n    cx: &Cx,\n    device: &dyn BlockDevice,\n    bn: BlockNumber,\n    recovery: &RecoveryOrchestrator,\n) -> Result<BlockBuf> {\n    match device.read_block(cx, bn) {\n        Ok(buf) if verify_checksum(&buf) => Ok(buf),\n        Ok(buf) => {\n            // Checksum mismatch - attempt recovery\n            tracing::warn!(block = %bn, \"Checksum mismatch, attempting recovery\");\n            recovery.attempt_recovery(cx, bn)?;\n            // Retry read\n            device.read_block(cx, bn)\n        }\n        Err(e) => {\n            // Read error - attempt recovery\n            tracing::warn!(block = %bn, error = %e, \"Read error, attempting recovery\");\n            recovery.attempt_recovery(cx, bn)?;\n            device.read_block(cx, bn)\n        }\n    }\n}\n```\n\n## ALIEN-ARTIFACT EVIDENCE LEDGER\n\nPer AGENTS.md, recovery decisions need audit trail:\n```rust\npub struct RecoveryEvidence {\n    pub timestamp: DateTime<Utc>,\n    pub block: BlockNumber,\n    pub group: GroupNumber,\n    pub corrupted_count: u32,\n    pub available_symbols: u32,\n    pub required_symbols: u32,  // For this level of corruption\n    pub outcome: RecoveryResult,\n    pub confidence: f64,  // Bayesian posterior on successful recovery\n}\n```\n\n## FILES TO CREATE/MODIFY\n- crates/ffs-repair/src/recovery.rs (new)\n- crates/ffs-core/src/block_io.rs (integrate recovery)\n\n## ACCEPTANCE CRITERIA\n1. Corruption detected via checksum triggers recovery\n2. Recovery succeeds when sufficient symbols available\n3. Recovery fails gracefully when too many blocks corrupted\n4. Evidence ledger logged for each recovery attempt\n5. Recovered blocks verified via checksum before write\n\n## TESTING\n```rust\n#[test]\nfn test_end_to_end_recovery() {\n    // Create image with known content\n    // Generate repair symbols\n    // Corrupt 3% of blocks\n    // Attempt read - should recover automatically\n    // Verify read returns original data\n}\n```\n\n## DEPENDENCIES\n- RaptorQ integration (bd-15c.1)\n- Symbol storage (bd-15c.2)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-12T15:01:52.376968324Z","created_by":"ubuntu","updated_at":"2026-02-12T15:01:57.813441076Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-15c.3","depends_on_id":"bd-15c","type":"parent-child","created_at":"2026-02-12T15:01:52.376968324Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-15c.3","depends_on_id":"bd-15c.1","type":"blocks","created_at":"2026-02-12T15:01:57.721519356Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-15c.3","depends_on_id":"bd-15c.2","type":"blocks","created_at":"2026-02-12T15:01:57.813402514Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-15c.4","title":"Add E2E corruption-injection + recovery verification scripts","description":"# Add E2E corruption-injection + recovery verification scripts\n\n## GOAL\nAdd an end-to-end script that:\n1. Produces (or copies) a working ext4 fixture image\n2. Generates/stores repair symbols\n3. Injects controlled corruption into a bounded set of blocks\n4. Runs a read/scrub path that triggers recovery\n5. Verifies the recovered bytes match the pre-corruption baseline\n6. Captures an evidence ledger + detailed logs\n\n## CONTEXT\nUnit tests can validate encode/decode and parsing, but recovery is an end-to-end behavior across:\n- block I/O\n- checksum validation\n- symbol store\n- RaptorQ decode\n- repair writeback\n\nThis bead makes recovery auditable and reproducible.\n\n## DELIVERABLES\n1. `scripts/e2e/ffs_repair_recovery_smoke.sh`\n2. Uses `scripts/e2e/lib.sh` logging conventions.\n3. Produces artifacts under `artifacts/e2e/<timestamp>/repair/`:\n   - `before_checksums.txt`\n   - `after_checksums.txt`\n   - `corruption_plan.json`\n   - `recovery_evidence.jsonl`\n\n## CORRUPTION INJECTION RULES\n- Inject corruption deterministically using a fixed seed.\n- Corrupt a small percentage (for example 1-3%) of blocks in a single group.\n- Record exactly which blocks were modified and how.\n- Corruption method must be reversible/controlled (bit flips, zeroing, truncation is not acceptable unless explicitly tested).\n\n## VERIFICATION\n- Before corruption, compute `sha256sum` of a bounded set of files read through FUSE (or read via CLI read-path if available).\n- After recovery, recompute and verify identical.\n- Additionally verify block-level checksums where available.\n\n## ACCEPTANCE CRITERIA\n1. Script is deterministic and idempotent.\n2. Logs include enough detail to reproduce a failure.\n3. Evidence ledger exists and is easy to parse (JSONL).\n4. Script skips gracefully (exit 0) if repair pipeline is not available yet, but prints a clear reason.\n\n## NOTES\n- This bead does not decide the recovery policy; it validates the implementation of bd-15c.3.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-12T20:57:01.928728898Z","created_by":"ubuntu","updated_at":"2026-02-12T20:57:01.928728898Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-15c.4","depends_on_id":"bd-15c","type":"parent-child","created_at":"2026-02-12T20:57:01.928728898Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-15c.4","depends_on_id":"bd-15c.3","type":"blocks","created_at":"2026-02-12T20:57:01.928728898Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-15c.4","depends_on_id":"bd-2jk.1","type":"blocks","created_at":"2026-02-12T20:57:01.928728898Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-15c.4","depends_on_id":"bd-2jk.6","type":"blocks","created_at":"2026-02-12T20:57:01.928728898Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-16f","title":"Repair: Define repair symbol format + storage strategy (per block group)","description":"Goal: decide where repair symbols live and how they are versioned.\n\nDeliverables:\n- Symbol format: header (block id, commit seq, checksum), payload.\n- Storage strategy options:\n  - reserved metadata area (native-mode)\n  - sidecar file/log\n  - embedded in unused ext4 structures (only if clean-room + safe)\n- Versioning: how symbols relate to MVCC commit sequences.\n\nAcceptance:\n- Strategy is compatible with \"drop-in mount ext4 image\" goals (no silent mutation unless explicitly in FrankenFS-native mode).\n- Recovery after crash is defined.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:24:23.352280114Z","created_by":"ubuntu","updated_at":"2026-02-11T01:47:28.750721304Z","closed_at":"2026-02-11T01:47:28.750624723Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["repair"],"dependencies":[{"issue_id":"bd-16f","depends_on_id":"bd-2qf","type":"blocks","created_at":"2026-02-10T03:25:00.601672595Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-16k","title":"Epic: ext4 Read-Only MVP (lookup/readdir/read)","description":"Deliver a minimal ext4 read-only stack sufficient for harness and FUSE read-only mount.\n\nAcceptance:\n- Can read root inode (2), list root directory, resolve a simple path, and read file bytes.\n- All behavior covered by fixtures/harness tests.","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-02-10T03:26:51.538465756Z","created_by":"ubuntu","updated_at":"2026-02-10T19:45:26.512201142Z","closed_at":"2026-02-10T19:45:26.512183559Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","semantics"],"dependencies":[{"issue_id":"bd-16k","depends_on_id":"bd-10t","type":"blocks","created_at":"2026-02-10T03:27:18.951838832Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16k","depends_on_id":"bd-1q6","type":"blocks","created_at":"2026-02-10T03:27:19.361469954Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16k","depends_on_id":"bd-2bu","type":"blocks","created_at":"2026-02-10T03:27:19.198878251Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16k","depends_on_id":"bd-2hm","type":"blocks","created_at":"2026-02-10T03:27:19.118770686Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16k","depends_on_id":"bd-2q7","type":"blocks","created_at":"2026-02-10T03:27:19.280819822Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16k","depends_on_id":"bd-2tq","type":"blocks","created_at":"2026-02-10T03:27:18.867757542Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16k","depends_on_id":"bd-ye4","type":"blocks","created_at":"2026-02-10T03:27:19.034749168Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-19k","title":"MVCC: Implement MVCC-aware block device wrapper (read snapshot -> version or base)","description":"Goal: make block reads/writes snapshot-aware.\n\nDeliverables:\n- Define MVCCBlockDevice (or similar) that wraps an underlying BlockDevice plus a version store.\n- Read path: read_visible(block, snapshot) else fall back to base device read_block().\n- Write path: stage_write(block, bytes) into the current transaction.\n- Ensure block buffers are block_size aligned and validated.\n\nAcceptance:\n- Unit tests demonstrate snapshot visibility with base+versioned reads.\n- No global locks on read path; concurrency model documented.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:22:33.557055409Z","created_by":"ubuntu","updated_at":"2026-02-10T21:13:45.798997520Z","closed_at":"2026-02-10T21:13:45.798977844Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["io","mvcc"],"dependencies":[{"issue_id":"bd-19k","depends_on_id":"bd-14w","type":"blocks","created_at":"2026-02-10T03:23:53.105838366Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-19k","depends_on_id":"bd-2fa","type":"blocks","created_at":"2026-02-10T03:23:53.024630399Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1a9","title":"ext4: Implement feature flag decoding + validation policy (compat/ro_compat/incompat)","description":"Goal: represent ext4 feature flags in a way that is (1) easy to validate, (2) easy to print for UX, and (3) safe against unknown bits.\n\nDeliverables:\n- Define bitflags (or equivalent) for compat/ro_compat/incompat.\n- Implement decode helpers that produce:\n  - required bits missing?\n  - explicitly rejected bits present?\n  - unknown incompatible bits present?\n- v1 mount policy:\n  - require FILETYPE + EXTENTS\n  - block sizes limited to 1K/2K/4K\n  - reject compression/encrypt/casefold/inline-data, etc.\n\nAcceptance:\n- validate_v1() returns ParseError with stable field+reason.\n- Tests cover: required bits missing, unknown incompat bit, known rejected bit, allowed-but-nonrequired bits.","status":"closed","priority":0,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:16:56.882710351Z","created_by":"ubuntu","updated_at":"2026-02-10T17:10:51.691798163Z","closed_at":"2026-02-10T17:10:51.691780019Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","ondisk"],"dependencies":[{"issue_id":"bd-1a9","depends_on_id":"bd-2cn","type":"blocks","created_at":"2026-02-10T03:18:06.539865706Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1cc","title":"FUSE: Define MVCC snapshot/transaction policy for FUSE operations","description":"Goal: decide how MVCC integrates with the kernel request model.\n\nQuestions:\n- Does each FUSE op run at a single snapshot (read-only), or do we start a transaction per open()/release()?\n- How do we handle readdir+lookup consistency?\n\nDeliverables:\n- A documented policy in COMPREHENSIVE_SPEC.\n- Implementation hooks in ffs-fuse to acquire snapshot/tx per request.\n\nAcceptance:\n- Concurrency tests show consistent reads under concurrent writers (once writes exist).\n- Policy is simple enough to reason about and test.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:28:39.467984753Z","created_by":"ubuntu","updated_at":"2026-02-11T02:50:37.112290445Z","closed_at":"2026-02-11T02:50:37.112268945Z","close_reason":"done: documented Phase 7A policy + implemented FsOps request-scope hooks in ffs-fuse with tests","source_repo":".","compaction_level":0,"original_size":0,"labels":["fuse","mvcc"],"dependencies":[{"issue_id":"bd-1cc","depends_on_id":"bd-2t1","type":"blocks","created_at":"2026-02-10T03:28:52.176803421Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1cq","title":"Track: btrfs On-Disk Parsing + Mapping (ffs-ondisk)","description":"Implement btrfs superblock parsing, sys_chunk_array bootstrap mapping, single-device logical->physical mapping, and basic B-tree node/item parsing for read-only discovery.\\n\\nAcceptance: can open a btrfs image, validate superblock, map a logical address via sys_chunk, and iterate root tree items without panics; all guarded by fixtures.","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-02-10T03:08:41.112572499Z","created_by":"ubuntu","updated_at":"2026-02-10T20:27:28.404663163Z","closed_at":"2026-02-10T20:27:28.404641172Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1cq","depends_on_id":"bd-1fo","type":"blocks","created_at":"2026-02-10T03:19:03.414637386Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1cq","depends_on_id":"bd-2vt","type":"blocks","created_at":"2026-02-10T03:19:03.259271763Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1cq","depends_on_id":"bd-kdk","type":"blocks","created_at":"2026-02-10T03:19:03.337640505Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1cq","depends_on_id":"bd-o76","type":"blocks","created_at":"2026-02-10T03:19:03.495838876Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1cq","depends_on_id":"bd-z7n","type":"blocks","created_at":"2026-02-10T03:19:16.963240254Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":6,"issue_id":"bd-1cq","author":"Dicklesworthstone","text":"btrfs bootstrap is different from ext4: you must parse sys_chunk_array to map logical->physical before you can even read the trees.\n\nEarly wins:\n- parse superblock + sys_chunk\n- map root/chunk_root\n- parse nodes and walk root tree (read-only)\n\nMulti-device and RAID profiles remain explicitly excluded until read-only parity is solid.","created_at":"2026-02-10T03:34:38Z"}]}
{"id":"bd-1ds","title":"ffs-types: Finalize InodeNumber strategy (ext4 u32 vs btrfs u64)","description":"Context: ext4 stores inode numbers as u32; btrfs uses u64 objectids. The code currently standardizes on InodeNumber(u64) in ffs-types.\n\nGoal: make the representation choice explicit and safe:\n- keep InodeNumber(u64) canonical, but add per-format wrappers (Ext4InodeNumber(u32), BtrfsObjectId(u64)) OR\n- switch to an enum/typed wrapper that prevents mixing domains.\n\nDeliverables:\n- Chosen representation documented in ffs-types docs + COMPREHENSIVE_SPEC.\n- Conversion functions at parsing boundaries.\n- Tests proving no silent truncation or sign/width bugs.\n\nAcceptance:\n- No crate invents its own inode/objectid types.\n- Public APIs do not require callers to guess whether a u64 is ext4 or btrfs.","status":"closed","priority":1,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:14:31.298330901Z","created_by":"ubuntu","updated_at":"2026-02-10T16:00:54.151204622Z","closed_at":"2026-02-10T16:00:54.151178934Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["foundation"]}
{"id":"bd-1fo","title":"btrfs: Implement node parse helpers (internal + leaf, item table bounds)","description":"Goal: provide safe parsing primitives for btrfs tree nodes.\n\nDeliverables:\n- Parse node header (already exists) and validate bytenr, nritems, level.\n- Parse item table for:\n  - leaf nodes (items point to payload within block)\n  - internal nodes (items point to child block pointers)\n- Provide iterators that yield keys + (offset,size) or key + child pointer.\n\nAcceptance:\n- Unit tests cover malformed item tables (overlaps, out-of-bounds offsets, overflow).\n- Works on at least one real btrfs node fixture.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:18:52.067406279Z","created_by":"ubuntu","updated_at":"2026-02-10T20:11:54.582074584Z","closed_at":"2026-02-10T20:11:54.582054607Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["btrfs","ondisk"]}
{"id":"bd-1q6","title":"ext4 semantics: Implement FsOps adapter over ext4 context (ffs-core/ffs-ext4)","description":"Goal: provide an implementation of the internal FsOps trait for ext4, backed by parsing + block I/O.\n\nDeliverables:\n- Ext4Fs struct: holds Ext4Context + BlockDevice + caches.\n- Implement getattr/lookup/readdir/read by calling ffs-inode/ffs-dir/ffs-extent helpers.\n\nAcceptance:\n- Harness can instantiate Ext4Fs and run end-to-end tests (no FUSE required).\n- FUSE layer can mount by delegating to this implementation.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:26:22.710623047Z","created_by":"ubuntu","updated_at":"2026-02-10T19:45:04.823712447Z","closed_at":"2026-02-10T19:45:04.823690245Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","semantics"],"dependencies":[{"issue_id":"bd-1q6","depends_on_id":"bd-2bu","type":"blocks","created_at":"2026-02-10T03:27:34.440914590Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1q6","depends_on_id":"bd-2q7","type":"blocks","created_at":"2026-02-10T03:27:34.553154230Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1q6","depends_on_id":"bd-2tq","type":"blocks","created_at":"2026-02-10T03:27:34.325211421Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1q6","depends_on_id":"bd-3nc","type":"blocks","created_at":"2026-02-10T03:27:34.663416995Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1qu","title":"Harness: add btrfs conformance fixtures for tree-walk and sys_chunk mapping","description":"Context: btrfs parsing (superblock, sys_chunk_array, logical-to-physical mapping, tree-walk with cycle detection) is now implemented but lacks dedicated conformance fixtures.\n\nScope:\n- Add sparse fixtures for btrfs superblock parsing verification\n- Add fixtures for sys_chunk_array parsing and map_logical_to_physical\n- Add fixtures for tree-walk entry enumeration (leaf items from a known tree)\n- Wire fixtures into ffs-harness conformance tests\n\nAcceptance:\n- At least one btrfs superblock fixture with known-good field values\n- At least one sys_chunk mapping fixture that exercises map_logical_to_physical\n- Harness tests that parse fixtures and assert key fields\n- FEATURE_PARITY.md parity counts updated if coverage increases","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-12T08:24:24.094234427Z","created_by":"ubuntu","updated_at":"2026-02-12T08:25:27.729953099Z","closed_at":"2026-02-12T08:25:27.729887296Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1sb","title":"btrfs: Implement checksum verification hooks (superblock + nodes) [phased]","description":"Future task: verify btrfs checksums according to csum_type.\n\nDeliverables:\n- Implement superblock checksum verification.\n- Implement node/header checksum verification.\n- Support at least CRC32C first; extend as needed.\n\nAcceptance:\n- Flipped-bit fixture fails verification and is reported by scrub.","status":"closed","priority":3,"issue_type":"task","owner":"PinkCreek","created_at":"2026-02-10T03:32:08.779052310Z","created_by":"ubuntu","updated_at":"2026-02-11T03:31:49.779818610Z","closed_at":"2026-02-11T03:31:49.779797540Z","close_reason":"Implemented verify_superblock_checksum + verify_tree_block_checksum for btrfs CRC32C. Added BTRFS_CSUM_TYPE constants. 5 tests (valid/corrupt/unsupported).","source_repo":".","compaction_level":0,"original_size":0,"labels":["btrfs","ondisk"]}
{"id":"bd-1su","title":"Add ArcCache opt-in write-back policy with flush-on-evict + sync","description":"Introduce ArcCache write policy enum, keep default write-through, add opt-in write-back behavior that defers direct writes until sync while preserving dirty-eviction flushing, and add tests.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-12T08:45:42.311433356Z","created_by":"ubuntu","updated_at":"2026-02-12T08:46:48.018696770Z","closed_at":"2026-02-12T08:46:48.018678636Z","close_reason":"Added ArcCache write policy enum + opt-in write-back mode, added coverage tests, and passed fmt/check/clippy/test.","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1tz","title":"btrfs walker: reject duplicate node references across traversal","description":"Context: cycle detection landed in bd-382, but duplicate child pointers can still cause repeated traversal/results without forming an ancestor cycle.\\n\\nProblem:\\n- walk_tree currently guards active recursion cycles; it still accepts non-cyclic duplicate logical node references (same child reached twice).\\n\\nScope:\\n- Track globally visited logical nodes across full walk.\\n- Return ParseError::InvalidField for duplicate node references.\\n- Add regression test for duplicated child pointer case.\\n\\nAcceptance:\\n- Valid trees unchanged.\\n- Duplicate node references fail fast with deterministic error.\\n- cargo test -p ffs-btrfs passes.","status":"closed","priority":1,"issue_type":"task","assignee":"FoggyIsland","created_at":"2026-02-11T18:47:59.042964538Z","created_by":"ubuntu","updated_at":"2026-02-11T18:49:06.145413817Z","closed_at":"2026-02-11T18:49:06.145395372Z","close_reason":"Implemented duplicate-node reference detection in btrfs tree walk + regression test; full gates pass","source_repo":".","compaction_level":0,"original_size":0,"labels":["btrfs","conformance","safety"]}
{"id":"bd-1u7","title":"MVCC: Design persistence for versioned blocks (COW log / version store)","description":"Goal: move MVCC from in-memory-only to durable versioned storage.\n\nDeliverables:\n- Choose a version store strategy:\n  - append-only log of (block, commit_seq, bytes)\n  - or a separate COW file with mapping table\n- Define crash consistency (fsync points, recovery on restart).\n- Define GC/pruning strategy and watermark semantics.\n\nAcceptance:\n- A concrete design exists in COMPREHENSIVE_SPEC + PROPOSED_ARCHITECTURE.\n- We can replay the version store to reconstruct the latest snapshot after restart.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:22:46.671058738Z","created_by":"ubuntu","updated_at":"2026-02-11T02:12:58.738125666Z","closed_at":"2026-02-11T02:12:58.738102002Z","close_reason":"Documented durable MVCC version store overlay (COMPREHENSIVE_SPEC ¬ß5.9 + PROPOSED_ARCHITECTURE ¬ß3.2.1 + config hook); cargo fmt/check/clippy/test pass","source_repo":".","compaction_level":0,"original_size":0,"labels":["mvcc"],"dependencies":[{"issue_id":"bd-1u7","depends_on_id":"bd-2t1","type":"blocks","created_at":"2026-02-10T03:23:53.343628845Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1va","title":"Docs: Reconcile ext4/btrfs scope + phased support statements","description":"Goal: make sure every doc says the same thing about what FrankenFS V1 supports and what is explicitly excluded/phased.\n\nMust be consistent across: COMPREHENSIVE_SPEC, PLAN, README, FEATURE_PARITY.\n\nScope items to pin down:\n- ext4 v1: supports 1K/2K/4K block sizes; requires extents+filetype; read-only mount first; journaling replay and writes later.\n- btrfs: phased; single-device sys_chunk mapping and read-only discovery first; no multi-device/RAID/compression initially.\n\nAcceptance:\n- There is one canonical exclusions list and it is referenced everywhere.\n- Any test corpus/harness fixtures do not contradict exclusions.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:13:50.654693806Z","created_by":"ubuntu","updated_at":"2026-02-11T02:08:56.766242627Z","closed_at":"2026-02-11T02:08:56.766159160Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs"],"dependencies":[{"issue_id":"bd-1va","depends_on_id":"bd-2ds","type":"blocks","created_at":"2026-02-10T03:14:05.746969333Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1wx","title":"MVCC: Implement SSI conflict detection (phased, correctness-first)","description":"Goal: detect and prevent write skew / dangerous structures beyond FCW.\n\nDeliverables:\n- Define read_set/write_set tracking strategy with version info (not just BlockNumber).\n- Implement rw-antidependency tracking and dangerous-structure detection per SSI literature.\n- Provide an escape hatch: run SSI only for write transactions (read txns should be cheap).\n\nAcceptance:\n- A formal model section in the spec that matches code.\n- Tests cover a known write-skew scenario that FCW allows but SSI rejects.","status":"closed","priority":2,"issue_type":"task","owner":"PinkCreek","created_at":"2026-02-10T03:22:53.621860531Z","created_by":"ubuntu","updated_at":"2026-02-11T03:12:17.306135330Z","closed_at":"2026-02-11T03:12:17.306114320Z","close_reason":"SSI commit_ssi() with rw-antidependency detection, read-set tracking, lab_ssi_rejects_write_skew test across 20 seeds, spec ¬ß5.0 updated","source_repo":".","compaction_level":0,"original_size":0,"labels":["mvcc"],"dependencies":[{"issue_id":"bd-1wx","depends_on_id":"bd-2t1","type":"blocks","created_at":"2026-02-10T03:23:53.427799854Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1wx","depends_on_id":"bd-hrv","type":"blocks","created_at":"2026-02-10T03:23:53.512260635Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1x8","title":"Cleanup: Delete bootstrap temp files (.spec_*.md) [REQUIRES EXPLICIT USER PERMISSION]","description":"Rule: DO NOT delete any file without explicit written user permission (AGENTS.md Rule 1).\n\nScope:\n- Remove leftover bootstrap concatenation artifacts like .spec_*.md (currently at least .spec_10_11.md exists).\n\nAcceptance:\n- Only proceed after user provides explicit written permission naming the files/pattern.\n- Record the permission text and exact command(s) executed in the final response when it happens.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-02-10T03:31:17.885693003Z","created_by":"ubuntu","updated_at":"2026-02-11T18:46:22.416106468Z","closed_at":"2026-02-11T18:46:22.416080529Z","close_reason":"Verified no .spec_* files exist in repository; no deletion required and no destructive action taken","source_repo":".","compaction_level":0,"original_size":0,"labels":["cleanup"]}
{"id":"bd-1xe","title":"EPIC: ext4 Read Path Completion","description":"# EPIC: ext4 Read Path Completion\n\n## PURPOSE\nComplete the ext4 read-only support to achieve >90% metadata parsing parity. Currently at 47.4% (9/19).\n\n## BACKGROUND\nFrom FEATURE_PARITY.md, the ext4 gaps are:\n- ext4 journal replay parity: üü° PARTIAL (basic replay done, mount-path integration pending)\n- ext4 allocator parity: ‚ùå NOT IMPLEMENTED\n- ext4 orphan recovery parity: ‚ùå NOT IMPLEMENTED\n\nThe ext4 read path is foundational for:\n1. The write path (must understand allocator state before mutating)\n2. FUSE mount reliability (orphan visibility needed for clean diagnostics)\n3. Conformance testing (need full parsing to generate fixtures)\n\n## CURRENT STATE\n- Superblock, inode, extent, group descriptor decode: ‚úÖ\n- Feature flag validation: ‚úÖ\n- Directory entry parsing: ‚úÖ\n- Inode device read: ‚úÖ\n- Path resolution: ‚úÖ\n- Journal descriptor/commit/revoke replay: ‚úÖ (basic)\n\n## GAPS TO CLOSE\n1. Journal mount-path integration (replay at open time)\n2. Allocator bitmap reading (free block/inode tracking)\n3. Orphan inode list processing (read-only diagnostics)\n\n## ACCEPTANCE CRITERIA\n1. ext4 metadata parsing reaches 90%+ parity (17+/19)\n2. Journal replay integrated into `OpenFs::open()` flow\n3. Allocator state readable (not writable; mutation is write-path work)\n4. Orphan list readable for diagnostic purposes\n5. All changes validated via golden fixtures\n\n## DEPENDENCIES\n- EPIC: Conformance & Quality Infrastructure (bd-2jk) - fixtures/goldens/E2E infrastructure\n\n## RELATED SPEC SECTIONS\n- EXISTING_EXT4_BTRFS_STRUCTURE.md ¬ß1-3 (ext4 behavior)\n- COMPREHENSIVE_SPEC_FOR_FRANKENFS_V1.md ¬ß1 (ext4 format)\n\n## Success Criteria\n1. `ffs inspect <ext4.img> --json` reports ext4 geometry + journal state + free-space summary without panicking on corrupted images.\n2. `ffs mount <ext4.img> <mnt>` supports stable read-only browsing for supported images.\n3. New behavior is covered by:\n   - unit tests for parsing/geometry/bitmap/orphan logic\n   - golden fixture validation\n   - E2E smoke (`scripts/e2e/ffs_smoke.sh`)","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-12T14:55:46.887323231Z","created_by":"ubuntu","updated_at":"2026-02-12T20:58:01.114952182Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1xe","depends_on_id":"bd-2jk","type":"blocks","created_at":"2026-02-12T15:04:28.930036950Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1xe.1","title":"Integrate journal replay into OpenFs mount flow","description":"# Integrate journal replay into OpenFs mount flow\n\n## GOAL\nWire ffs-journal's replay logic into ffs-core's OpenFs::open() so journal recovery happens automatically at mount time.\n\n## CONTEXT\nFrom FEATURE_PARITY.md: \"ext4 journal replay parity: üü° Phase 1 implemented (descriptor/commit/revoke replay + tests); full mount-path integration and complete parity still pending\"\n\nThe journal replay code exists in ffs-journal but isn't called from the mount path. When opening an ext4 image that wasn't cleanly unmounted, we need to:\n1. Detect journal needs replay (check JBD2 sequence numbers)\n2. Replay committed transactions\n3. Report replay statistics\n4. For read-only mount: don't actually modify image, just track what WOULD change\n\n## IMPLEMENTATION APPROACH\n\n### Step 1: Add journal detection to OpenFs\n```rust\n// In ffs-core/src/open.rs\nimpl OpenFs {\n    pub fn open(cx: &Cx, path: &Path, options: MountOptions) -> Result<Self> {\n        // ... existing code ...\n        \n        // Check if journal needs replay\n        if detected == FsFlavor::Ext4 {\n            let journal_outcome = ffs_journal::check_and_replay(\n                cx, \n                &device, \n                &superblock,\n                options.read_only\n            )?;\n            \n            if !journal_outcome.clean {\n                // Log replay statistics\n                tracing::info!(\n                    replayed_txns = journal_outcome.replayed_count,\n                    revoked_blocks = journal_outcome.revoked_count,\n                    \"Journal replay completed\"\n                );\n            }\n        }\n        // ...\n    }\n}\n```\n\n### Step 2: Add JournalReplayOutcome to OpenFs\n```rust\npub struct OpenFs {\n    // ... existing fields ...\n    pub journal_outcome: Option<JournalReplayOutcome>,\n}\n\npub struct JournalReplayOutcome {\n    pub clean: bool,\n    pub replayed_count: u32,\n    pub revoked_count: u32,\n    pub highest_sequence: u32,\n}\n```\n\n### Step 3: Update CLI inspect to report journal state\nInclude journal health in inspect output JSON.\n\n## FILES TO MODIFY\n- crates/ffs-core/src/open.rs\n- crates/ffs-journal/src/lib.rs (export replay API)\n- crates/ffs-cli/src/commands/inspect.rs\n\n## ACCEPTANCE CRITERIA\n1. Opening unclean ext4 image triggers journal replay\n2. Replay statistics reported in OpenFs\n3. Read-only mode: replay is simulated, not written\n4. Golden test with unclean shutdown image\n5. FEATURE_PARITY.md updated: journal replay ‚Üí ‚úÖ\n\n## DEPENDENCIES\n- ext4 golden fixtures (bd-2jk.1) - need test images\n\n## VERIFICATION\nCreate test image, write files, don't sync, kill. Then:\n```bash\ncargo run -p ffs-cli -- inspect unclean.img --json | jq .journal\n```","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-12T14:56:03.656388753Z","created_by":"ubuntu","updated_at":"2026-02-12T14:56:41.149087334Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1xe.1","depends_on_id":"bd-1xe","type":"parent-child","created_at":"2026-02-12T14:56:03.656388753Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1xe.1","depends_on_id":"bd-2jk.1","type":"blocks","created_at":"2026-02-12T14:56:41.149027162Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1xe.2","title":"Implement ext4 allocator bitmap reading","description":"# Implement ext4 allocator bitmap reading\n\n## GOAL\nImplement read-only access to ext4 block and inode allocation bitmaps for free space reporting and allocator state inspection.\n\n## CONTEXT\nFrom FEATURE_PARITY.md: \"ext4 allocator parity: ‚ùå Not yet implemented\"\n\nThe allocator reading is needed for:\n1. Free space reporting (df-like output)\n2. Write path planning (need to know allocator state before mutation)\n3. fsck-like integrity checks (bitmap vs actual usage)\n4. Scrub validation\n\n## IMPLEMENTATION APPROACH\n\n### Data Structures (from EXISTING_EXT4_BTRFS_STRUCTURE.md)\n```rust\n// In ffs-alloc or ffs-ondisk\npub struct BlockGroupBitmap {\n    pub group: GroupNumber,\n    pub block_bitmap: Vec<u8>,  // 1 bit per block\n    pub inode_bitmap: Vec<u8>,  // 1 bit per inode\n    pub free_blocks_count: u32,\n    pub free_inodes_count: u32,\n}\n\nimpl BlockGroupBitmap {\n    pub fn is_block_free(&self, block_in_group: u32) -> bool;\n    pub fn is_inode_free(&self, inode_in_group: u32) -> bool;\n    pub fn count_free_blocks(&self) -> u32;\n    pub fn count_free_inodes(&self) -> u32;\n}\n```\n\n### Read Implementation\n```rust\n// In ffs-core or ffs-alloc\npub fn read_block_bitmap(\n    cx: &Cx,\n    device: &dyn BlockDevice,\n    geometry: &Ext4Geometry,\n    group: GroupNumber,\n) -> Result<Vec<u8>> {\n    let gd = read_group_descriptor(cx, device, geometry, group)?;\n    let bitmap_block = gd.bg_block_bitmap_lo; // + hi for 64-bit\n    device.read_block(cx, BlockNumber(bitmap_block))\n}\n```\n\n### Integration with OpenFs\n```rust\nimpl OpenFs {\n    pub fn free_space_summary(&self, cx: &Cx) -> Result<FreeSpaceSummary> {\n        let mut total_free_blocks = 0u64;\n        let mut total_free_inodes = 0u64;\n        \n        for group in 0..self.geometry.groups_count {\n            let bitmap = self.read_block_bitmap(cx, GroupNumber(group))?;\n            total_free_blocks += count_set_bits(&bitmap) as u64;\n            // ... similar for inodes\n        }\n        \n        Ok(FreeSpaceSummary { total_free_blocks, total_free_inodes })\n    }\n}\n```\n\n## FILES TO MODIFY\n- crates/ffs-alloc/src/bitmap.rs (add reading functions)\n- crates/ffs-ondisk/src/ext4/group_desc.rs (ensure 64-bit bitmap location support)\n- crates/ffs-core/src/ext4/mod.rs (integrate bitmap reading)\n- crates/ffs-cli/src/commands/inspect.rs (add free space to output)\n\n## ACCEPTANCE CRITERIA\n1. Can read block bitmap for any group\n2. Can read inode bitmap for any group\n3. Free block/inode counts match superblock reported values\n4. Bitmap checksum verification (if checksums enabled)\n5. ffs inspect shows free space summary\n6. FEATURE_PARITY.md updated: allocator bitmap reading ‚Üí ‚úÖ\n\n## TESTING\n```bash\n# Create image, check free space matches mkfs output\nmkfs.ext4 -F test.img\ntune2fs -l test.img | grep \"Free blocks\"\ncargo run -p ffs-cli -- inspect test.img --json | jq .free_blocks\n# Must match!\n```\n\n## NOTE\nThis is READ-ONLY. Mutation (allocation/deallocation) is in the write path epic.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-12T14:56:21.142369233Z","created_by":"ubuntu","updated_at":"2026-02-12T14:56:21.142369233Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1xe.2","depends_on_id":"bd-1xe","type":"parent-child","created_at":"2026-02-12T14:56:21.142369233Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1xe.3","title":"Implement ext4 orphan inode list reading","description":"# Implement ext4 orphan inode list reading\n\n## GOAL\nImplement read-only access to ext4's orphan inode list for diagnostics and eventual recovery.\n\n## CONTEXT\nFrom FEATURE_PARITY.md: \"ext4 orphan recovery parity: ‚ùå Not yet implemented\"\n\nOrphan inodes are files that were deleted while still open, or files being truncated when system crashed. The orphan list is a linked list starting from superblock.s_last_orphan.\n\nFor V1 read-only mount, we need to:\n1. Detect orphan inodes exist\n2. Report them in diagnostics\n3. For read-only: don't process (just warn)\n\n## IMPLEMENTATION APPROACH\n\n### Data Structures\n```rust\npub struct OrphanList {\n    pub orphan_inodes: Vec<InodeNumber>,\n    pub total_size: u64,  // Sum of orphan file sizes\n}\n\nimpl OrphanList {\n    pub fn is_empty(&self) -> bool;\n    pub fn len(&self) -> usize;\n}\n```\n\n### Walk the orphan list\n```rust\npub fn read_orphan_list(\n    cx: &Cx,\n    device: &dyn BlockDevice,\n    geometry: &Ext4Geometry,\n    superblock: &Ext4Superblock,\n) -> Result<OrphanList> {\n    let mut orphans = Vec::new();\n    let mut current = superblock.s_last_orphan;\n    \n    while current != 0 {\n        let ino = InodeNumber(current);\n        orphans.push(ino);\n        \n        let inode = read_inode(cx, device, geometry, ino)?;\n        current = inode.i_dtime; // Next orphan stored in dtime field\n        \n        // Cycle detection\n        if orphans.len() > geometry.inodes_count as usize {\n            return Err(FfsError::Corruption {\n                block: None,\n                detail: \"Orphan list cycle detected\".into(),\n            });\n        }\n    }\n    \n    Ok(OrphanList { orphan_inodes: orphans, total_size: 0 })\n}\n```\n\n### Integration\n```rust\nimpl OpenFs {\n    pub fn orphan_status(&self, cx: &Cx) -> Result<OrphanList>;\n}\n```\n\n## FILES TO MODIFY\n- crates/ffs-ondisk/src/ext4/orphan.rs (new file)\n- crates/ffs-core/src/ext4/mod.rs\n- crates/ffs-cli/src/commands/inspect.rs\n\n## ACCEPTANCE CRITERIA\n1. Can read orphan list from superblock\n2. Cycle detection prevents infinite loop\n3. Orphan count reported in inspect output\n4. Warning emitted if orphans exist on read-only mount\n5. FEATURE_PARITY.md updated\n\n## TESTING\nCreate orphan by:\n1. Open file, delete it while keeping fd open\n2. Kill process without closing fd\n3. Inspect image - should show orphan\n\n## NOTE\nActual orphan RECOVERY (deleting the orphans) is write-path work. This task is read-only detection.","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-12T14:56:37.082624742Z","created_by":"ubuntu","updated_at":"2026-02-12T14:56:37.082624742Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1xe.3","depends_on_id":"bd-1xe","type":"parent-child","created_at":"2026-02-12T14:56:37.082624742Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1y2","title":"Docs: sync README parity stats and legacy corpus note with current repo","description":"Context: current README status numbers and one repo-state statement drift from canonical parity/report reality.\\n\\nScope:\\n- Update README feature-parity table to match FEATURE_PARITY.md and ParityReport::current() values.\\n- Update README statement that legacy source corpus is not included (repo currently contains legacy_ext4_and_btrfs_code/linux-fs).\\n- Keep changes documentation-only and non-destructive.\\n\\nAcceptance:\\n- README parity values match FEATURE_PARITY.md exactly.\\n- README legacy corpus statement accurately reflects current repository contents.\\n- cargo fmt/check/clippy/test pass after change.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T18:43:17.729431835Z","created_by":"ubuntu","updated_at":"2026-02-12T08:22:46.109429168Z","closed_at":"2026-02-12T08:22:46.109361031Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1zw","title":"Randomized deep bug-hunt across core workflows","description":"Sample core files, trace execution/import flows, find defects with fresh eyes, patch issues, and run full workspace gates.","notes":"Completed randomized deep investigation; fixed ffs-fuse readdir filename byte-loss, ffs-cli scrub btrfs alignment + severity-based corrupt count, and ffs-mvcc snapshot lifetime registration/release. Ran fmt/check/clippy/test successfully.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-12T14:54:05.875288741Z","created_by":"ubuntu","updated_at":"2026-02-12T15:05:26.320857160Z","closed_at":"2026-02-12T15:05:26.320763184Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-220","title":"Track: MVCC Engine (ffs-mvcc)","description":"Implement block-level MVCC: snapshot reads, versioned writes, commit/abort semantics, and (phased) conflict detection (SSI-style) suitable for filesystem workloads.\\n\\nAcceptance: deterministic correctness under asupersync lab runtime (reproducible schedules), explicit invariants + tests, and integration with ffs-block for versioned block storage. Must be designed so FUSE operations can run concurrently without global locks.","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-02-10T03:08:56.957648896Z","created_by":"ubuntu","updated_at":"2026-02-11T03:13:22.508009156Z","closed_at":"2026-02-11T03:13:22.507987376Z","close_reason":"All dependencies closed: FCW + SSI conflict detection, watermark GC, MVCC block device wrapper, deterministic lab tests. 37 ffs-mvcc tests pass.","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-220","depends_on_id":"bd-126","type":"blocks","created_at":"2026-02-10T03:31:38.690642752Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-220","depends_on_id":"bd-19k","type":"blocks","created_at":"2026-02-10T03:23:03.836737989Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-220","depends_on_id":"bd-1u7","type":"blocks","created_at":"2026-02-10T03:23:04.004715853Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-220","depends_on_id":"bd-1wx","type":"blocks","created_at":"2026-02-10T03:23:04.085808825Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-220","depends_on_id":"bd-2fa","type":"blocks","created_at":"2026-02-10T03:24:01.103674030Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-220","depends_on_id":"bd-2l4","type":"blocks","created_at":"2026-02-10T03:31:38.603663691Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-220","depends_on_id":"bd-2t1","type":"blocks","created_at":"2026-02-10T03:23:03.750610235Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-220","depends_on_id":"bd-3ie","type":"blocks","created_at":"2026-02-10T03:23:04.170472987Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-220","depends_on_id":"bd-hrv","type":"blocks","created_at":"2026-02-10T03:23:03.921347970Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-220","depends_on_id":"bd-hv6","type":"blocks","created_at":"2026-02-10T03:31:38.514583520Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-220","depends_on_id":"bd-z7n","type":"blocks","created_at":"2026-02-10T03:24:01.025948959Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":9,"issue_id":"bd-220","author":"Dicklesworthstone","text":"MVCC is the core FrankenFS differentiator.\n\nWe start with FCW (simple, testable), then evolve toward SSI only where necessary.\nKey integration points:\n- MVCC-aware block reads: (snapshot -> version or base)\n- Durable version store design\n- Deterministic concurrency tests\n\nThis track must remain mathematically defensible and heavily tested.","created_at":"2026-02-10T03:34:39Z"}]}
{"id":"bd-22w","title":"EPIC: MVCC Core Completion","description":"# EPIC: MVCC Core Completion\n\n## PURPOSE\nComplete the MVCC (Multi-Version Concurrency Control) subsystem to production quality. Currently at 28.6% (4/14).\n\n## BACKGROUND\nMVCC is FrankenFS's key architectural innovation over kernel ext4/btrfs. It enables:\n- Concurrent readers + writers without JBD2's global lock\n- Snapshot isolation for consistent reads\n- First-committer-wins (FCW) and SSI conflict detection\n\nFrom FEATURE_PARITY.md:\n- MVCC snapshot visibility: ‚úÖ\n- MVCC commit sequencing: ‚úÖ\n- FCW conflict detection: ‚úÖ\n- Version retention policy: ‚úÖ (in-memory)\n- COW block rewrite path: üü° PARTIAL (basic version copy only)\n\n## GAPS TO CLOSE\n1. MVCC persistence - version chains survive restart (WAL)\n2. Version chain compression - reduce memory overhead\n3. GC optimization - efficient pruning under active snapshots\n4. COW block rewrite path - complete write-side semantics\n\n## ALIEN-ARTIFACT QUALITY BAR (from AGENTS.md)\nMVCC conflict logic is explicitly called out for principled treatment:\n- explicit invariants\n- evidence ledger for commit/reject decisions\n- formal SSI cycle detection (or equivalent correctness argument + tests)\n\n## CURRENT STATE\n- `MvccStore` with in-memory version chains\n- Transaction APIs (`begin` / `commit` / `commit_ssi`)\n- Snapshot visibility via search\n- GC via `prune_versions_older_than` and watermarking\n\n## ACCEPTANCE CRITERIA\n1. MVCC persistence layer (WAL or equivalent)\n2. Version chain memory bounded (compression and/or GC policy)\n3. Memory overhead documented and benchmarked\n4. Conflict detection formally verified (proptest + deterministic interleavings)\n5. MVCC/COW reaches 70%+ parity (10/14)\n\n## DEPENDENCIES\n- EPIC: Conformance & Quality Infrastructure (bd-2jk) - need baselines and goldens for safe iteration\n\n## RELATED SPEC SECTIONS\n- COMPREHENSIVE_SPEC_FOR_FRANKENFS_V1.md ¬ß3 (MVCC)\n- PROPOSED_ARCHITECTURE.md (ffs-mvcc crate)\n\n## Success Criteria\n1. Crash-restart tests demonstrate committed versions persist and uncommitted versions do not.\n2. Deterministic concurrency tests (lab runtime) cover snapshot stability and FCW/SSI invariants.\n3. Compression reduces memory on representative workloads and is covered by unit tests + microbenchmarks.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-12T15:00:05.830436981Z","created_by":"ubuntu","updated_at":"2026-02-12T20:58:37.626278285Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-22w","depends_on_id":"bd-2jk","type":"blocks","created_at":"2026-02-12T15:04:29.145275027Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-22w.1","title":"Implement MVCC version persistence layer","description":"# Implement MVCC version persistence layer\n\n## GOAL\nMake MVCC version chains survive process restart by persisting to disk.\n\n## CONTEXT\nCurrent MvccStore is entirely in-memory. On crash/restart, all uncommitted writes and version history are lost. For durability, we need:\n1. WAL (write-ahead log) for version changes\n2. Or: dedicated version store file alongside image\n\n## DESIGN OPTIONS\n\n### Option A: WAL-based persistence\n```\nWrite flow:\n1. Transaction writes to in-memory version chain\n2. On commit: append to WAL file\n3. Periodically checkpoint: compact WAL into base state\n4. On startup: replay WAL\n\nPros: Standard pattern, well-understood\nCons: Two-layer commit (WAL + actual blocks)\n```\n\n### Option B: Separate version store\n```\nWrite flow:\n1. Version chains stored in dedicated mmap'd file\n2. Format: append-only log of (block, seq, data)\n3. Index: separate file with BlockNumber ‚Üí offset mappings\n4. On startup: rebuild index from log\n\nPros: Simpler than WAL\nCons: Unbounded growth needs compaction\n```\n\n### Option C: Inline with MVCC blocks\n```\nStore versions in a reserved region of the filesystem image itself.\nRequires modifying image format - probably too invasive for V1.\n```\n\n## RECOMMENDED: Option A (WAL)\n\n### Implementation\n```rust\n// In ffs-mvcc/src/persist.rs\npub struct MvccWal {\n    wal_path: PathBuf,\n    writer: BufWriter<File>,\n    sequence: AtomicU64,\n}\n\npub enum WalEntry {\n    BeginTxn { txn_id: TxnId, snapshot: CommitSeq },\n    WriteBlock { txn_id: TxnId, block: BlockNumber, data: Vec<u8> },\n    Commit { txn_id: TxnId, commit_seq: CommitSeq },\n    Abort { txn_id: TxnId },\n    Checkpoint { commit_seq: CommitSeq },\n}\n\nimpl MvccWal {\n    pub fn append(&mut self, entry: WalEntry) -> Result<()>;\n    pub fn replay(&self) -> Result<MvccStore>;\n    pub fn checkpoint(&mut self, store: &MvccStore) -> Result<()>;\n}\n```\n\n### Integration\n```rust\nimpl MvccStore {\n    pub fn with_persistence(wal_path: &Path) -> Result<Self>;\n    \n    pub fn commit(&mut self, txn: Transaction) -> Result<CommitSeq> {\n        // Existing FCW logic\n        let seq = self.do_commit(txn)?;\n        // NEW: persist to WAL\n        self.wal.as_mut().map(|w| w.append(WalEntry::Commit { ... }));\n        Ok(seq)\n    }\n}\n```\n\n## FILES TO CREATE/MODIFY\n- crates/ffs-mvcc/src/persist.rs (new)\n- crates/ffs-mvcc/src/lib.rs (integrate persistence)\n- crates/ffs-mvcc/src/wal.rs (WAL format)\n\n## ACCEPTANCE CRITERIA\n1. WAL file created alongside operations\n2. After crash, replay restores committed transactions\n3. Uncommitted transactions not restored\n4. Checkpoint compaction reduces WAL size\n5. Benchmark: persistence overhead < 20% of in-memory\n\n## TESTING\n```rust\n#[test]\nfn test_crash_recovery() {\n    let store = MvccStore::with_persistence(tmp_wal_path);\n    let txn = store.begin();\n    store.write(txn, block, data);\n    store.commit(txn);\n    drop(store);  // Simulate crash\n    \n    let recovered = MvccStore::with_persistence(tmp_wal_path);\n    assert_eq!(recovered.read_latest(block), data);\n}\n```\n\n## RATIONALE\nPer alien-artifact-coding: \"Graceful degradation - Works with whatever data is available.\"\nWAL provides durability while maintaining in-memory speed for common case.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-12T15:00:26.082439996Z","created_by":"ubuntu","updated_at":"2026-02-12T15:00:26.082439996Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-22w.1","depends_on_id":"bd-22w","type":"parent-child","created_at":"2026-02-12T15:00:26.082439996Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-22w.2","title":"Implement version chain compression","description":"# Implement version chain compression\n\n## GOAL\nReduce memory overhead of MVCC version chains through intelligent compression and deduplication.\n\n## CONTEXT\nEach block version stores full block data (4KB-64KB). For frequently modified blocks, version chains grow large. Need strategies to bound memory usage.\n\n## COMPRESSION STRATEGIES\n\n### Strategy 1: Delta encoding\n```rust\npub enum BlockVersionData {\n    Full(Vec<u8>),\n    Delta { base_seq: CommitSeq, delta: Vec<DeltaOp> },\n}\n\npub enum DeltaOp {\n    Copy { src_off: u32, dst_off: u32, len: u32 },\n    Insert { off: u32, data: Vec<u8> },\n}\n```\n- Store first version as full, subsequent as deltas\n- Reconstruct by applying deltas to base\n- Tradeoff: CPU for memory\n\n### Strategy 2: Version chain capping\n```rust\nimpl MvccStore {\n    pub fn compact_versions(&mut self, block: BlockNumber, max_versions: usize) {\n        let versions = self.versions.get_mut(&block)?;\n        if versions.len() > max_versions {\n            // Keep newest max_versions, discard old\n            // Only safe if no active transaction needs old versions\n            let min_needed = self.oldest_active_snapshot();\n            versions.retain(|v| v.commit_seq >= min_needed);\n        }\n    }\n}\n```\n\n### Strategy 3: Block content deduplication\n```rust\n// If block content identical to previous version, store reference instead\npub enum BlockVersionData {\n    Full(Vec<u8>),\n    SameAs(CommitSeq),  // \"This version is identical to version at seq N\"\n}\n```\n\n## RECOMMENDED: Combination approach\n1. Delta encoding for sequential modifications\n2. Deduplication for unchanged blocks\n3. Cap at N versions (configurable, default 100)\n4. LRU eviction for very old versions\n\n## MEMORY BUDGET\n- Target: <10x overhead vs single-version storage\n- Example: 1GB image, 10% hot blocks = 100MB hot data\n- With 100 versions per hot block = 10GB uncompressed\n- With delta encoding: ~500MB (5x compression typical)\n\n## IMPLEMENTATION\n```rust\n// In ffs-mvcc/src/compression.rs\npub struct CompressedVersionChain {\n    base: Vec<u8>,\n    deltas: Vec<(CommitSeq, Vec<DeltaOp>)>,\n}\n\nimpl CompressedVersionChain {\n    pub fn reconstruct(&self, target_seq: CommitSeq) -> Vec<u8>;\n    pub fn append(&mut self, seq: CommitSeq, new_data: &[u8]);\n    pub fn memory_usage(&self) -> usize;\n}\n```\n\n## ACCEPTANCE CRITERIA\n1. Delta encoding implemented and tested\n2. Memory usage reduced 5x for typical workloads\n3. Reconstruction latency < 1ms for reasonable chain lengths\n4. Benchmark: compression overhead < 5% CPU\n5. Config option to tune compression vs speed\n\n## TESTING\n```rust\n#[test]\nfn test_delta_reconstruction() {\n    let mut chain = CompressedVersionChain::new(block_data_v1);\n    chain.append(seq2, block_data_v2);  // Small change\n    chain.append(seq3, block_data_v3);  // Small change\n    \n    assert_eq!(chain.reconstruct(seq1), block_data_v1);\n    assert_eq!(chain.reconstruct(seq3), block_data_v3);\n    assert!(chain.memory_usage() < 2 * BLOCK_SIZE);  // Compressed\n}\n```","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-12T15:00:45.109316421Z","created_by":"ubuntu","updated_at":"2026-02-12T15:00:45.109316421Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-22w.2","depends_on_id":"bd-22w","type":"parent-child","created_at":"2026-02-12T15:00:45.109316421Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-23n","title":"CLI: Implement mount subcommand (ffs-cli mount <image> <mountpoint>)","description":"Goal: provide the primary entry point for users: mount an ext4 image via FUSE.\n\nDeliverables:\n- Add CLI parsing for mount args + options (read-only, allow-other, etc).\n- Use ffs-core OpenFs API to open/validate image.\n- Construct ffs-fuse Filesystem + start fuser mount loop.\n\nAcceptance:\n- Manual E2E: mount fixture image read-only and run ls/stat/cat.\n- Errors are user-friendly and map to correct errno/log messages.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:29:29.600373001Z","created_by":"ubuntu","updated_at":"2026-02-10T19:51:15.997299182Z","closed_at":"2026-02-10T19:51:15.997281579Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli","fuse"],"dependencies":[{"issue_id":"bd-23n","depends_on_id":"bd-27a","type":"blocks","created_at":"2026-02-10T03:30:05.615727710Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23n","depends_on_id":"bd-3is","type":"blocks","created_at":"2026-02-10T03:30:05.699859913Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23n","depends_on_id":"bd-3vn","type":"blocks","created_at":"2026-02-10T03:30:05.526795366Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-23r","title":"TUI: Minimal dashboard (cache/MVCC/repair metrics) via ftui [phased]","description":"Goal: provide a live view of internal state for debugging and demos.\n\nDeliverables:\n- A ftui-based screen that shows:\n  - cache hit/miss, p value\n  - mvcc commit seq, open txns\n  - repair overhead + recent scrub events\n\nAcceptance:\n- Can run against a local mount or harness run and update periodically.","status":"closed","priority":3,"issue_type":"task","owner":"PinkCreek","created_at":"2026-02-10T03:29:50.041190926Z","created_by":"ubuntu","updated_at":"2026-02-11T03:42:44.518865708Z","closed_at":"2026-02-11T03:42:44.518843938Z","close_reason":"Implemented: Dashboard model + DashboardSnapshot DTO + 3-panel view (cache/MVCC/scrub) + 12 tests","source_repo":".","compaction_level":0,"original_size":0,"labels":["tui"],"dependencies":[{"issue_id":"bd-23r","depends_on_id":"bd-2t1","type":"blocks","created_at":"2026-02-10T03:30:06.040436183Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23r","depends_on_id":"bd-ece","type":"blocks","created_at":"2026-02-10T03:30:05.953356393Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-23s","title":"Epic: ext4 Advanced Semantics (htree, symlink, xattr, permissions)","description":"Follow-on work to reach stronger ext4 parity beyond the MVP.\n\nScope examples:\n- htree indexed dirs\n- symlinks\n- xattrs\n- permission bits and ownership mapping details\n\nAcceptance:\n- Each feature has fixtures and at least one conformance comparison against kernel behavior.","status":"closed","priority":3,"issue_type":"epic","created_at":"2026-02-10T03:26:58.089326744Z","created_by":"ubuntu","updated_at":"2026-02-11T03:27:48.847862449Z","closed_at":"2026-02-11T03:27:48.847835960Z","close_reason":"All 4 sub-tasks complete: permissions/rdev (bd-q0g), xattr (bd-3bu), symlinks (bd-wse), htree (bd-3qy).","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","semantics"],"dependencies":[{"issue_id":"bd-23s","depends_on_id":"bd-3bu","type":"blocks","created_at":"2026-02-10T03:27:52.066658739Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23s","depends_on_id":"bd-3qy","type":"blocks","created_at":"2026-02-10T03:27:51.896047591Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23s","depends_on_id":"bd-q0g","type":"blocks","created_at":"2026-02-10T03:27:52.155086654Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-23s","depends_on_id":"bd-wse","type":"blocks","created_at":"2026-02-10T03:27:51.978648257Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-25k","title":"ext4: Implement group descriptor checksum verification hooks (metadata_csum)","description":"Goal: prepare for ext4 metadata checksum verification (CRC32C) when the filesystem uses metadata_csum.\n\nDeliverables:\n- Determine checksum algorithm and seed for group descriptors (from legacy ext4 code) including how group number participates.\n- Add a function: verify_group_desc_checksum(desc_bytes, group_nr, superblock) -> Result<()>.\n- If metadata_csum is not enabled, skip verification.\n\nAcceptance:\n- Unit tests: known-good descriptor passes; flipped-bit fails.\n- Error type is stable and mapped correctly for user-facing surfaces.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:17:25.017305716Z","created_by":"ubuntu","updated_at":"2026-02-10T21:06:44.617157237Z","closed_at":"2026-02-10T21:06:44.617138091Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","ondisk"],"dependencies":[{"issue_id":"bd-25k","depends_on_id":"bd-1a9","type":"blocks","created_at":"2026-02-10T03:18:06.845984197Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-25k","depends_on_id":"bd-28h","type":"blocks","created_at":"2026-02-10T03:32:02.431265758Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-25k","depends_on_id":"bd-2cn","type":"blocks","created_at":"2026-02-10T03:18:06.766942705Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-265","title":"ffs-block: Concurrency design for cache (sharding + no I/O under lock)","description":"Goal: make sure cache remains safe and fast under many concurrent FUSE operations.\n\nDeliverables:\n- Decide locking strategy (single Mutex vs sharded) and document it.\n- Ensure we never do disk I/O while holding a global cache lock.\n- Add a concurrency stress test under asupersync lab runtime (deterministic schedule) or standard threads with timeouts.\n\nAcceptance:\n- Stress test does not deadlock.\n- Contention is measured (optional) and stays reasonable under synthetic load.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:16:00.946703440Z","created_by":"ubuntu","updated_at":"2026-02-10T20:53:29.955924247Z","closed_at":"2026-02-10T20:53:29.955906063Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["io"],"dependencies":[{"issue_id":"bd-265","depends_on_id":"bd-5iz","type":"blocks","created_at":"2026-02-10T03:16:27.057034396Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-27a","title":"FUSE: Implement read-only ext4 ops (getattr/lookup/readdir/open/read)","description":"Goal: mount a supported ext4 image read-only and run basic workloads (ls/stat/cat).\n\nDeliverables:\n- Map fuser callbacks to FsOps: getattr, lookup, readdir, read.\n- Correct file type + mode bits in replies.\n- Correct errno mapping for not found / not dir / permission.\n\nAcceptance:\n- Manual E2E: mount fixture image and run ls -la, stat, cat on a known file.\n- Integration test (if feasible) exercises a mounted instance in CI or as a local script.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:28:33.287038771Z","created_by":"ubuntu","updated_at":"2026-02-10T19:49:04.786727381Z","closed_at":"2026-02-10T19:49:04.786708486Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","fuse"],"dependencies":[{"issue_id":"bd-27a","depends_on_id":"bd-16k","type":"blocks","created_at":"2026-02-10T03:28:52.094578108Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-27a","depends_on_id":"bd-vgu","type":"blocks","created_at":"2026-02-10T03:28:52.009592894Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-28h","title":"ext4: Implement CRC32C checksum verification (superblock + inode + dir tail)","description":"Goal: verify ext4 metadata checksums when metadata_csum feature is enabled.\n\nDeliverables:\n- Add crc32c dependency (or equivalent) in appropriate crate.\n- Implement verify_superblock_checksum() (note: superblock checksum rules differ from group desc).\n- Implement inode checksum verification hook (phased).\n- Implement directory block tail checksum verification hook (phased).\n\nAcceptance:\n- Unit tests: checksum passes on known-good fixture bytes, fails on flipped bit.\n- Scrub pipeline can reuse these hooks to detect corruption.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:31:52.406901062Z","created_by":"ubuntu","updated_at":"2026-02-10T21:06:32.714655569Z","closed_at":"2026-02-10T21:06:32.714637305Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","ondisk"]}
{"id":"bd-29o","title":"Track: Filesystem Semantics (alloc, btree, extent, inode, dir, xattr, journal)","description":"Implement the actual filesystem metadata/data operations (initially ext4 read-only path) on top of on-disk parsing + block I/O. This is where we reach meaningful parity beyond 'can parse a superblock'.\\n\\nAcceptance: can resolve paths, read directory entries, read file data (extents), and report metadata via harness; later phases add writes + journal/MVCC integration.","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-02-10T03:09:13.171667672Z","created_by":"ubuntu","updated_at":"2026-02-11T03:44:02.965848077Z","closed_at":"2026-02-11T03:44:02.965826407Z","close_reason":"Read-only acceptance criteria met (lookup/readdir/read/xattr/htree/symlink). Write path deferred to bd-zge.","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-29o","depends_on_id":"bd-16k","type":"blocks","created_at":"2026-02-10T03:28:06.665935833Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-29o","depends_on_id":"bd-23s","type":"blocks","created_at":"2026-02-10T03:28:06.747444684Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-29o","depends_on_id":"bd-zge","type":"blocks","created_at":"2026-02-10T03:28:06.832473890Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":11,"issue_id":"bd-29o","author":"Dicklesworthstone","text":"This track is where parsing turns into an actual filesystem.\n\nWe split into:\n- ext4 Read-Only MVP (lookup/readdir/read) to unlock FUSE mounting and meaningful parity.\n- ext4 Advanced (htree, symlink, xattr, etc).\n- Write Path epic (alloc/journal/MVCC integration) for later.\n\nAll semantics work must be fixture-backed and routed through a minimal FsOps trait.","created_at":"2026-02-10T03:34:39Z"}]}
{"id":"bd-2bu","title":"ext4 semantics: Implement path lookup (linear scan) for name -> inode","description":"Goal: resolve paths like /a/b/c by walking directory inodes.\n\nDeliverables:\n- ffs-dir: lookup(parent_inode, name) -> inode_no.\n- Use linear scan over parsed dir entries first (htree phased).\n- Handle special entries: \".\" and \"..\".\n\nAcceptance:\n- Fixture test resolves a few known paths.\n- Errors are correct: NotFound vs NotDirectory vs NameTooLong.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:26:01.193715842Z","created_by":"ubuntu","updated_at":"2026-02-10T19:42:21.801329805Z","closed_at":"2026-02-10T19:42:21.801311261Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","semantics"],"dependencies":[{"issue_id":"bd-2bu","depends_on_id":"bd-2hm","type":"blocks","created_at":"2026-02-10T03:27:33.976312271Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2cn","title":"ext4: Expand superblock parsing toward full layout (ffs-ondisk)","description":"Goal: parse the ext4 superblock fields needed for mount validation and downstream math (group desc locations, inode table, features, checksums).\n\nDeliverables (incremental):\n- Parse additional core fields: s_log_cluster_size, s_blocks_per_group, s_inodes_per_group, s_inode_size, s_desc_size, s_feature_* full, s_uuid, s_checksum_seed (if present), s_checksum.\n- Provide a structured representation for feature flags (bitflags or typed wrapper).\n- Add tests that compare parsed values against known bytes (fixtures).\n\nAcceptance:\n- No panics on malformed data (ParseError only).\n- validate_v1() can be implemented purely from parsed fields.\n- Fixture-based test covers at least one real ext4 image superblock dump.","status":"closed","priority":0,"issue_type":"task","assignee":"QuietFalcon","created_at":"2026-02-10T03:16:48.654535684Z","created_by":"ubuntu","updated_at":"2026-02-10T16:40:23.557657945Z","closed_at":"2026-02-10T16:40:23.557635854Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","ondisk"],"comments":[{"id":15,"issue_id":"bd-2cn","author":"QuietFalcon","text":"In progress: added ext4 superblock parsing for s_log_cluster_size / s_clusters_per_group (with computed cluster_size), introduced typed wrappers for ext4 feature flag sets (compat/incompat/ro_compat) and updated validate_v1 to use them. Generating a real mkfs.ext4 superblock fixture (4096-byte blocks) and wiring it into ffs-harness conformance tests next; then running full cargo gates.","created_at":"2026-02-10T15:57:10Z"},{"id":16,"issue_id":"bd-2cn","author":"QuietFalcon","text":"Completed: superblock parsing now includes s_log_cluster_size + computed cluster_size and s_clusters_per_group; feature flag fields are wrapped in typed structs (compat/incompat/ro_compat) preserving raw bits; added real mkfs.ext4 fixture  and harness test coverage. Gates: cargo fmt --check, cargo check --all-targets, cargo clippy --all-targets -- -D warnings, cargo test --workspace (all pass).","created_at":"2026-02-10T16:39:51Z"},{"id":17,"issue_id":"bd-2cn","author":"QuietFalcon","text":"Note: fixture path is `conformance/fixtures/ext4_superblock_mkfs_4096.json` (previous comment lost it due to shell backticks).","created_at":"2026-02-10T16:40:16Z"}]}
{"id":"bd-2dk","title":"ext4: Implement directory entry parsing (ext4_dir_entry_2)","description":"Goal: parse ext4 directory blocks into entries for name lookup and readdir.\n\nDeliverables:\n- Parse ext4_dir_entry_2: inode(u32), rec_len(u16), name_len(u8), file_type(u8), name bytes.\n- Validate rec_len alignment and bounds within the directory block.\n- Expose an iterator that walks entries in a block without allocations where possible.\n\nAcceptance:\n- Unit tests cover: empty entry, last entry, invalid rec_len (zero, overflow), name_len > rec_len.\n- Fixture test parses a real directory block and matches debugfs listing.","status":"closed","priority":1,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:17:38.171911631Z","created_by":"ubuntu","updated_at":"2026-02-10T16:22:37.417436876Z","closed_at":"2026-02-10T16:22:37.417417891Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","ondisk"]}
{"id":"bd-2ds","title":"Docs Gate: Re-audit docs for contradictions + produce current Errata list","description":"Purpose: re-check the current versions of COMPREHENSIVE_SPEC_FOR_FRANKENFS_V1.md / PROPOSED_ARCHITECTURE.md / PLAN_TO_PORT_FRANKENFS_TO_RUST.md / FEATURE_PARITY.md / AGENTS.md / README.md for internal contradictions and doc-vs-code drift.\\n\\nWhy: implementation is unsafe if normative types/traits/crate boundaries disagree across docs. This task produces the *current* (not historical) Errata list used to drive follow-up fixes.\\n\\nDeliverables:\\n- A short Errata section added to COMPREHENSIVE_SPEC_FOR_FRANKENFS_V1.md (or a dedicated doc) listing each mismatch with file+anchor and the intended resolution.\\n- A mechanical checklist for future audits (rg patterns + invariants to verify).\\n\\nAcceptance:\\n- Errata list is complete enough that every other docs-fix issue below can reference it.\\n- No 'unknown unknowns' in the top-level normative contracts (types, errors, core traits, crate map).","status":"closed","priority":0,"issue_type":"task","assignee":"codex","created_at":"2026-02-10T03:12:31.044783552Z","created_by":"ubuntu","updated_at":"2026-02-10T06:45:48.220996449Z","closed_at":"2026-02-10T06:45:48.220978175Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs"]}
{"id":"bd-2f5","title":"Docs: Reconcile README mount/status claims with actual implementation","description":"Scope:\n- Audit README sections that conflict on mountability/status (for example, \"not yet mountable\" versus ext4 read-only mount support).\n- Align wording with current implementation boundaries: ext4 read-only mount available/experimental, btrfs mount unavailable.\n- Keep statements consistent with FEATURE_PARITY.md and ffs-cli behavior.\n\nAcceptance:\n- README has no internal contradiction about mount support.\n- Status/limitations/FAQ language is internally consistent and technically accurate.\n- cargo fmt --check and cargo test -p ffs-harness parity_markdown_is_in_sync_with_current_report pass after edits.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T16:21:12.362933613Z","created_by":"ubuntu","updated_at":"2026-02-11T16:24:20.885536362Z","closed_at":"2026-02-11T16:24:20.885518699Z","close_reason":"README mount/status consistency patched and validated","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs","parity"]}
{"id":"bd-2fa","title":"Track: Image I/O + Block Cache (ffs-block)","description":"Provide the Cx-aware ByteDevice/BlockDevice abstraction plus metadata cache. This is the performance-critical hot path for parsing, MVCC, and repair.\\n\\nAcceptance: safe superblock reads by fixed offsets, block-aligned geometry validation, deterministic cancellation behavior via &Cx checkpoints, and an ARC-like metadata cache with fixtures + benchmarks. Write-back is phased; start with read-cache correctness.","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-02-10T03:08:30.524700957Z","created_by":"ubuntu","updated_at":"2026-02-10T21:07:03.257052603Z","closed_at":"2026-02-10T21:07:03.257027356Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2fa","depends_on_id":"bd-265","type":"blocks","created_at":"2026-02-10T03:16:17.593253994Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2fa","depends_on_id":"bd-5iz","type":"blocks","created_at":"2026-02-10T03:16:17.379484681Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2fa","depends_on_id":"bd-ece","type":"blocks","created_at":"2026-02-10T03:16:17.483567687Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":4,"issue_id":"bd-2fa","author":"Dicklesworthstone","text":"ffs-block is the hot path.\n\nRules of engagement:\n- Every public I/O method takes &Cx for cancellation.\n- No disk I/O while holding global locks.\n- Cache design must be benchmarked only after fixtures prove correctness.\n\nThis track feeds parsing, MVCC, repair, and FUSE latency.","created_at":"2026-02-10T03:34:38Z"}]}
{"id":"bd-2fs","title":"CLI: Switch to clap-based structured CLI (inspect/mount/fsck/stats)","description":"Goal: move from ad-hoc env::args parsing to a structured CLI.\n\nDeliverables:\n- Use clap derive.\n- Subcommands: inspect, mount, fsck/scrub, parity, stats.\n- Ensure --json output is stable for automation.\n\nAcceptance:\n- Help output is clear and matches README.\n- Existing inspect behavior preserved.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:29:35.569588551Z","created_by":"ubuntu","updated_at":"2026-02-10T21:04:18.045099216Z","closed_at":"2026-02-10T21:04:18.045081073Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli"]}
{"id":"bd-2fy","title":"ffs-core: Implement ParseError -> FfsError mapping with context","description":"Goal: when pure parsers return ParseError, higher layers must convert to FfsError with enough context to be actionable.\n\nDeliverables:\n- A function (or trait) in ffs-core: map_parse_error(context, ParseError) -> FfsError.\n- Context includes: which structure (ext4 superblock vs group desc vs inode), what offset/group/inode number, etc.\n- Ensure mapping preserves details without leaking huge byte dumps.\n\nAcceptance:\n- CLI inspect prints useful errors.\n- FUSE mount returns stable errno plus a clear log message.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-10T03:20:27.019823860Z","created_by":"ubuntu","updated_at":"2026-02-10T17:04:44.506835168Z","closed_at":"2026-02-10T17:04:44.506816042Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["core"],"dependencies":[{"issue_id":"bd-2fy","depends_on_id":"bd-126","type":"blocks","created_at":"2026-02-10T03:20:48.558474490Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2fy","depends_on_id":"bd-owq","type":"blocks","created_at":"2026-02-10T03:20:48.478581732Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2hm","title":"ext4 semantics: Implement readdir (ffs-dir) via dir_entry_2 parsing","description":"Goal: list directory entries for a directory inode.\n\nDeliverables:\n- ffs-dir: read_dir(inode_no) -> Vec<DirEntry> or iterator.\n- Read directory data blocks via extent mapping (or direct blocks for very small dirs if needed).\n- Parse each directory block using ext4_dir_entry_2 parser.\n\nAcceptance:\n- Fixture test: root directory (inode 2) entries match expected set.\n- Handles malformed entries defensively (skips or errors with context, per policy).","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:25:52.386775767Z","created_by":"ubuntu","updated_at":"2026-02-10T19:39:44.917174735Z","closed_at":"2026-02-10T19:39:44.917157112Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","semantics"],"dependencies":[{"issue_id":"bd-2hm","depends_on_id":"bd-10t","type":"blocks","created_at":"2026-02-10T03:27:33.859471361Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hm","depends_on_id":"bd-2dk","type":"blocks","created_at":"2026-02-10T03:27:33.563555113Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2hm","depends_on_id":"bd-ye4","type":"blocks","created_at":"2026-02-10T03:27:33.719070546Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2ij","title":"Harness: Define Linux-kernel reference capture strategy (ext4/btrfs)","description":"Goal: prove behavioral parity by comparing FrankenFS outputs against a trusted reference.\n\nOptions:\n- Use kernel tools (dumpe2fs/debugfs/btrfs inspect-internal) to produce golden JSON.\n- Or mount images via kernel FS driver and observe syscalls/metadata.\n\nDeliverables:\n- Choose which behaviors are compared (superblock fields, inode stat, directory listing, extent mapping).\n- Define a reproducible capture pipeline that produces versioned golden outputs.\n\nAcceptance:\n- At least one end-to-end conformance test exists that compares FrankenFS output to kernel-derived golden output.","status":"closed","priority":2,"issue_type":"task","owner":"PinkCreek","created_at":"2026-02-10T03:21:45.455326215Z","created_by":"ubuntu","updated_at":"2026-02-11T02:38:51.625512548Z","closed_at":"2026-02-11T02:38:51.625490867Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["harness"]}
{"id":"bd-2io","title":"Implement ArcCache dirty tracking and flush-on-sync scaffold","description":"Add dirty block metadata to ArcCache, flush dirty blocks during sync/eviction without changing write-through semantics, add tests, and update parity/spec docs if needed.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-12T08:33:56.018300674Z","created_by":"ubuntu","updated_at":"2026-02-12T08:40:46.522263504Z","closed_at":"2026-02-12T08:40:46.522181200Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-2jk","title":"EPIC: Conformance & Quality Infrastructure","description":"# EPIC: Conformance & Quality Infrastructure\n\n## PURPOSE\nEstablish the testing, benchmarking, golden-output, and E2E infrastructure that enables confident iteration on all other work.\n\nThis epic is foundational: without conformance fixtures, goldens, and reproducible E2E runs, we cannot prove correctness or detect regressions.\n\n## BACKGROUND\nFrom AGENTS.md: \"FrankenFS must include fixture-based conformance tests (goldens for ext4/btrfs metadata behavior), benchmark suite with baselines and regression detection, feature parity report with explicit percentages.\"\n\nThe current state has:\n- `ffs-harness` with `ParityReport` and sparse fixtures\n- Criterion benchmark scaffolding\n- Some unit tests for core crates\n\nWhat‚Äôs missing is the user-facing, reproducible contract:\n- real ext4/btrfs images\n- golden outputs with checksums\n- CI gates\n- E2E scripts with detailed logging and cleanup guarantees\n\n## SCOPE (WHAT THIS EPIC MUST DELIVER)\n1. Fixture images (ext4 + btrfs) with documented, reproducible contents.\n2. Golden outputs for `ffs inspect` (at minimum), checksummed.\n3. CI gate that fails on golden checksum mismatch.\n4. Property-based tests for parsers (proptest) that validate invariants and ‚Äúno panics on adversarial bytes‚Äù.\n5. Performance baselines (hyperfine) + documented regression thresholds.\n6. E2E smoke tests that exercise the user workflows and produce actionable logs + artifacts.\n\n## ACCEPTANCE CRITERIA\n1. Golden output fixtures for ext4 inspect on 3+ real images\n2. Golden output fixtures for btrfs inspect on 3+ real images\n3. Hyperfine baseline measurements for all CLI commands\n4. Property-based tests for parsing invariants (proptest)\n5. CI gate that fails on golden checksum mismatch\n6. sha256sum-based verification workflow documented\n\n## DEPENDENCIES\nNone. This epic is the dependency root for most other work.\n\n## RATIONALE\nPer \"Extreme Software Optimization\": must have baselines before any optimization work.\nPer \"Porting to Rust\": conformance harness is the arbiter, not vibes.\nPer AGENTS.md: \"No hand-wavy done claims without tests, metrics, and parity evidence.\"\n\n## RELATED SPEC SECTIONS\n- COMPREHENSIVE_SPEC_FOR_FRANKENFS_V1.md ¬ß7 (Conformance Harness)\n- COMPREHENSIVE_SPEC_FOR_FRANKENFS_V1.md ¬ß8 (Benchmarking)\n\n## Success Criteria\n1. A new developer can run a single E2E command and get a PASS/FAIL with logs: `scripts/e2e/ffs_smoke.sh`.\n2. CI fails on behavior changes unless goldens are explicitly updated via a scripted workflow.\n3. Fixture generation is documented, repeatable, and produces identical goldens on the same toolchain.\n4. Parser invariants are enforced by property tests and failures are easy to reproduce (seed + minimized case logged).\n5. Baselines are recorded with environment metadata and regression thresholds are explicit.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-12T14:54:28.559990135Z","created_by":"ubuntu","updated_at":"2026-02-12T20:57:43.898231008Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-2jk.1","title":"Create ext4 golden fixture images","description":"# Create ext4 golden fixture images\n\n## GOAL\nGenerate 3+ real ext4 filesystem images with known, reproducible contents to serve as golden conformance fixtures.\n\n## CONTEXT\nThe conformance harness needs real filesystem images to validate parsing correctness. These fixtures must be reproducible so that a developer can regenerate them and obtain identical golden outputs on the same toolchain.\n\n## CANONICAL PATHS (USE THESE EVERYWHERE)\n- Images: `tests/fixtures/images/*.img`\n- Goldens: `tests/fixtures/golden/*.json`\n- Checksums: `tests/fixtures/golden/checksums.txt`\n\n## DELIVERABLES\n1. A generator script with detailed logging:\n   - `scripts/fixtures/make_ext4_fixtures.sh`\n2. A short, human-readable layout doc:\n   - `tests/fixtures/README.md` (or `scripts/fixtures/README.md`)\n3. Fixture images:\n   - `tests/fixtures/images/ext4_small.img` (~16MiB)\n   - `tests/fixtures/images/ext4_medium.img` (~64MiB)\n   - `tests/fixtures/images/ext4_large.img` (~128MiB)\n4. Golden outputs (inspect JSON):\n   - `tests/fixtures/golden/ext4_small.json`\n   - `tests/fixtures/golden/ext4_medium.json`\n   - `tests/fixtures/golden/ext4_large.json`\n5. Update `tests/fixtures/golden/checksums.txt` to include the new goldens.\n\n## IMPLEMENTATION APPROACH\n\n### Step 1: Create images\n```bash\ndd if=/dev/zero of=tests/fixtures/images/ext4_small.img bs=1M count=16\nmkfs.ext4 -F -O extent,filetype tests/fixtures/images/ext4_small.img\n\ndd if=/dev/zero of=tests/fixtures/images/ext4_medium.img bs=1M count=64\nmkfs.ext4 -F -O extent,filetype,dir_index tests/fixtures/images/ext4_medium.img\n\ndd if=/dev/zero of=tests/fixtures/images/ext4_large.img bs=1M count=128\nmkfs.ext4 -F -O extent,filetype,dir_index,sparse_super tests/fixtures/images/ext4_large.img\n```\n\n### Step 2: Populate with known structure (PREFERRED: rootless)\nPrefer `debugfs` so CI and developers don‚Äôt need `sudo mount`:\n- create:\n  - `/README.txt`\n  - `/dir1/file1.bin` (binary)\n  - `/dir1/dir2/file2.txt`\n  - `/symlink -> dir1/file1.bin`\n- set at least one xattr (document exact key/value)\n\nIf `debugfs` cannot set the desired attributes, use a privileged fallback:\n```bash\nsudo mount -o loop tests/fixtures/images/ext4_small.img /mnt/test\n# populate...\nsudo umount /mnt/test\n```\n\n### Step 3: Capture goldens + checksums\n```bash\ncargo run -p ffs-cli --release -- inspect tests/fixtures/images/ext4_small.img --json > tests/fixtures/golden/ext4_small.json\ncargo run -p ffs-cli --release -- inspect tests/fixtures/images/ext4_medium.img --json > tests/fixtures/golden/ext4_medium.json\ncargo run -p ffs-cli --release -- inspect tests/fixtures/images/ext4_large.img --json > tests/fixtures/golden/ext4_large.json\n\n( cd tests/fixtures/golden && sha256sum *.json > checksums.txt )\n```\n\n## LOGGING REQUIREMENTS\nThe generator script must print:\n- tool versions (`mkfs.ext4 -V`, `debugfs -V`)\n- every command executed\n- start/end timestamps\n- paths of all artifacts produced\n\n## VERIFICATION\n```bash\n( cd tests/fixtures/golden && sha256sum -c checksums.txt )\n```\n\n## ACCEPTANCE CRITERIA\n1. Three ext4 images exist with different feature combinations.\n2. Each image has documented contents and a deterministic generator workflow.\n3. Golden JSON outputs exist for all three images.\n4. Checksums file verifies cleanly.\n5. `scripts/e2e/ffs_smoke.sh` (bd-2jk.6) can use these fixtures.\n\n## NOTES\n- Keep fixtures small but representative; avoid unnecessary bloat.\n- If fixture generation requires `sudo`, that must be clearly documented and isolated in the script.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-12T14:54:44.142980605Z","created_by":"ubuntu","updated_at":"2026-02-12T21:11:02.555886712Z","closed_at":"2026-02-12T21:11:02.555819536Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2jk.1","depends_on_id":"bd-2jk","type":"parent-child","created_at":"2026-02-12T14:54:44.142980605Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2jk.2","title":"Create btrfs golden fixture images","description":"# Create btrfs golden fixture images\n\n## GOAL\nGenerate 3+ real btrfs filesystem images with known contents to serve as golden test fixtures.\n\n## CONTEXT\nbtrfs parsing is at 35% parity. We need real images to validate superblock decode, btree header/leaf parsing, and sys_chunk mapping. Single-device images only (multi-device out of V1 scope).\n\n## IMPLEMENTATION APPROACH\n\n### Step 1: Create test images using mkfs.btrfs\n```bash\n# Create different size images\ndd if=/dev/zero of=fixtures/btrfs_small.img bs=1M count=256  # btrfs needs minimum ~256MB\nmkfs.btrfs -f fixtures/btrfs_small.img\n\ndd if=/dev/zero of=fixtures/btrfs_medium.img bs=1M count=512\nmkfs.btrfs -f -O no-holes fixtures/btrfs_medium.img\n\ndd if=/dev/zero of=fixtures/btrfs_large.img bs=1M count=1024\nmkfs.btrfs -f fixtures/btrfs_large.img\n```\n\n### Step 2: Populate with known structure\n```bash\nsudo mount -o loop fixtures/btrfs_small.img /mnt/test\n# Create subvolume, files, directories\nbtrfs subvolume create /mnt/test/subvol1\n# Similar hierarchy to ext4 for cross-comparison\nsudo umount /mnt/test\n```\n\n### Step 3: Capture golden output\n```bash\ncargo run -p ffs-cli --release -- inspect fixtures/btrfs_small.img --json > golden/btrfs_small.json\nsha256sum golden/btrfs_small.json >> golden/checksums.txt\n```\n\n## FILES TO CREATE\n- tests/fixtures/images/btrfs_small.img (256MB)\n- tests/fixtures/images/btrfs_medium.img (512MB)  \n- tests/fixtures/images/btrfs_large.img (1GB)\n- tests/fixtures/golden/btrfs_small.json\n- tests/fixtures/golden/btrfs_medium.json\n- tests/fixtures/golden/btrfs_large.json\n\n## VERIFICATION\nMust pass: `sha256sum -c tests/fixtures/golden/checksums.txt`\n\n## NOTE ON STORAGE\nImages are large. Consider:\n1. .gitignore for images, git-lfs for >100MB\n2. Script to regenerate from scratch\n3. Compressed archives in repo\n\n## ACCEPTANCE CRITERIA\n1. Three btrfs single-device images\n2. Different feature combinations tested\n3. Golden JSON outputs captured\n4. Verification script passes","status":"in_progress","priority":2,"issue_type":"task","created_at":"2026-02-12T14:54:57.010840906Z","created_by":"ubuntu","updated_at":"2026-02-12T21:11:24.466464468Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2jk.2","depends_on_id":"bd-2jk","type":"parent-child","created_at":"2026-02-12T14:54:57.010840906Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2jk.3","title":"Establish hyperfine performance baselines","description":"# Establish hyperfine performance baselines\n\n## GOAL\nRecord baseline performance measurements for all CLI commands using hyperfine, establishing regression detection thresholds.\n\n## CONTEXT\nFrom \"Extreme Software Optimization\" skill: \"Profile first. Prove behavior unchanged. One change at a time.\"\nNo optimization work can be validated without baselines.\n\n## IMPLEMENTATION APPROACH\n\n### Step 1: Install hyperfine if needed\n```bash\ncargo install hyperfine\n```\n\n### Step 2: Measure all CLI operations\n```bash\n# Inspect command (most common path)\nhyperfine --warmup 3 --runs 10 --export-json bench/inspect_ext4_small.json \\\n  'cargo run -p ffs-cli --release -- inspect fixtures/ext4_small.img --json'\n\nhyperfine --warmup 3 --runs 10 --export-json bench/inspect_btrfs_small.json \\\n  'cargo run -p ffs-cli --release -- inspect fixtures/btrfs_small.img --json'\n\n# Scrub command  \nhyperfine --warmup 3 --runs 10 --export-json bench/scrub_ext4_small.json \\\n  'cargo run -p ffs-cli --release -- scrub fixtures/ext4_small.img --json'\n\n# Parity report\nhyperfine --warmup 3 --runs 10 --export-json bench/parity.json \\\n  'cargo run -p ffs-cli --release -- parity --json'\n```\n\n### Step 3: Create baseline summary\nDocument in `BENCHMARKS.md`:\n- Mean, stddev, min, max for each command\n- Hardware spec (CPU, memory, disk type)\n- Date recorded\n- Git commit hash\n\n## FILES TO CREATE\n- bench/baselines/inspect_ext4_small.json\n- bench/baselines/inspect_btrfs_small.json\n- bench/baselines/scrub_ext4_small.json\n- bench/baselines/parity.json\n- BENCHMARKS.md (baseline documentation)\n\n## REGRESSION THRESHOLD\n- Alert if p95 regresses >10% from baseline\n- Block merge if p95 regresses >25%\n\n## ACCEPTANCE CRITERIA\n1. Hyperfine JSON for all CLI commands\n2. BENCHMARKS.md with hardware spec + commit hash\n3. CI step that compares against baselines\n4. Clear threshold for regression alerts\n\n## DEPENDENCIES\n- Requires golden fixture images (bd-2jk.1, bd-2jk.2)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-12T14:55:09.487500812Z","created_by":"ubuntu","updated_at":"2026-02-12T14:55:31.560247381Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2jk.3","depends_on_id":"bd-2jk","type":"parent-child","created_at":"2026-02-12T14:55:09.487500812Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2jk.3","depends_on_id":"bd-2jk.1","type":"blocks","created_at":"2026-02-12T14:55:31.472893446Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2jk.3","depends_on_id":"bd-2jk.2","type":"blocks","created_at":"2026-02-12T14:55:31.560208959Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2jk.4","title":"Add proptest property-based tests for parsing","description":"# Add proptest property-based tests for parsing\n\n## GOAL\nImplement property-based tests using proptest for all ffs-ondisk parsing functions to catch edge cases and parsing invariants.\n\n## CONTEXT\nFrom AGENTS.md: \"Deterministic testability. Concurrency-sensitive logic must be testable.\"\nProperty-based testing catches edge cases that unit tests miss - malformed inputs, boundary conditions, integer overflows.\n\n## IMPLEMENTATION APPROACH\n\n### Properties to Test\n\n#### ext4 superblock parsing\n```rust\nproptest! {\n    #[test]\n    fn superblock_roundtrip_invariant(bytes in prop::collection::vec(any::<u8>(), 1024)) {\n        // If parsing succeeds, parsed values must be within valid ranges\n        if let Ok(sb) = Ext4Superblock::parse(&bytes) {\n            prop_assert!(sb.s_block_size().is_power_of_two());\n            prop_assert!(sb.s_block_size() >= 1024);\n            prop_assert!(sb.s_block_size() <= 65536);\n        }\n    }\n}\n```\n\n#### btrfs superblock parsing\n```rust\nproptest! {\n    #[test]\n    fn btrfs_superblock_magic_or_reject(bytes in prop::collection::vec(any::<u8>(), 4096)) {\n        // Either parses with valid magic, or rejects\n        match BtrfsSuperblock::parse(&bytes) {\n            Ok(sb) => prop_assert_eq!(sb.magic, BTRFS_MAGIC),\n            Err(_) => (), // rejection is fine\n        }\n    }\n}\n```\n\n#### extent parsing\n```rust\nproptest! {\n    #[test]\n    fn extent_entries_fit_in_header_count(bytes in prop::collection::vec(any::<u8>(), 12..4096)) {\n        if let Ok(header) = ExtentHeader::parse(&bytes) {\n            // eh_entries must not exceed eh_max\n            prop_assert!(header.eh_entries <= header.eh_max);\n        }\n    }\n}\n```\n\n### Strategy Generators\n- `arb_ext4_superblock_bytes()`: Valid superblock with fuzzed non-critical fields\n- `arb_btrfs_superblock_bytes()`: Valid btrfs superblock structure\n- `arb_extent_tree()`: Valid extent tree with random depth\n\n## FILES TO MODIFY\n- crates/ffs-ondisk/Cargo.toml (add proptest dev-dependency)\n- crates/ffs-ondisk/src/ext4/tests.rs (or new file)\n- crates/ffs-ondisk/src/btrfs/tests.rs\n\n## ACCEPTANCE CRITERIA\n1. At least 10 property tests for ext4 parsing\n2. At least 10 property tests for btrfs parsing\n3. All tests pass with 1000 iterations\n4. No shrinking failures (if any fail, fix the parser)\n5. Document which invariants each test validates\n\n## RATIONALE\nPer alien-artifact-coding: \"Graceful degradation - Works with whatever data is available; never fails catastrophically.\"\nProperty tests verify parsers handle adversarial/corrupted input safely.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-12T14:55:26.960730308Z","created_by":"ubuntu","updated_at":"2026-02-12T14:55:26.960730308Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2jk.4","depends_on_id":"bd-2jk","type":"parent-child","created_at":"2026-02-12T14:55:26.960730308Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2jk.5","title":"Implement CI gate for golden checksum verification","description":"# Implement CI gate for golden checksum verification\n\n## GOAL\nCreate a CI workflow that automatically fails if golden output checksums don't match, preventing regressions from being merged.\n\n## CONTEXT\nFrom AGENTS.md acceptance criteria for conformance: \"CI gate that fails on golden checksum mismatch.\"\n\nThis is CRITICAL infrastructure - without it, regressions can slip in undetected.\n\n## IMPLEMENTATION APPROACH\n\n### Step 1: Create verification script\n```bash\n#!/bin/bash\n# scripts/verify-goldens.sh\nset -euo pipefail\n\nGOLDEN_DIR=\"tests/fixtures/golden\"\nCHECKSUM_FILE=\"$GOLDEN_DIR/checksums.txt\"\n\necho \"=== Golden Output Verification ===\"\necho \"Timestamp: $(date -Iseconds)\"\necho \"Git commit: $(git rev-parse HEAD)\"\necho \"\"\n\n# Check checksums file exists\nif [[ ! -f \"$CHECKSUM_FILE\" ]]; then\n    echo \"ERROR: Checksum file not found: $CHECKSUM_FILE\"\n    exit 1\nfi\n\n# Regenerate outputs\necho \"Regenerating golden outputs...\"\nfor img in tests/fixtures/images/*.img; do\n    name=$(basename \"$img\" .img)\n    echo \"  Processing: $name\"\n    cargo run -p ffs-cli --release -- inspect \"$img\" --json > \"$GOLDEN_DIR/${name}_current.json\" 2>&1\ndone\n\n# Verify checksums\necho \"\"\necho \"Verifying checksums...\"\ncd \"$GOLDEN_DIR\"\nif sha256sum -c checksums.txt; then\n    echo \"\"\n    echo \"‚úÖ All golden outputs match!\"\n    exit 0\nelse\n    echo \"\"\n    echo \"‚ùå GOLDEN OUTPUT MISMATCH DETECTED\"\n    echo \"\"\n    echo \"This means behavior has changed. Either:\"\n    echo \"1. A regression was introduced (fix the code)\"\n    echo \"2. Intentional change (update goldens with: scripts/update-goldens.sh)\"\n    exit 1\nfi\n```\n\n### Step 2: Create GitHub Actions workflow\n```yaml\n# .github/workflows/ci.yml\nname: CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\nenv:\n  CARGO_TERM_COLOR: always\n  RUST_BACKTRACE: 1\n\njobs:\n  lint-and-test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Install Rust toolchain\n        uses: dtolnay/rust-action@stable\n        with:\n          toolchain: nightly\n          components: rustfmt, clippy\n      \n      - name: Cache cargo\n        uses: Swatinem/rust-cache@v2\n      \n      - name: Format check\n        run: cargo fmt --check\n      \n      - name: Clippy\n        run: cargo clippy --all-targets -- -D warnings\n      \n      - name: Build\n        run: cargo build --workspace\n      \n      - name: Unit tests\n        run: cargo test --workspace -- --nocapture\n      \n      - name: Golden verification\n        run: ./scripts/verify-goldens.sh\n      \n      - name: Benchmark (no regression check)\n        run: cargo bench -p ffs-harness -- --noplot\n\n  golden-update-check:\n    runs-on: ubuntu-latest\n    if: github.event_name == 'pull_request'\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Check for golden changes\n        run: |\n          if git diff --name-only origin/main...HEAD | grep -q \"tests/fixtures/golden\"; then\n            echo \"::warning::Golden outputs modified - ensure this is intentional\"\n          fi\n```\n\n### Step 3: Create golden update script\n```bash\n#!/bin/bash\n# scripts/update-goldens.sh\nset -euo pipefail\n\necho \"=== Updating Golden Outputs ===\"\necho \"WARNING: This will overwrite existing goldens!\"\nread -p \"Continue? [y/N] \" confirm\n[[ \"$confirm\" =~ ^[Yy]$ ]] || exit 1\n\nGOLDEN_DIR=\"tests/fixtures/golden\"\n\n# Regenerate all goldens\nfor img in tests/fixtures/images/*.img; do\n    name=$(basename \"$img\" .img)\n    echo \"Generating: $name\"\n    cargo run -p ffs-cli --release -- inspect \"$img\" --json > \"$GOLDEN_DIR/${name}.json\"\ndone\n\n# Update checksums\necho \"Updating checksums...\"\ncd \"$GOLDEN_DIR\"\nsha256sum *.json > checksums.txt\n\necho \"\"\necho \"‚úÖ Goldens updated. Don't forget to:\"\necho \"   git add tests/fixtures/golden/\"\necho \"   git commit -m 'chore: update golden outputs'\"\n```\n\n## FILES TO CREATE\n- scripts/verify-goldens.sh\n- scripts/update-goldens.sh\n- .github/workflows/ci.yml\n\n## LOGGING REQUIREMENTS\nAll scripts must:\n1. Print timestamp at start\n2. Print git commit hash\n3. Print each step being executed\n4. Print clear SUCCESS/FAILURE status\n5. Exit with appropriate code (0=success, 1=failure)\n\n## ACCEPTANCE CRITERIA\n1. CI workflow runs on every PR\n2. CI fails if checksums mismatch\n3. Clear error message explains what to do\n4. Update script exists for intentional changes\n5. PR shows warning if goldens modified\n\n## UNIT TESTS\n```rust\n// In ffs-harness/src/lib.rs\n#[test]\nfn test_golden_checksum_verification() {\n    let golden_dir = Path::new(\"tests/fixtures/golden\");\n    let checksum_file = golden_dir.join(\"checksums.txt\");\n    \n    assert!(checksum_file.exists(), \"Checksum file must exist\");\n    \n    // Parse checksum file\n    let contents = std::fs::read_to_string(&checksum_file).unwrap();\n    for line in contents.lines() {\n        let parts: Vec<&str> = line.split_whitespace().collect();\n        assert_eq!(parts.len(), 2, \"Invalid checksum line format\");\n        \n        let (expected_hash, filename) = (parts[0], parts[1]);\n        let file_path = golden_dir.join(filename);\n        \n        assert!(file_path.exists(), \"Golden file missing: {}\", filename);\n        \n        // Verify hash\n        let actual_hash = sha256_file(&file_path);\n        assert_eq!(actual_hash, expected_hash, \n            \"Checksum mismatch for {}\", filename);\n    }\n}\n```\n\n## DEPENDENCIES\n- ext4 golden fixtures (bd-2jk.1)\n- btrfs golden fixtures (bd-2jk.2)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-12T20:45:05.756835188Z","created_by":"ubuntu","updated_at":"2026-02-12T20:53:17.200156594Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2jk.5","depends_on_id":"bd-2jk","type":"parent-child","created_at":"2026-02-12T20:45:05.756835188Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2jk.5","depends_on_id":"bd-2jk.1","type":"blocks","created_at":"2026-02-12T20:53:17.153818837Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2jk.5","depends_on_id":"bd-2jk.2","type":"blocks","created_at":"2026-02-12T20:53:17.200085Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2jk.6","title":"Add E2E smoke tests (CLI + ext4 RO mount) with detailed logging","description":"# Add E2E smoke tests (CLI + ext4 RO mount) with detailed logging\n\n## GOAL\nAdd reproducible end-to-end test scripts that exercise the user-facing workflows:\n- `ffs inspect`\n- `ffs scrub`\n- `ffs parity`\n- `ffs mount` (ext4 read-only) + basic file operations through the mount\n\nThe scripts must produce high-quality logs so failures are actionable.\n\n## CONTEXT\nWe already have good unit tests and some harness tests, but we are missing a single-command black-box validation that:\n1. Builds the workspace\n2. Runs the CLI on real fixture images\n3. Mounts via FUSE and validates basic behavior\n4. Captures structured logs + environment info\n\nThis is the fastest way for humans (and CI) to detect regressions.\n\n## DELIVERABLES\n1. `scripts/e2e/ffs_smoke.sh`\n2. `scripts/e2e/lib.sh` (shared helpers: logging, cleanup, assertions)\n3. `scripts/e2e/README.md` (how to run locally + troubleshooting)\n4. Optional CI hook-up (either in bd-2jk.5 or a follow-up bead): run when `/dev/fuse` is available\n\n## SCRIPT REQUIREMENTS (NON-NEGOTIABLE)\n- `set -euo pipefail`\n- `IFS=$'\\n\\t'`\n- `trap`-based cleanup that always unmounts and removes temp dirs\n- Log directory: `artifacts/e2e/<timestamp>/`\n- One combined log file: `artifacts/e2e/<timestamp>/run.log`\n- Every major step prints:\n  - Start/end timestamps\n  - Command line\n  - Exit code\n  - Duration\n- Always print environment header:\n  - `uname -a`\n  - `id`\n  - `rustc -Vv`, `cargo -V`\n  - `ls -l /dev/fuse` (if present)\n  - `mount --version` / `fusermount3 --version` if available\n- Always run with verbose Rust logs:\n  - `RUST_LOG=trace`\n  - `RUST_BACKTRACE=1`\n\n## TEST MATRIX\n### CLI\n1. `cargo build --workspace` (or `cargo build -p ffs-cli`)\n2. `cargo run -p ffs-cli -- inspect tests/fixtures/images/ext4_small.img --json`\n3. `cargo run -p ffs-cli -- scrub tests/fixtures/images/ext4_small.img --json`\n4. `cargo run -p ffs-cli -- parity --json`\n\n### FUSE ext4 read-only mount\n1. Mount: `cargo run -p ffs-cli -- mount tests/fixtures/images/ext4_small.img <mnt>`\n2. Validate:\n   - `ls -la <mnt>`\n   - `stat <mnt>`\n   - `find <mnt> -maxdepth 2 -type f -print` (bounded)\n   - `cat <mnt>/README.txt` (or whatever fixture defines)\n3. Unmount: `fusermount -u <mnt>` (fallback: `fusermount3 -u`)\n\n## ACCEPTANCE CRITERIA\n1. `scripts/e2e/ffs_smoke.sh` is deterministic and idempotent.\n2. On success, it exits 0 and prints a clear PASS summary.\n3. On failure, it exits non-zero and prints:\n   - failing command\n   - path to log directory\n   - last ~200 lines of the log\n4. Mount cleanup works even on failure (no leaked mounts).\n5. Script runs in <5 minutes on a typical dev laptop.\n\n## NOTES\n- This bead is ext4-only for mount because btrfs mount is not yet supported; btrfs E2E lives under bd-375.\n- Use the fixture‚Äôs documented file layout; do not hardcode paths that don‚Äôt exist.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-12T20:56:07.035282987Z","created_by":"ubuntu","updated_at":"2026-02-12T20:56:07.035282987Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2jk.6","depends_on_id":"bd-2jk","type":"parent-child","created_at":"2026-02-12T20:56:07.035282987Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2jk.6","depends_on_id":"bd-2jk.1","type":"blocks","created_at":"2026-02-12T20:56:07.035282987Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2l4","title":"Docs: Canonicalize normative trait definitions (BlockDevice/MVCC/Repair)","description":"Goal: ensure each major trait contract is defined once (normative), with stable signatures (including &Cx requirements) and stable semantic notes.\\n\\nScope:\\n- ffs-block traits: ByteDevice/BlockDevice (or equivalent)\\n- MVCC public traits (begin read tx, begin write tx, versioned read/write, commit/abort)\\n- Repair interfaces (scrub report, symbol generation, decode proof)\\n\\nDeliverables:\\n- Choose canonical trait locations (PROPOSED_ARCHITECTURE + COMPREHENSIVE_SPEC).\\n- Remove or mark 'illustrative' any alternate trait signatures that diverge.\\n- Ensure &asupersync::Cx usage is consistent (no ambient authority).\\n\\nAcceptance:\\n- No doc section defines a second incompatible trait signature.\\n- The crate that owns each trait is explicit (no 'phantom home crate').","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-10T03:12:46.237190866Z","created_by":"ubuntu","updated_at":"2026-02-11T01:47:53.293870581Z","closed_at":"2026-02-11T01:47:53.293847658Z","close_reason":"Canonicalized ByteDevice/BlockDevice + MVCC/Repair trait signatures across COMPREHENSIVE_SPEC+PROPOSED_ARCHITECTURE; removed divergent duplicates","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs"],"dependencies":[{"issue_id":"bd-2l4","depends_on_id":"bd-2ds","type":"blocks","created_at":"2026-02-10T03:14:05.375138586Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2oa","title":"ffs-error: Decide mount-validation error variants (UnsupportedFeature/UnsupportedBlockSize/etc)","description":"Context: mount validation can fail because of unsupported feature bits, block sizes, or incompatible on-disk geometry.\n\nQuestion: do we represent these as:\n- ParseError::InvalidField (and map to FfsError::Format), OR\n- dedicated FfsError variants (UnsupportedFeature, UnsupportedBlockSize, InvalidGeometry), OR\n- an intermediate MountValidationError used by ffs-core.\n\nDeliverables:\n- Chosen approach documented.\n- Error mapping yields correct errno (EINVAL vs EOPNOTSUPP) for FUSE and CLI.\n\nAcceptance:\n- ext4 validate_v1() failures are surfaced with stable errors and good UX.\n- Docs/spec do not list non-existent FfsError variants.","status":"closed","priority":1,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:14:59.187355332Z","created_by":"ubuntu","updated_at":"2026-02-10T15:53:56.423539784Z","closed_at":"2026-02-10T15:53:56.423519876Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["foundation"]}
{"id":"bd-2q7","title":"ext4 semantics: Implement file read (cat) using extent mapping","description":"Goal: read regular file data by mapping logical blocks to physical blocks and reading from BlockDevice.\n\nDeliverables:\n- ffs-inode or ffs-core: read_file(inode_no, offset,len) -> bytes.\n- Support partial-block reads.\n- Handle holes/unwritten extents conservatively (return zeroes or error, per policy).\n\nAcceptance:\n- Fixture test reads a known file and matches golden bytes.\n- Works through the same path used by FUSE read().","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:26:08.054881047Z","created_by":"ubuntu","updated_at":"2026-02-10T19:41:12.022257501Z","closed_at":"2026-02-10T19:41:12.022235780Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","semantics"],"dependencies":[{"issue_id":"bd-2q7","depends_on_id":"bd-10t","type":"blocks","created_at":"2026-02-10T03:27:34.205321398Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2q7","depends_on_id":"bd-ye4","type":"blocks","created_at":"2026-02-10T03:27:34.090645093Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2qf","title":"Repair: Extract FrankenSQLite RaptorQ approach and adapt to filesystem blocks","description":"Goal: reuse proven strategy from /dp/frankensqlite for fountain-code-based self-healing, but adapted to ext4/btrfs block groups and filesystem workloads.\n\nDeliverables:\n- Summarize the key FrankenSQLite design choices: symbol placement, overhead tuning, decode proofs, ledgering.\n- Identify what changes for filesystems: block group boundaries, metadata vs data, read/write patterns.\n- Update COMPREHENSIVE_SPEC repair section with an explicit design derived from this extraction.\n\nAcceptance:\n- We can explain exactly how a corrupted block is detected and which symbols are used to repair it.\n- The design includes tunable parameters and an expected-loss justification (not ad-hoc thresholds).","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:24:15.796807569Z","created_by":"ubuntu","updated_at":"2026-02-11T01:40:19.555409837Z","closed_at":"2026-02-11T01:40:19.555343092Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["repair"]}
{"id":"bd-2t1","title":"MVCC: Specify API + invariants (TxnId/CommitSeq/Snapshot) and phase plan (FCW -> SSI)","description":"Goal: make MVCC semantics unambiguous before integrating with filesystem operations.\n\nDeliverables:\n- Document: what a Snapshot means (visibility window), when it is taken, and how it is used for reads.\n- Document: transaction lifecycle (begin -> stage writes -> commit/abort) and what is allowed concurrently.\n- Phase plan:\n  - Phase A: First-committer-wins (FCW) conflict check at commit.\n  - Phase B: Add optional SSI tracking (read sets + rw-antidependencies) where needed.\n\nAcceptance:\n- ffs-mvcc public API is stable enough that ffs-fuse can build on it.\n- There is a simple correctness proof sketch for FCW and a roadmap for SSI.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:22:27.695269767Z","created_by":"ubuntu","updated_at":"2026-02-10T17:20:36.304894733Z","closed_at":"2026-02-10T17:20:36.304874716Z","close_reason":"Spec: clarify MVCC Snapshot/Transaction lifecycle; Phase A (FCW) vs Phase B (SSI) plan; document Phase A API","source_repo":".","compaction_level":0,"original_size":0,"labels":["mvcc"]}
{"id":"bd-2tq","title":"Semantics: Define a minimal VFS operations trait for FUSE/harness","description":"Goal: define the internal interface FUSE and harness call.\n\nDeliverables:\n- A trait (e.g. FsOps) with methods needed for read-only mount:\n  - getattr(inode)\n  - lookup(parent,name)\n  - readdir(inode)\n  - read(inode,offset,len)\n- Types for file attributes and directory entries.\n\nAcceptance:\n- ffs-fuse can implement the fuser::Filesystem methods by delegating to this trait.\n- ext4 implementation can live behind this interface (and later btrfs too).","status":"closed","priority":1,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:26:14.391246994Z","created_by":"ubuntu","updated_at":"2026-02-10T16:29:28.832170053Z","closed_at":"2026-02-10T16:29:28.832148523Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["semantics"]}
{"id":"bd-2uj","title":"Track: Performance Baselines + Regression Gates","description":"Establish measurable performance goals and prevent regressions via baselines + profiling discipline (per extreme-software-optimization).\\n\\nAcceptance: (1) hyperfine baselines exist for key paths (detect/open/parse ext4 superblock+groupdesc, btrfs superblock+map), (2) benchmark harness exists for cache hot paths, (3) optimization PRs require before/after numbers + isomorphism proof + golden checksums.","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-02-10T03:09:32.851180914Z","created_by":"ubuntu","updated_at":"2026-02-11T03:32:05.682177354Z","closed_at":"2026-02-11T03:32:05.682151305Z","close_reason":"All 3 sub-tasks complete: criterion benches (bd-3ff), golden/isomorphism proofs (bd-p1l), benchmark baselines (bd-37g).","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2uj","depends_on_id":"bd-37g","type":"blocks","created_at":"2026-02-10T03:31:02.651808806Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2uj","depends_on_id":"bd-3ff","type":"blocks","created_at":"2026-02-10T03:31:02.817592180Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2uj","depends_on_id":"bd-p1l","type":"blocks","created_at":"2026-02-10T03:31:02.735713072Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":14,"issue_id":"bd-2uj","author":"Dicklesworthstone","text":"Optimization is not a vibe.\n\nWe follow the loop:\n- baseline (hyperfine)\n- profile\n- change one lever\n- prove behavior unchanged (goldens + checksums)\n- re-measure\n\nThis track exists to keep performance claims honest and regressions detectable.","created_at":"2026-02-10T03:34:39Z"}]}
{"id":"bd-2vt","title":"btrfs: Expand superblock parsing (sys_chunk_array + validation)","description":"Goal: parse enough of btrfs superblock to bootstrap mapping without reading chunk tree yet.\n\nDeliverables:\n- Parse sys_chunk_array and sys_chunk_array_size fields.\n- Add validation for sectorsize/nodesize/stripesize: non-zero, power-of-two where required, and sane bounds.\n- Expose a helper to iterate sys_chunk_array entries.\n\nAcceptance:\n- Fixture-backed test: parse a real btrfs superblock and confirm sys_chunk_array size/contents are plausible.\n- No panics on malformed input.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:18:36.214652115Z","created_by":"ubuntu","updated_at":"2026-02-10T20:04:40.654597651Z","closed_at":"2026-02-10T20:04:40.654575529Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["btrfs","ondisk"]}
{"id":"bd-2w9","title":"ext4: Implement superblock geometry validation (group/inode math sanity)","description":"Goal: validate ext4 geometry beyond magic+block size, so later helpers (inode table math, group desc iteration) are safe.\n\nDeliverables:\n- Validate blocks_per_group and inodes_per_group are non-zero and within sane bounds.\n- Validate inode_size is supported (>=128, power-of-two-ish, and fits in block).\n- Validate desc_size is supported (>=32, <= block size, and consistent with 64BIT feature).\n- Validate first_data_block rules: for 1K blocks, first_data_block=1; for 2K/4K, typically 0 (document exact rule).\n- Compute group_count and validate group desc table length fits in device.\n\nAcceptance:\n- validate_v1_geometry() (or equivalent) exists with unit tests for edge cases.\n- No unchecked division by zero or overflow in downstream code.","status":"closed","priority":0,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:17:06.586128480Z","created_by":"ubuntu","updated_at":"2026-02-10T17:33:34.372291716Z","closed_at":"2026-02-10T17:33:34.372273762Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","ondisk"],"dependencies":[{"issue_id":"bd-2w9","depends_on_id":"bd-2cn","type":"blocks","created_at":"2026-02-10T03:18:06.614254690Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2yi","title":"Harness: Add btrfs fixtures for sys_chunk mapping + node parsing","description":"Goal: fixture-back btrfs mapping and node parsing work.\n\nDeliverables:\n- Add sparse fixtures for:\n  - superblock with non-empty sys_chunk_array\n  - at least 1 btrfs leaf node block (nodesize) with a few items\n- Add harness helpers to parse and validate:\n  - mapping of root/chunk_root logical addresses\n  - leaf item table bounds\n\nAcceptance:\n- btrfs mapping + node parse tasks have fixtures that fail if bounds checks regress.\n- Parity counts updated when coverage increases.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:21:38.058404262Z","created_by":"ubuntu","updated_at":"2026-02-10T20:21:04.997119020Z","closed_at":"2026-02-10T20:21:04.997097259Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["btrfs","harness"],"dependencies":[{"issue_id":"bd-2yi","depends_on_id":"bd-1fo","type":"blocks","created_at":"2026-02-10T03:22:05.881201973Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yi","depends_on_id":"bd-2vt","type":"blocks","created_at":"2026-02-10T03:22:05.800109863Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2zg","title":"Harness: Add ext4 fixtures for group descriptors + inode parsing","description":"Goal: extend conformance/fixtures beyond superblock so new ext4 parsing work is fixture-backed.\n\nDeliverables:\n- Add sparse fixtures for:\n  - at least 1 group descriptor (32-byte) and 1 group descriptor (64-byte)\n  - at least 1 inode (with extents) and 1 directory inode\n  - at least 1 directory block (dir_entry_2)\n- Add harness validation helpers that parse these fixtures and assert key fields.\n\nAcceptance:\n- New ext4 parsing tasks (group desc, inode parse, dirent parse) have at least one fixture each.\n- FEATURE_PARITY and ParityReport are updated in the same change when coverage increases.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-10T03:21:32.375616672Z","created_by":"ubuntu","updated_at":"2026-02-10T19:58:21.396958654Z","closed_at":"2026-02-10T19:58:21.396936793Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","harness"],"dependencies":[{"issue_id":"bd-2zg","depends_on_id":"bd-2dk","type":"blocks","created_at":"2026-02-10T03:22:05.715953282Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2zg","depends_on_id":"bd-3qq","type":"blocks","created_at":"2026-02-10T03:22:05.634893223Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-375","title":"EPIC: btrfs Read Path & FUSE Mount","description":"# EPIC: btrfs Read Path & FUSE Mount\n\n## PURPOSE\nEnable read-only btrfs FUSE mounting, achieving parity with ext4 read-only support. Currently btrfs metadata parsing is at 35% (7/20).\n\n## BACKGROUND\nFrom FEATURE_PARITY.md, btrfs gaps:\n- btrfs transaction parity: ‚ùå NOT IMPLEMENTED\n- btrfs delayed refs parity: ‚ùå NOT IMPLEMENTED\n- btrfs scrub parity: ‚ùå NOT IMPLEMENTED\n- FUSE mount for btrfs: NOT WORKING\n\nHowever, significant foundation exists:\n- Superblock decode: ‚úÖ\n- B-tree header decode: ‚úÖ\n- Leaf item metadata decode: ‚úÖ\n- Internal node parsing: ‚úÖ\n- sys_chunk mapping: ‚úÖ\n- Read-only tree walk: ‚úÖ\n- BtrfsContext in ffs-core: ‚úÖ\n\nThe remaining gap is wiring tree walking and extent resolution into `FsOps` and then into the FUSE surface.\n\n## CURRENT STATE\n- `ffs-ondisk` has comprehensive btrfs parsing\n- `ffs-btrfs` provides `walk_tree`\n- `ffs-core` has a `BtrfsContext` stub\n- `ffs-fuse` operations currently route to ext4-only `FsOps`\n\n## GAPS TO CLOSE\n1. Implement `BtrfsFsOps` (like `Ext4FsOps` but for btrfs)\n2. Wire `ffs-cli mount` path to btrfs ops\n3. Implement btrfs inode -> extent resolution sufficient for read-only file reads\n4. Add E2E coverage for btrfs mount\n\n## ACCEPTANCE CRITERIA\n1. `ffs mount btrfs.img /mnt` works read-only\n2. Basic file operations work: `ls`, `cat`, `stat`\n3. Subvolume visibility (at least default subvol)\n4. Performance baseline established (hyperfine) for inspect + mount smoke\n5. btrfs metadata parsing reaches 60%+ parity\n\n## DEPENDENCIES\n- EPIC: Conformance & Quality Infrastructure (bd-2jk) - need fixtures/goldens\n- btrfs golden fixtures (bd-2jk.2)\n\n## EXPLICIT V1 EXCLUSIONS (per PLAN_TO_PORT_FRANKENFS_TO_RUST.md)\n- Multi-device / RAID: OUT OF SCOPE\n- Compression: OUT OF SCOPE\n- Send/receive: OUT OF SCOPE\n- Write support: OUT OF SCOPE\n\n## RELATED SPEC SECTIONS\n- EXISTING_EXT4_BTRFS_STRUCTURE.md ¬ß4-6 (btrfs behavior)\n- COMPREHENSIVE_SPEC_FOR_FRANKENFS_V1.md ¬ß2 (btrfs format)\n\n## Success Criteria\n1. `ffs inspect <btrfs.img>` and `ffs mount <btrfs.img>` work on the golden fixtures without panics.\n2. `scripts/e2e/ffs_btrfs_ro_smoke.sh` passes (or skips cleanly when FUSE unavailable) and produces actionable logs.\n3. Unit tests cover:\n   - inode lookup by key\n   - directory enumeration ordering/offsets\n   - extent resolution for file reads","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-12T14:56:58.365480057Z","created_by":"ubuntu","updated_at":"2026-02-12T20:58:21.000305491Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-375","depends_on_id":"bd-2jk","type":"blocks","created_at":"2026-02-12T15:04:29.039027573Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-375.1","title":"Implement BtrfsFsOps for FUSE operations","description":"# Implement BtrfsFsOps for FUSE operations\n\n## GOAL\nCreate BtrfsFsOps implementing the FsOps trait to enable btrfs FUSE mounting.\n\n## CONTEXT\nffs-core defines FsOps trait with getattr, lookup, readdir, read, readlink, listxattr, getxattr.\nExt4FsOps exists and works. We need the btrfs equivalent.\n\n## IMPLEMENTATION APPROACH\n\n### BtrfsFsOps Structure\n```rust\n// In ffs-core/src/btrfs/ops.rs\npub struct BtrfsFsOps {\n    image: Arc<Vec<u8>>,  // Or Arc<dyn ByteDevice>\n    superblock: BtrfsSuperblock,\n    root_tree_root: u64,  // From superblock\n    fs_tree_root: u64,    // Looked up from root tree\n}\n\nimpl FsOps for BtrfsFsOps {\n    fn getattr(&self, cx: &Cx, ino: InodeNumber) -> Result<InodeAttr> {\n        // 1. Lookup INODE_ITEM in fs tree for ino\n        // 2. Convert to InodeAttr\n    }\n    \n    fn lookup(&self, cx: &Cx, parent: InodeNumber, name: &OsStr) -> Result<InodeAttr> {\n        // 1. Search DIR_ITEM in parent's inode\n        // 2. Get child inode number\n        // 3. Return child's InodeAttr\n    }\n    \n    fn readdir(&self, cx: &Cx, ino: InodeNumber, offset: u64) -> Result<Vec<DirEntry>> {\n        // 1. Iterate DIR_INDEX items for ino\n        // 2. Convert to DirEntry list\n    }\n    \n    fn read(&self, cx: &Cx, ino: InodeNumber, offset: u64, size: u32) -> Result<Vec<u8>> {\n        // 1. Get EXTENT_DATA items for ino\n        // 2. Map logical offset to physical\n        // 3. Read data blocks\n    }\n    \n    fn readlink(&self, cx: &Cx, ino: InodeNumber) -> Result<Vec<u8>> {\n        // Symlink target stored in EXTENT_DATA or inline\n    }\n    \n    fn listxattr(&self, cx: &Cx, ino: InodeNumber) -> Result<Vec<String>> {\n        // Search XATTR_ITEM keys for ino\n    }\n    \n    fn getxattr(&self, cx: &Cx, ino: InodeNumber, name: &str) -> Result<Option<Vec<u8>>> {\n        // Lookup specific XATTR_ITEM\n    }\n}\n```\n\n### Key btrfs Concepts\n- Object ID: inode number\n- Key: (objectid, type, offset) tuple\n- Tree search: walk B-tree looking for key range\n- Item types: INODE_ITEM (1), DIR_ITEM (84), DIR_INDEX (96), EXTENT_DATA (108), XATTR_ITEM (24)\n\n### Helper Functions Needed\n```rust\nfn find_inode_item(tree: &BtrfsTree, ino: u64) -> Result<BtrfsInodeItem>;\nfn find_dir_items(tree: &BtrfsTree, ino: u64) -> Result<Vec<BtrfsDirItem>>;\nfn find_extent_data(tree: &BtrfsTree, ino: u64, offset: u64) -> Result<BtrfsExtentData>;\n```\n\n## FILES TO CREATE/MODIFY\n- crates/ffs-core/src/btrfs/ops.rs (new)\n- crates/ffs-core/src/btrfs/mod.rs (wire up)\n- crates/ffs-ondisk/src/btrfs/items.rs (ensure item parsing complete)\n\n## ACCEPTANCE CRITERIA\n1. BtrfsFsOps compiles and implements FsOps\n2. getattr returns valid InodeAttr for root inode\n3. lookup finds files by name\n4. readdir lists directory contents\n5. read returns file data\n6. Unit tests with mock btrfs data\n\n## DEPENDENCIES\n- btrfs golden fixtures (bd-2jk.2)\n\n## REFERENCE\n- btrfs on-disk format: https://btrfs.wiki.kernel.org/index.php/On-disk_Format\n- Key types: fs/btrfs/btrfs_inode.h","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-12T14:59:30.943924807Z","created_by":"ubuntu","updated_at":"2026-02-12T14:59:48.566461964Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-375.1","depends_on_id":"bd-2jk.2","type":"blocks","created_at":"2026-02-12T14:59:48.566411670Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-375.1","depends_on_id":"bd-375","type":"parent-child","created_at":"2026-02-12T14:59:30.943924807Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-375.2","title":"Wire btrfs mount command in ffs-cli","description":"# Wire btrfs mount command in ffs-cli\n\n## GOAL\nEnable `ffs mount btrfs.img /mnt` to work, parallel to ext4 mount support.\n\n## CONTEXT\nffs-cli mount command currently only works for ext4. Need to:\n1. Detect btrfs images in mount path\n2. Instantiate BtrfsFsOps\n3. Pass to FrankenFuse\n\n## IMPLEMENTATION\n\n### Update mount command\n```rust\n// In ffs-cli/src/commands/mount.rs\npub fn run_mount(cx: &Cx, image_path: &Path, mountpoint: &Path, options: MountOptions) -> Result<()> {\n    let flavor = detect_filesystem_at_path(image_path)?;\n    \n    let fs_ops: Box<dyn FsOps> = match flavor {\n        FsFlavor::Ext4 => {\n            let image = std::fs::read(image_path)?;\n            Box::new(Ext4FsOps::new(Arc::new(image))?)\n        }\n        FsFlavor::Btrfs => {\n            let image = std::fs::read(image_path)?;\n            Box::new(BtrfsFsOps::new(Arc::new(image))?)  // NEW\n        }\n    };\n    \n    let fuse = FrankenFuse::new(fs_ops);\n    fuser::mount2(fuse, mountpoint, &options.to_fuse_options())?;\n    Ok(())\n}\n```\n\n## ACCEPTANCE CRITERIA\n1. `ffs mount btrfs.img /mnt` succeeds\n2. Can `ls /mnt` and see files\n3. Can `cat /mnt/file.txt` and read contents\n4. Clean unmount via fusermount -u\n5. Error message if btrfs features unsupported\n\n## DEPENDENCIES\n- BtrfsFsOps implementation (bd-375.1)\n- btrfs golden fixtures (bd-2jk.2)\n\n## TESTING\n```bash\n# Create btrfs image\ndd if=/dev/zero of=test.btrfs bs=1M count=256\nmkfs.btrfs test.btrfs\nsudo mount -o loop test.btrfs /mnt\necho \"hello\" | sudo tee /mnt/test.txt\nsudo umount /mnt\n\n# Test FrankenFS mount\ncargo run -p ffs-cli --release -- mount test.btrfs /tmp/ffs_mnt\ncat /tmp/ffs_mnt/test.txt  # Should print \"hello\"\nfusermount -u /tmp/ffs_mnt\n```","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-12T14:59:43.818041537Z","created_by":"ubuntu","updated_at":"2026-02-12T14:59:48.650793477Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-375.2","depends_on_id":"bd-375","type":"parent-child","created_at":"2026-02-12T14:59:43.818041537Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-375.2","depends_on_id":"bd-375.1","type":"blocks","created_at":"2026-02-12T14:59:48.650751368Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-375.3","title":"Add E2E smoke tests for btrfs RO mount (FUSE)","description":"# Add E2E smoke tests for btrfs RO mount (FUSE)\n\n## GOAL\nAdd an end-to-end smoke test script that mounts a btrfs fixture image read-only and validates basic filesystem operations through FUSE.\n\n## CONTEXT\nUnit tests validate parsing and tree walking, but btrfs mount correctness is ultimately a black-box property:\n- does `ffs mount` succeed?\n- does `ls`/`stat`/`cat` work?\n- are subvolumes visible at least for the default subvolume?\n\nThis bead adds a reproducible, logged script that answers those questions.\n\n## DELIVERABLES\n1. `scripts/e2e/ffs_btrfs_ro_smoke.sh`\n2. Uses `scripts/e2e/lib.sh` from bd-2jk.6 for logging/cleanup conventions.\n3. Updates `scripts/e2e/README.md` with btrfs-specific notes.\n\n## SCRIPT REQUIREMENTS\n- Same logging + cleanup invariants as bd-2jk.6.\n- Must mount to a temp dir and unmount reliably.\n- Must print the detected btrfs geometry (sectorsize/nodesize) from `ffs inspect` in the log header.\n\n## TEST MATRIX\n1. `cargo run -p ffs-cli -- inspect tests/fixtures/images/btrfs_small.img --json`\n2. `cargo run -p ffs-cli -- mount tests/fixtures/images/btrfs_small.img <mnt>`\n3. Validate:\n   - `ls -la <mnt>`\n   - `stat <mnt>`\n   - bounded `find` of a small depth\n   - `cat` at least one known fixture file\n   - if fixture includes a subvolume: ensure it appears (document exact expectation)\n4. Unmount.\n\n## ACCEPTANCE CRITERIA\n1. Script passes on a correct implementation and fails on obvious regressions (mount failure, empty listing, unreadable known file).\n2. Failure output is actionable (command + logs + mount cleanup).\n3. Runtime <5 minutes.\n\n## NOTES\n- This bead should not add new btrfs functionality; it only validates bd-375.2 end-to-end.\n- If `/dev/fuse` is not available, script must SKIP with a clear message and exit 0 (so it can run in constrained CI environments).","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-12T20:56:25.606708296Z","created_by":"ubuntu","updated_at":"2026-02-12T20:56:25.606708296Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-375.3","depends_on_id":"bd-2jk.2","type":"blocks","created_at":"2026-02-12T20:56:25.606708296Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-375.3","depends_on_id":"bd-2jk.6","type":"blocks","created_at":"2026-02-12T20:56:25.606708296Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-375.3","depends_on_id":"bd-375","type":"parent-child","created_at":"2026-02-12T20:56:25.606708296Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-375.3","depends_on_id":"bd-375.2","type":"blocks","created_at":"2026-02-12T20:56:25.606708296Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-37g","title":"Perf: Maintain scripts/benchmark.sh + add baselines for key paths","description":"Goal: keep a repeatable benchmark entry point as the CLI/harness evolves.\n\nDeliverables:\n- Update scripts/benchmark.sh when commands or binaries change.\n- Add baselines for:\n  - ffs-cli inspect <fixture>\n  - ffs-core open/validate ext4\n  - btrfs sys_chunk mapping (once implemented)\n\nAcceptance:\n- scripts/benchmark.sh runs successfully on a fresh checkout.\n- Baselines are stable enough to detect regressions.","status":"closed","priority":2,"issue_type":"task","owner":"PinkCreek","created_at":"2026-02-10T03:30:37.930757625Z","created_by":"ubuntu","updated_at":"2026-02-11T02:58:21.589581022Z","closed_at":"2026-02-11T02:58:21.589552609Z","close_reason":"ondisk_parse benchmarks + CLI baselines + baseline-20260211","source_repo":".","compaction_level":0,"original_size":0,"labels":["perf"],"dependencies":[{"issue_id":"bd-37g","depends_on_id":"bd-3q4","type":"blocks","created_at":"2026-02-10T03:31:10.672820552Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-37u","title":"FrankenFS V1 Program (Spec -> Parity -> MVCC -> Self-Heal -> FUSE)","description":"Root epic for FrankenFS V1. Goal: a memory-safe, clean-room Rust filesystem that is mount-compatible with ext4 images (and later btrfs), but internally uses copy-on-write MVCC and self-healing durability.\\n\\nThis epic is CLOSED only when all track epics are closed (docs/spec integrity, ext4/btrfs on-disk parsing, I/O + caching, harness/conformance + benchmarks, MVCC, repair, FUSE mount surface, CLI/TUI, performance gates).\\n\\nNon-negotiables (see AGENTS.md): no unsafe code, Rust 2024/nightly, no destructive commands, spec-first + conformance-first, keep FEATURE_PARITY current.","notes":"Closure evidence: all 13 blocking track epics are closed; write-path epic bd-zge and bd-zge.8 are closed with integration + conformance coverage. Validation rerun on 2026-02-11: cargo check --all-targets (pass), cargo clippy --all-targets -- -D warnings (pass), cargo test --workspace (pass), cargo test -p ffs-harness -- --nocapture (pass), cargo bench -p ffs-harness (pass). Workspace cargo fmt --check still reports pre-existing formatting drift in unrelated crates and is tracked separately from this closure decision.","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-10T03:08:01.941883096Z","created_by":"ubuntu","updated_at":"2026-02-11T06:36:41.379179778Z","closed_at":"2026-02-11T06:36:41.379082476Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-37u","depends_on_id":"bd-1cq","type":"blocks","created_at":"2026-02-10T03:09:39.083100981Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37u","depends_on_id":"bd-220","type":"blocks","created_at":"2026-02-10T03:09:39.279735383Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37u","depends_on_id":"bd-29o","type":"blocks","created_at":"2026-02-10T03:09:39.411627077Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37u","depends_on_id":"bd-2fa","type":"blocks","created_at":"2026-02-10T03:09:38.949282548Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37u","depends_on_id":"bd-2uj","type":"blocks","created_at":"2026-02-10T03:09:39.620238013Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37u","depends_on_id":"bd-397","type":"blocks","created_at":"2026-02-10T03:09:39.014323044Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37u","depends_on_id":"bd-3pm","type":"blocks","created_at":"2026-02-10T03:09:39.344547011Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37u","depends_on_id":"bd-3sk","type":"blocks","created_at":"2026-02-10T03:09:39.481917607Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37u","depends_on_id":"bd-3sp","type":"blocks","created_at":"2026-02-10T03:09:39.148591190Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37u","depends_on_id":"bd-6dl","type":"blocks","created_at":"2026-02-10T03:09:38.815163472Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37u","depends_on_id":"bd-c35","type":"blocks","created_at":"2026-02-10T03:09:39.213607120Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37u","depends_on_id":"bd-g30","type":"blocks","created_at":"2026-02-10T03:09:39.551168149Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-37u","depends_on_id":"bd-z7n","type":"blocks","created_at":"2026-02-10T03:09:38.881651409Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":1,"issue_id":"bd-37u","author":"Dicklesworthstone","text":"How To Use This Beads Graph (self-contained):\n\n- Find actionable work: br ready --json\n- Visualize dependencies: br dep tree <id>\n- Never create cycles: br dep cycles (must be empty)\n- After changing issues: br sync --flush-only (exports .beads/issues.jsonl)\n\nDesign doctrine (why this plan is structured into tracks):\n- Spec-first + conformance-first: we do not trust line-by-line C translations. Each behavior needs fixtures/harness coverage.\n- Most catastrophic filesystem bugs are unit/offset mismatches and unchecked arithmetic; foundations + parsing invariants are early epics.\n- FUSE is a thin adapter: it should delegate to a minimal internal FsOps trait so correctness can be tested without mounting.\n- MVCC + repair are treated as first-class tracks with explicit invariants and decision-theoretic policies (no magic thresholds).\n\nDefinition of Done for any implementation task:\n- Implement the behavior\n- Add fixture(s) or property tests\n- Update FEATURE_PARITY + ffs-harness ParityReport in the same change\n- Pass: cargo fmt --check, cargo check --all-targets, cargo clippy --all-targets -- -D warnings, cargo test --workspace","created_at":"2026-02-10T03:33:38Z"}]}
{"id":"bd-382","title":"btrfs walker: detect logical-pointer cycles during tree traversal","description":"Context: all existing open beads are either deletion-gated (bd-1x8) or already claimed in FUSE scope (bd-3gj).\\n\\nProblem:\\n- crates/ffs-btrfs/src/lib.rs walk_tree/walk_node recurse by following internal key pointers.\\n- Corrupt trees can point back to already-visited logical blocks, risking unbounded recursion / stack overflow.\\n\\nScope:\\n- Add cycle detection (visited logical set) to btrfs traversal.\\n- Return ParseError::InvalidField with clear reason when a cycle is detected.\\n- Add tests proving self-cycle and multi-node cycle are rejected.\\n\\nAcceptance:\\n- Traversal remains unchanged for valid trees.\\n- Cycles fail fast with deterministic error.\\n- cargo test -p ffs-btrfs passes.","status":"closed","priority":1,"issue_type":"task","assignee":"FoggyIsland","created_at":"2026-02-11T18:43:27.929779989Z","created_by":"ubuntu","updated_at":"2026-02-11T18:45:28.515265356Z","closed_at":"2026-02-11T18:45:28.515240089Z","close_reason":"Implemented btrfs logical-pointer cycle detection + regression tests; fmt/check/clippy/test all pass","source_repo":".","compaction_level":0,"original_size":0,"labels":["btrfs","conformance","safety"]}
{"id":"bd-397","title":"Track: ext4 On-Disk Parsing + Validation (ffs-ondisk)","description":"Implement ext4 superblock/groupdesc/inode/extents parsing and v1 validation rules (1K/2K/4K only).\\n\\nAcceptance: fixture-backed parsing (no panics on malformed input), explicit incompat/ro_compat/compat feature handling, geometry validation, inode location math, and enough metadata decode to support read-only directory traversal in harness.","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-10T03:08:36.666176234Z","created_by":"ubuntu","updated_at":"2026-02-10T21:07:31.048400234Z","closed_at":"2026-02-10T21:07:31.048381980Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-397","depends_on_id":"bd-1a9","type":"blocks","created_at":"2026-02-10T03:17:54.863758523Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-397","depends_on_id":"bd-25k","type":"blocks","created_at":"2026-02-10T03:17:55.098495316Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-397","depends_on_id":"bd-28h","type":"blocks","created_at":"2026-02-10T03:31:57.763623533Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-397","depends_on_id":"bd-2cn","type":"blocks","created_at":"2026-02-10T03:17:54.790696896Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-397","depends_on_id":"bd-2dk","type":"blocks","created_at":"2026-02-10T03:17:55.253783443Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-397","depends_on_id":"bd-2w9","type":"blocks","created_at":"2026-02-10T03:17:54.943155050Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-397","depends_on_id":"bd-3qq","type":"blocks","created_at":"2026-02-10T03:17:55.177411533Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-397","depends_on_id":"bd-8tr","type":"blocks","created_at":"2026-02-10T03:17:55.019820641Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-397","depends_on_id":"bd-z7n","type":"blocks","created_at":"2026-02-10T03:18:20.483819114Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":5,"issue_id":"bd-397","author":"Dicklesworthstone","text":"ext4 v1 support policy is intentionally narrow:\n- Block sizes: 1K/2K/4K only.\n- Required features: FILETYPE + EXTENTS.\n- Unknown incompatible feature bits must fail validation.\n\nThis track exists to make ext4 parsing boring: bounded, checksum-aware, and fixture-backed.","created_at":"2026-02-10T03:34:38Z"}]}
{"id":"bd-3bf","title":"Docs: Resolve layering contract (ffs-fuse / ffs-core / semantics boundaries)","description":"Goal: make crate boundaries unambiguous so implementation does not drift.\n\nDecisions to document:\n- ffs-fuse is a thin adapter over an internal FsOps trait.\n- ffs-core owns open/detect/validate and constructs per-format contexts.\n- semantics crates (ffs-inode/ffs-dir/ffs-extent/...) implement the actual operations.\n\nAcceptance:\n- PROPOSED_ARCHITECTURE.md + COMPREHENSIVE_SPEC agree on the layering and dependency direction.\n- Cargo.toml dependency edges match the documented layering.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-10T03:35:00.976855586Z","created_by":"ubuntu","updated_at":"2026-02-11T02:03:15.319842714Z","closed_at":"2026-02-11T02:03:15.319761162Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs"],"dependencies":[{"issue_id":"bd-3bf","depends_on_id":"bd-2ds","type":"blocks","created_at":"2026-02-10T03:35:19.679993026Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3bu","title":"ext4 semantics: Implement xattr parsing + getxattr/listxattr [phased]","description":"Future task: support extended attributes.\n\nDeliverables:\n- Parse ext4 xattr header and entries (inline and block-based).\n- Implement getxattr/listxattr semantics.\n\nAcceptance:\n- Fixture xattr values match kernel outputs.","status":"closed","priority":3,"issue_type":"task","owner":"PinkCreek","created_at":"2026-02-10T03:27:40.697810297Z","created_by":"ubuntu","updated_at":"2026-02-11T03:27:14.188846783Z","closed_at":"2026-02-11T03:27:14.188824792Z","close_reason":"Added listxattr/getxattr to FsOps trait. Implemented on Ext4FsOps and OpenFs. parse_ibody_xattrs + parse_xattr_block standalone functions added. 6 tests.","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","semantics"],"dependencies":[{"issue_id":"bd-3bu","depends_on_id":"bd-10t","type":"blocks","created_at":"2026-02-10T03:28:01.732139893Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3ff","title":"Perf: Add criterion benches for cache hot paths + parsing loops","description":"Goal: measure the actual hotspots once correctness fixtures exist.\n\nDeliverables:\n- Criterion bench for:\n  - ARC cache hit path\n  - ARC cache miss path (no I/O in bench; use in-memory ByteDevice)\n  - ext4 group desc iteration / inode read (once implemented)\n\nAcceptance:\n- Benches run in CI or at least locally with stable results.\n- Bench outputs are used to drive targeted optimization work.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:30:56.866258166Z","created_by":"ubuntu","updated_at":"2026-02-10T21:00:07.922658284Z","closed_at":"2026-02-10T21:00:07.922640581Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["io","perf"],"dependencies":[{"issue_id":"bd-3ff","depends_on_id":"bd-5iz","type":"blocks","created_at":"2026-02-10T03:31:10.503270335Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3gj","title":"FUSE: wire read-only xattr ops to FsOps","description":"Context: bd-1x8 is deletion-gated; this is the next non-destructive, high-value gap.\\n\\nScope:\\n- Implement FUSE callbacks for read-only xattr operations in crates/ffs-fuse/src/lib.rs.\\n- Route to existing FsOps listxattr/getxattr methods in crates/ffs-core/src/lib.rs.\\n- Add/extend tests for expected behavior and errno mapping.\\n\\nAcceptance:\\n- listxattr/getxattr paths work through FUSE adapter for FsOps implementations that already support them.\\n- Existing commands/tests continue passing (fmt/check/clippy/test).\\n- Coordinate file scope via Agent Mail.","status":"closed","priority":2,"issue_type":"task","assignee":"TurquoiseFox","created_at":"2026-02-11T18:41:44.610934120Z","created_by":"ubuntu","updated_at":"2026-02-11T18:47:03.974741351Z","closed_at":"2026-02-11T18:47:03.974719981Z","close_reason":"Implemented FUSE read-only xattr wiring and passed all required gates","source_repo":".","compaction_level":0,"original_size":0,"labels":["conformance","fuse","xattr"]}
{"id":"bd-3gt","title":"Docs: Reconcile crate map + dependency claims with Cargo workspace","description":"Goal: ensure every docs crate table/diagram matches Cargo.toml + crates/*/Cargo.toml reality (including phased dependencies like fuser).\n\nDeliverables:\n- COMPREHENSIVE_SPEC and PROPOSED_ARCHITECTURE crate lists reflect the 21-crate workspace (19 core + ffs-ext4/ffs-btrfs wrappers + ffs facade).\n- Each crate description states: purpose, allowed deps, and which phase introduces it.\n- Remove stale claims (e.g. crates that \"depend on X\" when they do not).\n\nAcceptance:\n- A reviewer can grep any crate name and find exactly one consistent description of its purpose and deps.\n- No phantom crates remain except in historical notes that explicitly say they are phantom.","status":"closed","priority":0,"issue_type":"task","assignee":"codex","created_at":"2026-02-10T03:12:54.518888Z","created_by":"ubuntu","updated_at":"2026-02-10T06:47:32.522979682Z","closed_at":"2026-02-10T06:47:32.522953282Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs"],"dependencies":[{"issue_id":"bd-3gt","depends_on_id":"bd-2ds","type":"blocks","created_at":"2026-02-10T03:14:05.449410020Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3h8","title":"Docs: Resolve missing ARCHITECTURE.md / CONVENTIONS.md references","description":"Problem: PLAN_TO_PORT_FRANKENFS_TO_RUST.md references ARCHITECTURE.md and CONVENTIONS.md as if they exist, but they do not in this repo.\n\nOptions:\n1) Create these docs with narrowly-scoped content:\n- ARCHITECTURE.md: crate topology + key invariants + dataflow diagrams (thin, points to COMPREHENSIVE_SPEC + PROPOSED_ARCHITECTURE).\n- CONVENTIONS.md: code style rules, Definition of Done, checklists (fmt/check/clippy/test), parity update protocol.\n2) Remove references entirely and ensure existing docs cover the intent.\n\nAcceptance:\n- No references to non-existent files remain.\n- If files are created, they are referenced as the canonical location for the promised content and kept small (no duplicate spec).","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:13:26.674127660Z","created_by":"ubuntu","updated_at":"2026-02-11T02:08:56.848685681Z","closed_at":"2026-02-11T02:08:56.848609359Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs"],"dependencies":[{"issue_id":"bd-3h8","depends_on_id":"bd-2ds","type":"blocks","created_at":"2026-02-10T03:14:05.524210264Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3hi","title":"Docs: Align errno mapping (Cancelled EINTR vs ECANCELED, etc.)","description":"Problem: COMPREHENSIVE_SPEC_FOR_FRANKENFS_V1.md currently documents Cancelled -> EINTR, but crates/ffs-error/src/lib.rs maps Cancelled -> ECANCELED.\n\nGoal: choose the canonical errno semantics for each FfsError variant (especially cancellation) and ensure:\n- ffs-error::to_errno() matches that choice\n- docs tables match that choice\n- FUSE behavior uses it consistently\n\nAcceptance:\n- A single authoritative errno mapping table exists (prefer ffs-error docs or code comment) and docs reference it.\n- COMPREHENSIVE_SPEC + PLAN + PROPOSED_ARCHITECTURE do not contradict the code.\n- We document the rationale (EINTR vs ECANCELED) in one place.","status":"closed","priority":0,"issue_type":"task","assignee":"codex","created_at":"2026-02-10T03:13:34.660731378Z","created_by":"ubuntu","updated_at":"2026-02-10T06:52:01.225330808Z","closed_at":"2026-02-10T06:52:01.225311371Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs"],"dependencies":[{"issue_id":"bd-3hi","depends_on_id":"bd-2ds","type":"blocks","created_at":"2026-02-10T03:14:05.597011413Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3hr","title":"ffs-error: Decide Cancelled errno semantics and update to_errno()","description":"Context: cancellation propagates via asupersync::Cx::checkpoint(). For FUSE, cancellation can be represented as EINTR (interrupted) or ECANCELED (explicit cancellation).\n\nGoal: choose one policy and implement it in crates/ffs-error/src/lib.rs::to_errno(), and align docs accordingly.\n\nAcceptance:\n- Cancelled maps to the chosen errno everywhere.\n- We document the rationale (kernel/VFS expectations, operator experience, retry semantics).","status":"closed","priority":0,"issue_type":"task","assignee":"codex","created_at":"2026-02-10T03:14:44.331239413Z","created_by":"ubuntu","updated_at":"2026-02-10T06:52:17.927026438Z","closed_at":"2026-02-10T06:52:17.927005458Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["foundation"],"dependencies":[{"issue_id":"bd-3hr","depends_on_id":"bd-3hi","type":"blocks","created_at":"2026-02-10T03:15:20.267006203Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3ib","title":"EPIC: Performance Optimization","description":"# EPIC: Performance Optimization\n\n## PURPOSE\nEstablish performance baselines and optimize hot paths following the \"Extreme Software Optimization\" methodology.\n\n## BACKGROUND\nFrom AGENTS.md and the extreme-software-optimization methodology:\n- Profile first\n- One optimization lever per commit\n- Prove behavior unchanged (goldens or invariants)\n- Isomorphism proof template required\n\nThis epic should never run ahead of conformance infrastructure: performance claims must be backed by reproducible baselines and behavior proofs.\n\n## METHODOLOGY (MANDATORY)\n\n### The Loop\n```\n1. BASELINE  -> hyperfine (record JSON)\n2. PROFILE   -> flamegraph/perf\n3. PROVE     -> goldens + invariants (sha256sum, unit tests)\n4. CHANGE    -> one lever per commit (Score >= 2.0)\n5. VERIFY    -> gates + golden verification\n6. REPEAT\n```\n\n### Opportunity Matrix Template\n| Hotspot | Impact (1-5) | Confidence (1-5) | Effort (1-5) | Score |\n|---------|--------------|------------------|--------------|-------|\n| func:line | x | x | / | Impact*Conf/Effort |\n\nRule: Only implement Score >= 2.0\n\n### Isomorphism Proof Template (per change)\n```\n## Change: [description]\n- Ordering preserved:     [yes/no + why]\n- Tie-breaking unchanged: [yes/no + why]\n- Floating-point:         N/A\n- RNG seeds:              N/A\n- Goldens:                sha256sum -c tests/fixtures/golden/checksums.txt (or equivalent) ‚úì\n```\n\n## ACCEPTANCE CRITERIA\n1. Hyperfine baselines exist for the supported CLI + mount smoke workflows\n2. Flamegraphs generated and analyzed\n3. Opportunity matrix populated\n4. At least 3 optimizations implemented with behavior proofs\n5. No performance regressions detected vs baseline\n\n## DEPENDENCIES\n- EPIC: Conformance & Quality Infrastructure (bd-2jk)\n\n## EXPLICIT NON-GOALS\n- Premature optimization before profiling\n- Micro-optimizations with Score < 2.0\n- Any change without an isomorphism proof + golden verification\n\n## Success Criteria\n1. Performance work is reproducible: a developer can re-run baseline + profile scripts and get comparable results.\n2. Every optimization includes an explicit proof of unchanged behavior (goldens/invariants) and a measured delta.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-12T15:04:09.695775688Z","created_by":"ubuntu","updated_at":"2026-02-12T20:59:27.990748791Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3ib","depends_on_id":"bd-2jk","type":"blocks","created_at":"2026-02-12T15:04:29.589736197Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3ib.1","title":"Profile read path and generate flamegraph","description":"# Profile read path and generate flamegraph\n\n## GOAL\nGenerate CPU flamegraph for read operations to identify actual hotspots.\n\n## CONTEXT\nBefore any optimization, must profile to find real bottlenecks. Predicted hotspots may not match reality.\n\n## IMPLEMENTATION\n\n### Step 1: Install profiling tools\n```bash\ncargo install flamegraph\n# May need: sudo sysctl kernel.perf_event_paranoid=-1\n```\n\n### Step 2: Profile inspect command\n```bash\ncargo flamegraph --bin ffs-cli -- inspect fixtures/ext4_large.img --json > /dev/null\n# Creates flamegraph.svg\n```\n\n### Step 3: Profile read via FUSE\n```bash\n# In one terminal:\ncargo build --release -p ffs-cli\n./target/release/ffs-cli mount fixtures/ext4_large.img /tmp/ffs\n\n# In another terminal:\nperf record -g dd if=/tmp/ffs/largefile of=/dev/null bs=4k count=10000\nperf script | stackcollapse-perf.pl | flamegraph.pl > read_path.svg\n```\n\n### Step 4: Analyze and document\nCreate PROFILE_ANALYSIS.md:\n```markdown\n## Profile Date: YYYY-MM-DD\n## Commit: <sha>\n## Hardware: <spec>\n\n### Top 10 Hotspots (% CPU)\n1. function_name:line - XX%\n2. ...\n\n### Opportunity Matrix\n| Hotspot | Impact | Conf | Effort | Score |\n|---------|--------|------|--------|-------|\n| ... | ... | ... | ... | ... |\n\n### Recommended First Optimization\n...\n```\n\n## FILES TO CREATE\n- profiles/read_inspect.svg\n- profiles/read_fuse.svg\n- PROFILE_ANALYSIS.md\n\n## ACCEPTANCE CRITERIA\n1. Flamegraphs generated for inspect and FUSE read\n2. Top 10 hotspots identified\n3. Opportunity matrix populated\n4. First optimization target selected (Score ‚â• 2.0)\n\n## DEPENDENCIES\n- Golden fixtures (bd-2jk.1, bd-2jk.2)\n- Baselines (bd-2jk.3)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-12T15:04:23.178997306Z","created_by":"ubuntu","updated_at":"2026-02-12T15:04:29.697427035Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3ib.1","depends_on_id":"bd-2jk.3","type":"blocks","created_at":"2026-02-12T15:04:29.697380738Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3ib.1","depends_on_id":"bd-3ib","type":"parent-child","created_at":"2026-02-12T15:04:23.178997306Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3ie","title":"MVCC: Version GC/pruning semantics + watermark API","description":"Goal: prevent unbounded growth of version chains while preserving snapshot correctness.\n\nDeliverables:\n- Define watermark: the oldest snapshot that must remain readable.\n- Implement prune_versions_older_than with clear semantics (keep at least one version per block).\n- Integrate with version store persistence design.\n\nAcceptance:\n- Tests show: pruning does not break reads at snapshots >= watermark.\n- Memory usage remains bounded in a long-running simulation.","status":"closed","priority":2,"issue_type":"task","owner":"PinkCreek","created_at":"2026-02-10T03:22:59.347693253Z","created_by":"ubuntu","updated_at":"2026-02-11T03:05:03.750023307Z","closed_at":"2026-02-11T03:05:03.749995605Z","close_reason":"watermark API + active snapshot tracking + prune_safe + bounded memory tests","source_repo":".","compaction_level":0,"original_size":0,"labels":["mvcc"],"dependencies":[{"issue_id":"bd-3ie","depends_on_id":"bd-1u7","type":"blocks","created_at":"2026-02-10T03:23:53.592572202Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3is","title":"ffs-core: Define OpenFs API (detect -> parse -> validate -> context)","description":"Goal: provide a single entry point for higher layers (CLI, harness, FUSE) to open a filesystem image.\n\nDeliverables:\n- Define types: FilesystemKind, OpenFs (or Ext4Context/BtrfsContext), OpenOptions.\n- OpenFs contains: parsed superblock, computed geometry, block device handle, and any derived constants.\n- API shape should make it impossible to forget validation (open() should validate by default).\n\nAcceptance:\n- ffs-cli and ffs-harness use this API exclusively (no ad-hoc probing).\n- Error messages include enough context for UX (which validation failed).","status":"closed","priority":1,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:19:50.195373546Z","created_by":"ubuntu","updated_at":"2026-02-10T16:37:52.596376784Z","closed_at":"2026-02-10T16:37:52.596356115Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["core"]}
{"id":"bd-3k4","title":"ffs-core: Implement btrfs open/validate pipeline (phased, read-only)","description":"Goal: open a btrfs image via ffs-block + ffs-ondisk, validate superblock fields, and produce a BtrfsContext suitable for read-only discovery.\n\nDeliverables:\n- Read btrfs superblock region via ffs-block helper.\n- Parse BtrfsSuperblock and validate nodesize/sectorsize.\n- Build sys_chunk logical->physical mapper for single-device images.\n- Provide a method to read the root tree node via mapping.\n\nAcceptance:\n- Can open a fixture btrfs image and enumerate a few root-tree items (using the btrfs track primitives).\n- Failures return structured errors (not just UnsupportedImage).","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:20:08.442820137Z","created_by":"ubuntu","updated_at":"2026-02-10T20:38:03.059596284Z","closed_at":"2026-02-10T20:38:03.059577078Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["btrfs","core"],"dependencies":[{"issue_id":"bd-3k4","depends_on_id":"bd-1fo","type":"blocks","created_at":"2026-02-10T03:20:48.403282392Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3k4","depends_on_id":"bd-kdk","type":"blocks","created_at":"2026-02-10T03:20:48.329083905Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3nc","title":"ffs-core: Implement ext4 open/validate pipeline on ByteDevice","description":"Goal: open an ext4 image via ffs-block + ffs-ondisk, validate v1 constraints, and produce an Ext4Context used by higher layers.\n\nDeliverables:\n- Read ext4 superblock region via ffs-block helper.\n- Parse Ext4Superblock and call validate_v1 + geometry validation.\n- Construct a BlockDevice using the ext4 block_size.\n- Compute group_count and any derived constants needed for later reads.\n\nAcceptance:\n- Unit tests: synthetic images + failure cases.\n- Fixture test: open a real ext4 image and print key fields (via harness/CLI) without panics.","status":"closed","priority":1,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:19:59.733624025Z","created_by":"ubuntu","updated_at":"2026-02-10T19:20:35.378477759Z","closed_at":"2026-02-10T19:20:35.378459014Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["core","ext4"],"dependencies":[{"issue_id":"bd-3nc","depends_on_id":"bd-1a9","type":"blocks","created_at":"2026-02-10T03:20:48.250186773Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nc","depends_on_id":"bd-2w9","type":"blocks","created_at":"2026-02-10T03:20:48.171049692Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3pm","title":"Track: Self-Healing Durability (ffs-repair + RaptorQ)","description":"Implement scrub + repair workflows using fountain-code repair symbols (RaptorQ) at block-group granularity, inspired by FrankenSQLite.\\n\\nAcceptance: (1) detect corruption via checksums/invariants, (2) decide repair policy via explicit expected-loss model, (3) generate/store repair symbols with bounded overhead, (4) demonstrate recovery in harness via fault-injection fixtures.","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-02-10T03:09:03.148300333Z","created_by":"ubuntu","updated_at":"2026-02-11T03:13:48.907677046Z","closed_at":"2026-02-11T03:13:48.907655636Z","close_reason":"All dependencies closed: scrub pipeline, RaptorQ encode/decode, expected-loss policy, repair format, MVCC engine. Repair + self-healing infrastructure complete.","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3pm","depends_on_id":"bd-12x","type":"blocks","created_at":"2026-02-10T03:24:51.850237578Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3pm","depends_on_id":"bd-16f","type":"blocks","created_at":"2026-02-10T03:24:51.684984729Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3pm","depends_on_id":"bd-220","type":"blocks","created_at":"2026-02-10T03:25:07.365358636Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3pm","depends_on_id":"bd-2qf","type":"blocks","created_at":"2026-02-10T03:24:51.602555705Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3pm","depends_on_id":"bd-98b","type":"blocks","created_at":"2026-02-10T03:24:51.932081315Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3pm","depends_on_id":"bd-b80","type":"blocks","created_at":"2026-02-10T03:24:51.767948606Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3pm","depends_on_id":"bd-z7n","type":"blocks","created_at":"2026-02-10T03:25:07.446263615Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":10,"issue_id":"bd-3pm","author":"Dicklesworthstone","text":"Self-healing must be explainable.\n\nWe will not ship \"magic\" repair heuristics:\n- detect corruption via checksums + invariants\n- decide redundancy/repair via expected-loss model\n- record decode proofs so we can audit why a repair succeeded/failed\n\nThis is explicitly patterned after FrankenSQLite, adapted to block groups.","created_at":"2026-02-10T03:34:39Z"}]}
{"id":"bd-3q4","title":"Harness: Define fixture generation workflow (sparse fixtures + real images)","description":"Goal: make it easy to add fixtures for new parsing/semantics features without ad-hoc manual bytes.\n\nDeliverables:\n- Document fixture formats used in conformance/fixtures (sparse JSON today).\n- Add a generator approach:\n  - option A: produce sparse fixtures from real images by extracting only relevant byte ranges\n  - option B: maintain small handcrafted fixtures for unit tests, and use real images for E2E\n\nAcceptance:\n- Adding a new fixture is a repeatable recipe (commands + expected outputs) documented in-repo.\n- Fixtures are small enough to live in git and stable across platforms.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:21:26.003872529Z","created_by":"ubuntu","updated_at":"2026-02-11T01:39:43.151468379Z","closed_at":"2026-02-11T01:39:43.151379713Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["harness"]}
{"id":"bd-3qq","title":"ext4: Expand inode parsing (timestamps, uid/gid, flags, extent/inline fields)","description":"Goal: parse enough of ext4 inode to support read-only semantics (stat + extent mapping + directory traversal).\n\nDeliverables:\n- Parse core inode fields: mode, uid/gid (low/high), size, atime/ctime/mtime/crtime, links, flags, generation, blocks.\n- Respect inode_size (128 vs 256) and avoid assuming fixed offsets exist if inode smaller.\n- Parse i_block area for extent header and inline extents.\n\nAcceptance:\n- Fixture tests compare against debugfs/dumpe2fs for at least a few inodes.\n- Parser never panics; InsufficientData is returned for short slices.\n- Extent parsing from inode is wired via parse_inode_extent_tree().","status":"closed","priority":1,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:17:32.338682915Z","created_by":"ubuntu","updated_at":"2026-02-10T17:19:14.312943367Z","closed_at":"2026-02-10T17:19:14.312924221Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","ondisk"],"dependencies":[{"issue_id":"bd-3qq","depends_on_id":"bd-2cn","type":"blocks","created_at":"2026-02-10T03:18:06.921918247Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3qy","title":"ext4: Implement HTree directory index parsing (dx_root/dx_entry) [phased]","description":"Future-facing task: implement parsing of ext4 hashed directory indexes (htree) for fast lookup in large directories.\n\nDeliverables:\n- Parse dx_root, dx_entry tables, and hash seed.\n- Implement lookup(name) -> candidate leaf blocks.\n- Verify hash algorithm matches kernel (TEA/half-MD4 depending on feature).\n\nAcceptance:\n- Works on a fixture directory with htree enabled.\n- Verified against debugfs for lookup paths.\n\nNote: small directories can be handled by linear scan; htree becomes required for parity on large dirs.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-10T03:17:45.431485210Z","created_by":"ubuntu","updated_at":"2026-02-11T03:27:43.348400195Z","closed_at":"2026-02-11T03:27:43.348375770Z","close_reason":"htree already implemented: parse_dx_root, Ext4DxRoot, Ext4DxEntry, dx_hash (legacy/half-md4/tea signed+unsigned), htree_lookup with O(log n) DX index. 6 tests pass.","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","ondisk"]}
{"id":"bd-3sk","title":"Track: FUSE Mount Surface (ffs-fuse)","description":"Implement the Linux userspace mount interface via the fuser crate. This is the external contract: kernel VFS calls -> FrankenFS operations.\\n\\nAcceptance: can mount a supported ext4 image read-only via FUSE and run basic operations (ls/stat/cat) with correct errno mappings, using ffs-core to open/validate and ffs-mvcc to provide snapshot semantics.","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-02-10T03:09:19.141431579Z","created_by":"ubuntu","updated_at":"2026-02-11T03:44:07.642883497Z","closed_at":"2026-02-11T03:44:07.642862167Z","close_reason":"All sub-tasks complete: FUSE skeleton, read-only ops, MVCC policy, errno mapping","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3sk","depends_on_id":"bd-1cc","type":"blocks","created_at":"2026-02-10T03:28:45.097243187Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sk","depends_on_id":"bd-220","type":"blocks","created_at":"2026-02-10T03:29:01.779940191Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sk","depends_on_id":"bd-27a","type":"blocks","created_at":"2026-02-10T03:28:44.985312747Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sk","depends_on_id":"bd-29o","type":"blocks","created_at":"2026-02-10T03:29:01.695737233Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sk","depends_on_id":"bd-2l4","type":"blocks","created_at":"2026-02-10T03:31:38.795251161Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sk","depends_on_id":"bd-3hi","type":"blocks","created_at":"2026-02-10T03:31:38.894635585Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sk","depends_on_id":"bd-3sp","type":"blocks","created_at":"2026-02-10T03:29:01.609787562Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sk","depends_on_id":"bd-vgu","type":"blocks","created_at":"2026-02-10T03:28:44.878119913Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":12,"issue_id":"bd-3sk","author":"Dicklesworthstone","text":"FUSE should stay thin and stupid.\n\nDesign intent:\n- ffs-fuse delegates to internal FsOps.\n- Errors map via FfsError::to_errno.\n- MVCC snapshot policy for requests is explicit and testable.\n\nThis keeps correctness testable without needing to mount during unit tests.","created_at":"2026-02-10T03:34:39Z"}]}
{"id":"bd-3sp","title":"Track: Core Orchestration (ffs-core)","description":"Own the high-level open/detect/validate orchestration. ffs-core should be the single entry point for: detect filesystem kind, parse superblock, validate v1 constraints, and construct per-format mount contexts for FUSE/harness.\\n\\nAcceptance: callers (CLI, harness, FUSE) use ffs-core (not ad-hoc probing). All detection uses fixed-offset reads and returns structured errors suitable for UX + errno.","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-02-10T03:08:47.639624742Z","created_by":"ubuntu","updated_at":"2026-02-10T21:07:31.124956934Z","closed_at":"2026-02-10T21:07:31.124935684Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3sp","depends_on_id":"bd-2fa","type":"blocks","created_at":"2026-02-10T03:20:52.754108430Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sp","depends_on_id":"bd-2fy","type":"blocks","created_at":"2026-02-10T03:20:33.629156010Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sp","depends_on_id":"bd-3is","type":"blocks","created_at":"2026-02-10T03:20:33.398379999Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sp","depends_on_id":"bd-3k4","type":"blocks","created_at":"2026-02-10T03:20:33.553948031Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sp","depends_on_id":"bd-3nc","type":"blocks","created_at":"2026-02-10T03:20:33.474813064Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sp","depends_on_id":"bd-z7n","type":"blocks","created_at":"2026-02-10T03:20:52.675188605Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":7,"issue_id":"bd-3sp","author":"Dicklesworthstone","text":"ffs-core is the integration choke-point.\n\nIt should be the only place that:\n- detects filesystem flavor\n- opens devices\n- validates v1 constraints\n- builds mount contexts\n\nAll UX surfaces (CLI/harness/FUSE) should call ffs-core, not re-implement probing.","created_at":"2026-02-10T03:34:39Z"}]}
{"id":"bd-3vn","title":"CLI: Replace Cx::for_testing() with production context acquisition","description":"Problem: ffs-cli currently uses Cx::for_testing() as a placeholder.\n\nGoal: decide how production binaries acquire a Cx (cancellation, deadlines, tracing) and implement it.\n\nDeliverables:\n- A helper in ffs-core or ffs-cli to build a Cx for CLI invocations.\n- Ensure cancellation behavior is testable (SIGINT -> cancel).\n\nAcceptance:\n- inspect and mount use the same Cx acquisition.\n- No test-only APIs are used in production code paths.","status":"closed","priority":1,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:29:20.263893477Z","created_by":"ubuntu","updated_at":"2026-02-10T16:16:25.541415028Z","closed_at":"2026-02-10T16:16:25.541394239Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli"]}
{"id":"bd-51o","title":"Docs: Sync README feature parity table with canonical ParityReport","description":"Scope:\n- Update README Feature Parity table values to match FEATURE_PARITY.md / ParityReport::current().\n- Ensure overall percentage and per-domain counts are consistent with canonical parity source.\n\nAcceptance:\n- README parity table matches FEATURE_PARITY.md exactly.\n- cargo test -p ffs-harness tests::parity_report_matches_feature_parity_md passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T16:26:25.132873566Z","created_by":"ubuntu","updated_at":"2026-02-11T16:27:30.468745629Z","closed_at":"2026-02-11T16:27:30.468722807Z","close_reason":"README parity table synced with canonical parity report","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs","parity"]}
{"id":"bd-5iz","title":"ffs-block: Implement ARC metadata cache (read-only first)","description":"Goal: implement an ARC-like cache wrapper for block reads to reduce random I/O during metadata-heavy operations.\n\nScope (v1):\n- Cache read_block() results (BlockNumber -> BlockBuf).\n- ARC lists: T1/T2 + ghost lists B1/B2; adaptive parameter p.\n- No write-back yet; writes either bypass or invalidate.\n\nKey design questions to answer explicitly:\n- Cache key: BlockNumber only, or (BlockNumber, CommitSeq/Version) for MVCC? (If MVCC needs versioned cache, design now.)\n- Concurrency: avoid holding cache locks while doing disk I/O.\n\nAcceptance:\n- Unit tests cover core ARC transitions (hits/misses/evictions) with deterministic sequences.\n- No deadlocks; no I/O under locks.\n- Benchmarks show cache improves repeated metadata reads on fixture images.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:15:43.959932112Z","created_by":"ubuntu","updated_at":"2026-02-10T17:07:09.719979271Z","closed_at":"2026-02-10T17:07:09.719959934Z","close_reason":"ARC cache: fix eviction logic + add transition tests; gates green","source_repo":".","compaction_level":0,"original_size":0,"labels":["io"],"dependencies":[{"issue_id":"bd-5iz","depends_on_id":"bd-14w","type":"blocks","created_at":"2026-02-10T03:16:27.296806735Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-6dl","title":"Track: Docs + Spec Integrity (No Drift, No Phantom Contracts)","description":"Make the documentation set internally consistent and consistent with the workspace. This track exists because implementation cannot proceed safely if types/traits/crate boundaries are contradictory across COMPREHENSIVE_SPEC/PROPOSED_ARCHITECTURE/PLAN/FEATURE_PARITY.\\n\\nAcceptance: (1) cross-doc audit returns no Critical/High contradictions, (2) all referenced crates/files exist, (3) canonical glossary/trait locations are unambiguous, (4) any intentional divergence is explicitly labeled 'illustrative' vs 'normative'.","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-10T03:08:19.383649624Z","created_by":"ubuntu","updated_at":"2026-02-11T02:30:30.154565665Z","closed_at":"2026-02-11T02:30:30.154544385Z","close_reason":"Doc integrity sweep: fixed remaining FfsError variant-count drift (14->18) across spec/arch/plan; removed stray 'master' wording; audit rg checks clean","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-6dl","depends_on_id":"bd-126","type":"blocks","created_at":"2026-02-10T03:13:56.949662975Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6dl","depends_on_id":"bd-1va","type":"blocks","created_at":"2026-02-10T03:13:57.023734986Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6dl","depends_on_id":"bd-2ds","type":"blocks","created_at":"2026-02-10T03:13:56.500113505Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6dl","depends_on_id":"bd-2l4","type":"blocks","created_at":"2026-02-10T03:13:56.649526704Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6dl","depends_on_id":"bd-3bf","type":"blocks","created_at":"2026-02-10T03:35:19.506425155Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6dl","depends_on_id":"bd-3gt","type":"blocks","created_at":"2026-02-10T03:13:56.725444543Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6dl","depends_on_id":"bd-3h8","type":"blocks","created_at":"2026-02-10T03:13:56.799958240Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6dl","depends_on_id":"bd-3hi","type":"blocks","created_at":"2026-02-10T03:13:56.875381332Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6dl","depends_on_id":"bd-89x","type":"blocks","created_at":"2026-02-10T03:35:19.593236853Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-6dl","depends_on_id":"bd-hv6","type":"blocks","created_at":"2026-02-10T03:13:56.574678369Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":2,"issue_id":"bd-6dl","author":"Dicklesworthstone","text":"Initial known doc/code drift to include in the Errata pass (starting point, not exhaustive):\n\n- Cancelled errno mapping mismatch: COMPREHENSIVE_SPEC currently says EINTR; crates/ffs-error maps to ECANCELED.\n- Parser error taxonomy needs explicit layering: ffs-ondisk returns ParseError today; user-facing layers return FfsError.\n- PLAN references ARCHITECTURE.md and CONVENTIONS.md which do not exist; decide create vs remove.\n\nAudit output should focus on: normative type/trait single-source-of-truth, crate map accuracy vs Cargo.toml, and eliminating phantom contracts/phantom crates.","created_at":"2026-02-10T03:33:45Z"}]}
{"id":"bd-6st","title":"Harness: Establish parity accounting protocol (FEATURE_PARITY.md <-> ParityReport)","description":"Goal: ensure we never lose track of what is implemented.\n\nDeliverables:\n- Define the canonical source of parity totals (FEATURE_PARITY.md) and the canonical source of parity current counts (ffs-harness ParityReport::current).\n- Add a lightweight check/test that FEATURE_PARITY.md numbers match the harness report (or the harness reads the file).\n\nAcceptance:\n- Any PR that changes implemented functionality updates parity in the same change.\n- No more hand-edited divergent numbers.","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-10T03:21:18.608385158Z","created_by":"ubuntu","updated_at":"2026-02-10T20:41:07.557633121Z","closed_at":"2026-02-10T20:41:07.557614125Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["harness"]}
{"id":"bd-89x","title":"Docs: Align toolchain/MSRV statements (Rust 2024, nightly toolchain)","description":"Goal: ensure docs do not state an impossible MSRV.\n\nContext:\n- Rust edition 2024 requires Rust >= 1.85.\n- The repo uses rust-toolchain.toml for nightly.\n\nDeliverables:\n- Ensure PLAN/README/COMPREHENSIVE_SPEC consistently state the toolchain policy.\n- If we claim an MSRV, it must be technically compatible with edition + dependencies.\n\nAcceptance:\n- No doc claims a Rust version that cannot compile the workspace.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:35:11.239312918Z","created_by":"ubuntu","updated_at":"2026-02-11T02:06:53.885173467Z","closed_at":"2026-02-11T02:06:53.885096573Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs"],"dependencies":[{"issue_id":"bd-89x","depends_on_id":"bd-2ds","type":"blocks","created_at":"2026-02-10T03:35:19.764294386Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-8tr","title":"ext4: Implement inode location math helper (inode -> table block + offset)","description":"Goal: given (inode_number, superblock fields, group desc), compute where the inode bytes live on disk.\n\nDeliverables:\n- Pure helper: inode_number -> (group, index_in_group).\n- Use group desc inode_table pointer and inode_size to compute:\n  - table_start_block\n  - inode_byte_offset_from_table_start\n  - absolute byte offset on device\n- Handle ext4 inode numbering rules (inode numbers start at 1; 0 invalid).\n\nAcceptance:\n- Unit tests for boundary inodes: 1, inodes_per_group, inodes_per_group+1, last inode.\n- Overflow/invalid inputs produce ParseError/FfsError (no panics).\n- Helper is used by higher-level inode read code (ffs-inode later).","status":"closed","priority":0,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:17:15.392537885Z","created_by":"ubuntu","updated_at":"2026-02-10T17:37:56.483745546Z","closed_at":"2026-02-10T17:37:56.483718756Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4"],"dependencies":[{"issue_id":"bd-8tr","depends_on_id":"bd-2w9","type":"blocks","created_at":"2026-02-10T03:18:06.687704093Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-98b","title":"Repair: Expected-loss policy for redundancy + repair (alien-artifact bar)","description":"Goal: choose repair symbol overhead and repair actions using an explicit decision-theoretic model (not heuristics).\n\nDeliverables:\n- Define loss model: corruption cost vs redundancy overhead cost.\n- Maintain posterior over corruption rate (Beta prior) or richer model as needed.\n- Decision rule: choose overhead that minimizes expected loss.\n- Integrate with ffs-core DurabilityAutopilot (or refactor it into ffs-repair if it belongs there).\n\nAcceptance:\n- Policy produces explainable outputs (posterior rate + chosen overhead + expected loss).\n- Harness includes a test that the policy reacts sensibly when corruption events are observed.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:24:45.985656598Z","created_by":"ubuntu","updated_at":"2026-02-11T01:56:31.419958515Z","closed_at":"2026-02-11T01:56:31.419854611Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["repair"],"dependencies":[{"issue_id":"bd-98b","depends_on_id":"bd-2qf","type":"blocks","created_at":"2026-02-10T03:25:00.757767434Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-b80","title":"Repair: Implement scrub pipeline (detect corruption signals)","description":"Goal: detect corrupted blocks/metadata reliably.\n\nDeliverables:\n- Define signals:\n  - ext4 metadata_csum / btrfs csum verification failures\n  - structural invariant violations (bounds, tree ordering)\n- Implement ScrubReport with per-block findings and severity.\n\nAcceptance:\n- Harness can inject corruption (flip bit) and scrub reports it deterministically.\n- Scrub does not panic on corrupted data; it returns structured findings.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:24:28.702412790Z","created_by":"ubuntu","updated_at":"2026-02-11T01:27:16.268015147Z","closed_at":"2026-02-11T01:27:16.267938143Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["repair"],"dependencies":[{"issue_id":"bd-b80","depends_on_id":"bd-25k","type":"blocks","created_at":"2026-02-10T03:25:00.916224379Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-b80","depends_on_id":"bd-2vt","type":"blocks","created_at":"2026-02-10T03:25:00.995317684Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-c35","title":"Track: Harness + Conformance + Parity Accounting (ffs-harness)","description":"Build the proof system: fixture generation, comparisons against Linux behavior where applicable, parity reporting, and regression gates.\\n\\nAcceptance: every implemented behavior has (1) a fixture or property test, (2) a harness test that exercises it end-to-end, (3) FEATURE_PARITY + harness parity numbers updated in the same change. Benchmarks are added only after fixtures exist.","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-10T03:08:52.148014837Z","created_by":"ubuntu","updated_at":"2026-02-11T03:28:15.385993623Z","closed_at":"2026-02-11T03:28:15.385971391Z","close_reason":"All 5 sub-tasks closed: ext4 fixtures (bd-2zg), parity accounting (bd-6st), fixture workflow (bd-3q4), kernel reference capture (bd-2ij), btrfs fixtures (bd-2yi).","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-c35","depends_on_id":"bd-2ij","type":"blocks","created_at":"2026-02-10T03:21:53.179975971Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c35","depends_on_id":"bd-2yi","type":"blocks","created_at":"2026-02-10T03:21:53.099617897Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c35","depends_on_id":"bd-2zg","type":"blocks","created_at":"2026-02-10T03:21:53.014650837Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c35","depends_on_id":"bd-3q4","type":"blocks","created_at":"2026-02-10T03:21:52.923718680Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-c35","depends_on_id":"bd-6st","type":"blocks","created_at":"2026-02-10T03:21:52.825671893Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":8,"issue_id":"bd-c35","author":"Dicklesworthstone","text":"ffs-harness is the proof engine.\n\nRules:\n- every new behavior needs fixtures or properties\n- parity numbers must match FEATURE_PARITY.md\n- only optimize after correctness vectors exist\n\nWithout this track, progress becomes vibes instead of verified parity.","created_at":"2026-02-10T03:34:39Z"}]}
{"id":"bd-ece","title":"ffs-block: Cache metrics + instrumentation surface","description":"Goal: expose cache behavior to harness/CLI/TUI so performance work is evidence-based.\n\nDeliverables:\n- CacheMetrics struct: hit/miss counts, evictions, current list sizes, p value.\n- A cheap snapshot method to read metrics without heavy locks.\n- Optional tracing hooks gated behind feature flag.\n\nAcceptance:\n- Metrics can be queried from ffs-core and printed by ffs-cli inspect.\n- Metrics are used by benchmarks to assert improvements are real (not noise).","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:15:53.854812013Z","created_by":"ubuntu","updated_at":"2026-02-10T20:49:40.556955612Z","closed_at":"2026-02-10T20:49:40.556929884Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["io"],"dependencies":[{"issue_id":"bd-ece","depends_on_id":"bd-5iz","type":"blocks","created_at":"2026-02-10T03:16:26.979686708Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-g30","title":"Track: CLI + TUI (ffs-cli + ffs-tui)","description":"Provide operator tooling: CLI subcommands (inspect/info/fsck/mount), and a frankentui-based dashboard to observe cache/MVCC/repair stats.\\n\\nAcceptance: CLI is the primary UX for harness + debugging; TUI can attach to a running mount to show internal metrics. Both must be deterministic/testable and use ffs-core as the integration surface.","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-02-10T03:09:24.040606496Z","created_by":"ubuntu","updated_at":"2026-02-11T03:44:14.103200953Z","closed_at":"2026-02-11T03:44:14.103175686Z","close_reason":"All sub-tasks complete: clap CLI, mount, fsck/scrub, TUI dashboard","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-g30","depends_on_id":"bd-23n","type":"blocks","created_at":"2026-02-10T03:29:55.362716892Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-g30","depends_on_id":"bd-23r","type":"blocks","created_at":"2026-02-10T03:29:55.620739199Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-g30","depends_on_id":"bd-2fs","type":"blocks","created_at":"2026-02-10T03:29:55.448460236Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-g30","depends_on_id":"bd-3sk","type":"blocks","created_at":"2026-02-10T03:30:12.268311230Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-g30","depends_on_id":"bd-3sp","type":"blocks","created_at":"2026-02-10T03:30:12.183821788Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-g30","depends_on_id":"bd-3vn","type":"blocks","created_at":"2026-02-10T03:29:55.275857697Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-g30","depends_on_id":"bd-m35","type":"blocks","created_at":"2026-02-10T03:29:55.535816523Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":13,"issue_id":"bd-g30","author":"Dicklesworthstone","text":"Operator UX track.\n\nCLI is the debugging and automation surface:\n- stable JSON output options\n- mount/fsck workflows\n- parity/stats commands\n\nTUI is phased and depends on metrics surfaces from cache/MVCC/repair.","created_at":"2026-02-10T03:34:39Z"}]}
{"id":"bd-hks","title":"ffs-block: Plan write-back cache + dirty tracking (phased)","description":"Future-facing task: define how cached writes become durable.\n\nDeliverables:\n- DirtyTracker design: which blocks are dirty, ordering constraints, flush scheduling.\n- Interaction with MVCC: versioned writes vs overwriting.\n- Failure/cancellation semantics (fsync, flush on close, etc).\n\nAcceptance:\n- A concrete design exists in docs/spec before any write-back code is added.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-10T03:16:07.481262948Z","created_by":"ubuntu","updated_at":"2026-02-11T03:48:46.342359407Z","closed_at":"2026-02-11T03:48:46.342335122Z","close_reason":"Design doc created at docs/design-writeback-cache.md with 3-phase plan: DirtyTracker, FlushDaemon, MVCC integration","source_repo":".","compaction_level":0,"original_size":0,"labels":["io"],"dependencies":[{"issue_id":"bd-hks","depends_on_id":"bd-220","type":"blocks","created_at":"2026-02-10T03:16:27.218279466Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-hks","depends_on_id":"bd-5iz","type":"blocks","created_at":"2026-02-10T03:16:27.135355880Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-hrv","title":"MVCC: Deterministic concurrency tests (asupersync lab runtime)","description":"Goal: make MVCC correctness testable under controlled schedules.\n\nDeliverables:\n- Add tests that run transactions under deterministic scheduling (asupersync lab runtime) and explore interleavings.\n- Encode invariants: snapshot visibility, FCW conflicts, no lost updates.\n- Add at least one regression test for a known anomaly pattern (write skew) once SSI is introduced.\n\nAcceptance:\n- Tests are deterministic and non-flaky.\n- A small set of schedules is enough to reproduce bugs reliably.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T03:22:40.713789731Z","created_by":"ubuntu","updated_at":"2026-02-11T01:33:22.088331821Z","closed_at":"2026-02-11T01:33:22.088246271Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["mvcc"],"dependencies":[{"issue_id":"bd-hrv","depends_on_id":"bd-19k","type":"blocks","created_at":"2026-02-10T03:23:53.264230369Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-hrv","depends_on_id":"bd-2t1","type":"blocks","created_at":"2026-02-10T03:23:53.183269997Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-huh","title":"EPIC: ext4 Write Path","description":"# EPIC: ext4 Write Path\n\n## PURPOSE\nEnable write operations on ext4 filesystem images through FUSE.\n\nThis is the largest V1 gap: today FrankenFS is read-only; this epic makes ext4 images mutable (with crash consistency) while preserving external ext4 behavior.\n\n## BACKGROUND\nFrom FEATURE_PARITY.md blocking gaps:\n- full compatibility-mode write-path equivalence\n\nFrom README limitations:\n- \"Read-only mount only\" (write support not yet implemented)\n\nThe ext4 write path requires orchestrating:\n1. Transaction boundaries (MVCC for concurrency control)\n2. Block allocation and freeing (ext4 mballoc semantics)\n3. Extent tree mutation (insert/split/merge + overlap handling)\n4. Inode updates and checksums\n5. Directory entry mutation (create/unlink/rename)\n6. Journal write path (JBD2-compatible logging) and replay integration\n7. FUSE + CLI surfaces (RW mount flag and write ops)\n\n## CURRENT STATE\n- ext4 read path works end-to-end for supported images\n- MVCC core exists (in-memory) for snapshot isolation + conflict detection\n- Journal replay exists; journal write side is missing\n- Generic allocation / extent structures exist but ext4 on-disk parity is not complete\n- FUSE surface is read-only\n\n## GAPS TO CLOSE\n1. Implement/ext4-verify the missing mutation semantics (allocator, extents, dir/inode)\n2. Implement journal writer + integrate commit/checkpoint semantics\n3. Expose write ops via `FsOps` and FUSE\n4. Add E2E write suite that proves persistence and basic crash behavior\n\n## ACCEPTANCE CRITERIA\n1. `echo \"test\" > /mnt/ffs/newfile.txt` works\n2. `mkdir /mnt/ffs/newdir` works\n3. `rm /mnt/ffs/file` works\n4. Changes persist after unmount/remount\n5. Crash recovery via journal replay\n6. Concurrent writers do not corrupt filesystem state (MVCC conflict detection)\n\n## DEPENDENCIES\n- EPIC: MVCC Core (bd-22w) - concurrency control and transactions\n- EPIC: ext4 Read Path (bd-1xe) - must understand read semantics first\n- EPIC: Self-Healing (bd-15c) - symbols updated on write (if enabled)\n\n## V1 SCOPE LIMITS\n- Single-device images only\n- No quotas\n- No special files (fifos, sockets, devices)\n- Start with metadata correctness; data journaling scope is explicitly defined in spec\n\n## RELATED SPEC SECTIONS\n- COMPREHENSIVE_SPEC_FOR_FRANKENFS_V1.md ¬ß5 (Write Path)\n- EXISTING_EXT4_BTRFS_STRUCTURE.md ¬ß2 (ext4 write behavior)\n\n## Success Criteria\n1. `scripts/e2e/ffs_ext4_rw_smoke.sh` passes on a supported fixture image and produces actionable logs/artifacts.\n2. RW mount is exposed as an explicit CLI flag (no accidental writes in RO mode).\n3. Basic crash tests either recover correctly or fail with clear diagnostics (no silent corruption).","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-12T15:02:15.648865297Z","created_by":"ubuntu","updated_at":"2026-02-12T20:59:12.324816971Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-huh","depends_on_id":"bd-15c","type":"blocks","created_at":"2026-02-12T20:53:17.181731561Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-huh","depends_on_id":"bd-1xe","type":"blocks","created_at":"2026-02-12T15:04:29.366651562Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-huh","depends_on_id":"bd-22w","type":"blocks","created_at":"2026-02-12T15:04:29.481380213Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-huh.1","title":"Implement B+tree insert/split operations","description":"# Implement B+tree insert/split operations\n\n## GOAL\nComplete the ffs-btree crate with insert and split operations needed for extent tree modification.\n\n## CONTEXT\nffs-btree currently has:\n- Search (find key in tree)\n- Walk (iterate all entries)\n\nMissing for writes:\n- Insert (add new key-value)\n- Split (when node overflows)\n- Merge (when node underflows after delete)\n- Delete (remove key)\n\n## IMPLEMENTATION APPROACH\n\n### ext4 extent tree structure\n```\nRoot (in inode i_block[]):\n  ExtentHeader { magic, entries, max, depth }\n  ExtentIndex[] (if depth > 0) or ExtentEntry[] (if depth == 0)\n\nInternal node:\n  ExtentHeader\n  ExtentIndex[] - points to child nodes\n\nLeaf node:\n  ExtentHeader  \n  ExtentEntry[] - maps logical block range to physical blocks\n```\n\n### Insert algorithm\n```rust\npub fn insert_extent(\n    cx: &Cx,\n    device: &mut dyn BlockDevice,\n    inode: &mut Ext4Inode,\n    new_extent: ExtentEntry,\n) -> Result<()> {\n    // 1. Find leaf node where extent belongs\n    let path = find_insert_path(cx, device, inode, new_extent.ee_block)?;\n    \n    // 2. Check if leaf has space\n    let leaf = &path.last().unwrap();\n    if leaf.header.eh_entries < leaf.header.eh_max {\n        // Simple insert: shift entries, add new one\n        insert_in_leaf(cx, device, &path, new_extent)?;\n    } else {\n        // Split required\n        split_and_insert(cx, device, &path, new_extent)?;\n    }\n    \n    Ok(())\n}\n```\n\n### Split algorithm\n```rust\nfn split_leaf(\n    cx: &Cx,\n    device: &mut dyn BlockDevice,\n    leaf_path: &[PathNode],\n    new_extent: ExtentEntry,\n) -> Result<()> {\n    // 1. Allocate new block for sibling\n    let sibling_block = allocate_block(cx, device)?;\n    \n    // 2. Move half of entries to sibling\n    let split_point = entries.len() / 2;\n    let (keep, move_to_sibling) = entries.split_at(split_point);\n    \n    // 3. Write sibling\n    write_extent_block(cx, device, sibling_block, move_to_sibling)?;\n    \n    // 4. Update parent with new index entry\n    // This may cascade splits up the tree\n    insert_index_in_parent(cx, device, &leaf_path[..len-1], sibling_first_key, sibling_block)?;\n    \n    // 5. Insert new extent in appropriate leaf\n    if new_extent.ee_block < sibling_first_key {\n        insert_in_node(keep, new_extent);\n    } else {\n        insert_in_node(move_to_sibling, new_extent);\n    }\n}\n```\n\n### Tree growth\nWhen root splits, tree depth increases:\n```rust\nfn grow_tree_depth(\n    cx: &Cx,\n    device: &mut dyn BlockDevice,\n    inode: &mut Ext4Inode,\n) -> Result<()> {\n    // 1. Allocate block for old root contents\n    // 2. Copy root entries to new block\n    // 3. Create new root with single index pointing to new block\n    // 4. Update inode i_block[]\n}\n```\n\n## FILES TO MODIFY\n- crates/ffs-btree/src/insert.rs (new)\n- crates/ffs-btree/src/split.rs (new)\n- crates/ffs-btree/src/lib.rs (export)\n\n## ACCEPTANCE CRITERIA\n1. Can insert extent into tree with space\n2. Leaf split works when node full\n3. Internal node split works (cascade)\n4. Tree depth increases when root splits\n5. All operations maintain B+tree invariants\n6. Property tests verify invariants hold\n\n## TESTING\n```rust\nproptest! {\n    #[test]\n    fn btree_insert_maintains_invariants(extents in vec(arb_extent(), 1..100)) {\n        let mut tree = EmptyTree::new();\n        for ext in extents {\n            tree.insert(ext)?;\n            prop_assert!(tree.verify_invariants());\n        }\n    }\n}\n```\n\n## INVARIANTS (must always hold)\n1. All leaves at same depth\n2. Each node has entries <= max\n3. Keys in each node are sorted\n4. Internal node keys = first key of child subtree\n5. No overlapping extents in leaves","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-12T15:02:37.183650589Z","created_by":"ubuntu","updated_at":"2026-02-12T15:02:37.183650589Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-huh.1","depends_on_id":"bd-huh","type":"parent-child","created_at":"2026-02-12T15:02:37.183650589Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-huh.2","title":"Implement mballoc block allocation","description":"# Implement mballoc block allocation\n\n## GOAL\nImplement ext4's multi-block allocator (mballoc) for efficient block allocation.\n\n## CONTEXT\nFrom EXISTING_EXT4_BTRFS_STRUCTURE.md:\n- mballoc uses buddy system for contiguous allocation\n- Preallocation for locality\n- Best-fit strategy within size class\n\nffs-alloc has bitmap reading but not mutation.\n\n## IMPLEMENTATION APPROACH\n\n### Allocator state\n```rust\npub struct MballAllocator {\n    geometry: Ext4Geometry,\n    /// Per-group allocation state\n    groups: Vec<GroupAllocState>,\n    /// Per-inode preallocation\n    inode_prealloc: BTreeMap<InodeNumber, Preallocation>,\n}\n\npub struct GroupAllocState {\n    /// Block bitmap (loaded on demand)\n    bitmap: Option<Vec<u8>>,\n    /// Free block count\n    free_count: u32,\n    /// Buddy bitmap for power-of-2 searches\n    buddy: Option<BuddyBitmap>,\n}\n\npub struct Preallocation {\n    pub start: BlockNumber,\n    pub len: u32,\n    pub used: u32,\n}\n```\n\n### Allocation algorithm\n```rust\nimpl MballAllocator {\n    pub fn allocate(\n        &mut self,\n        cx: &Cx,\n        device: &mut dyn BlockDevice,\n        goal: AllocationGoal,\n    ) -> Result<AllocatedExtent> {\n        // 1. Check inode preallocation\n        if let Some(prealloc) = self.inode_prealloc.get_mut(&goal.inode) {\n            if let Some(extent) = prealloc.try_satisfy(goal.blocks) {\n                return Ok(extent);\n            }\n        }\n        \n        // 2. Find best group (Orlov for dirs, locality for files)\n        let group = self.select_group(goal)?;\n        \n        // 3. Search within group using buddy allocator\n        let state = self.load_group_state(cx, device, group)?;\n        let extent = state.buddy.allocate_blocks(goal.blocks)?;\n        \n        // 4. Update bitmap\n        state.mark_allocated(&extent);\n        \n        // 5. Write bitmap back\n        self.write_bitmap(cx, device, group)?;\n        \n        Ok(extent)\n    }\n}\n```\n\n### Buddy bitmap\n```rust\npub struct BuddyBitmap {\n    /// Level 0: individual blocks\n    /// Level N: 2^N contiguous blocks\n    levels: Vec<BitVec>,\n}\n\nimpl BuddyBitmap {\n    pub fn find_free(&self, size_class: usize) -> Option<BlockNumber>;\n    pub fn mark_allocated(&mut self, start: BlockNumber, count: u32);\n    pub fn mark_free(&mut self, start: BlockNumber, count: u32);\n}\n```\n\n### Deallocation\n```rust\nimpl MballAllocator {\n    pub fn free(\n        &mut self,\n        cx: &Cx,\n        device: &mut dyn BlockDevice,\n        extent: AllocatedExtent,\n    ) -> Result<()> {\n        let group = self.block_to_group(extent.start);\n        let state = self.load_group_state(cx, device, group)?;\n        state.mark_free(&extent);\n        self.write_bitmap(cx, device, group)?;\n        Ok(())\n    }\n}\n```\n\n## FILES TO MODIFY\n- crates/ffs-alloc/src/mballoc.rs (new)\n- crates/ffs-alloc/src/buddy.rs (new)\n- crates/ffs-alloc/src/lib.rs (export)\n\n## ACCEPTANCE CRITERIA\n1. Can allocate single block\n2. Can allocate contiguous range\n3. Preallocation reduces fragmentation\n4. Deallocation marks blocks free\n5. Bitmap persisted correctly\n6. Free count matches bitmap population\n\n## TESTING\n```rust\n#[test]\nfn test_allocate_deallocate_roundtrip() {\n    let mut alloc = MballAllocator::new(geometry);\n    let initial_free = alloc.total_free_blocks();\n    \n    let extent = alloc.allocate(cx, device, AllocationGoal::new(100))?;\n    assert_eq!(extent.len, 100);\n    assert_eq!(alloc.total_free_blocks(), initial_free - 100);\n    \n    alloc.free(cx, device, extent)?;\n    assert_eq!(alloc.total_free_blocks(), initial_free);\n}\n```\n\n## DEPENDENCIES\n- ext4 allocator bitmap reading (bd-1xe.2)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-12T15:02:56.076860249Z","created_by":"ubuntu","updated_at":"2026-02-12T15:03:50.080334560Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-huh.2","depends_on_id":"bd-1xe.2","type":"blocks","created_at":"2026-02-12T15:03:50.080293183Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-huh.2","depends_on_id":"bd-huh","type":"parent-child","created_at":"2026-02-12T15:02:56.076860249Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-huh.3","title":"Implement journal write operations","description":"# Implement journal write operations\n\n## GOAL\nEnable writing transactions to ext4's JBD2 journal for crash consistency.\n\n## CONTEXT\nJournal replay exists in ffs-journal. Now need the write side:\n1. Begin transaction\n2. Log modified blocks to journal\n3. Commit transaction\n4. Checkpoint (write journal blocks to final locations)\n\n## IMPLEMENTATION APPROACH\n\n### Journal transaction lifecycle\n```rust\npub struct JournalTransaction {\n    pub tid: u32,  // Transaction ID\n    pub blocks: Vec<JournalBlock>,\n    pub state: TxnState,\n}\n\npub enum JournalBlock {\n    Descriptor { target: BlockNumber },\n    Data { target: BlockNumber, data: Vec<u8> },\n    Commit { sequence: u32, checksum: u32 },\n    Revoke { blocks: Vec<BlockNumber> },\n}\n\npub enum TxnState {\n    Running,\n    Committing,\n    Committed,\n    Checkpointed,\n}\n```\n\n### Journal writer\n```rust\npub struct JournalWriter {\n    device: Arc<dyn BlockDevice>,\n    superblock: JournalSuperblock,\n    head: u64,  // Next write position\n    tail: u64,  // Oldest uncheckpointed\n    current_txn: Option<JournalTransaction>,\n}\n\nimpl JournalWriter {\n    pub fn begin_transaction(&mut self) -> Result<&mut JournalTransaction> {\n        assert!(self.current_txn.is_none());\n        let tid = self.superblock.s_sequence + 1;\n        self.current_txn = Some(JournalTransaction::new(tid));\n        Ok(self.current_txn.as_mut().unwrap())\n    }\n    \n    pub fn log_block(\n        &mut self,\n        cx: &Cx,\n        target: BlockNumber,\n        data: &[u8],\n    ) -> Result<()> {\n        let txn = self.current_txn.as_mut().ok_or(FfsError::NoTransaction)?;\n        \n        // Write descriptor block if needed\n        if txn.needs_descriptor() {\n            self.write_descriptor(cx, txn)?;\n        }\n        \n        // Write data block to journal\n        let journal_block = self.allocate_journal_block()?;\n        self.device.write_block(cx, journal_block, data)?;\n        \n        txn.blocks.push(JournalBlock::Data { target, data: data.to_vec() });\n        Ok(())\n    }\n    \n    pub fn commit(&mut self, cx: &Cx) -> Result<()> {\n        let txn = self.current_txn.take().ok_or(FfsError::NoTransaction)?;\n        \n        // Write commit block\n        let commit_block = self.build_commit_block(&txn);\n        self.write_block(cx, commit_block)?;\n        \n        // Flush to ensure durability\n        self.device.sync(cx)?;\n        \n        // Update journal superblock\n        self.superblock.s_sequence = txn.tid;\n        self.write_journal_superblock(cx)?;\n        \n        Ok(())\n    }\n    \n    pub fn checkpoint(&mut self, cx: &Cx) -> Result<()> {\n        // Write logged blocks to their final locations\n        // Update tail pointer\n        // Reclaim journal space\n    }\n}\n```\n\n### Integration with MVCC\n```rust\n// In transaction commit flow:\npub fn commit_with_journal(\n    cx: &Cx,\n    mvcc: &mut MvccStore,\n    journal: &mut JournalWriter,\n    txn: Transaction,\n) -> Result<CommitSeq> {\n    // 1. Begin journal transaction\n    journal.begin_transaction()?;\n    \n    // 2. Log all modified blocks\n    for (bn, data) in txn.writes() {\n        journal.log_block(cx, bn, data)?;\n    }\n    \n    // 3. Commit journal (makes durable)\n    journal.commit(cx)?;\n    \n    // 4. Commit MVCC (updates version chains)\n    let seq = mvcc.commit(txn)?;\n    \n    // 5. Checkpoint can happen async\n    Ok(seq)\n}\n```\n\n## FILES TO MODIFY\n- crates/ffs-journal/src/writer.rs (new)\n- crates/ffs-journal/src/transaction.rs (new)\n- crates/ffs-core/src/write.rs (integration)\n\n## ACCEPTANCE CRITERIA\n1. Can begin/log/commit transaction\n2. Journal blocks written correctly\n3. Commit block has valid checksum\n4. After crash, replay recovers committed txns\n5. Checkpoint reclaims journal space\n6. No data loss on crash during commit\n\n## TESTING\n```rust\n#[test]\nfn test_journal_crash_recovery() {\n    let mut writer = JournalWriter::new(device)?;\n    writer.begin_transaction()?;\n    writer.log_block(cx, BlockNumber(100), &data)?;\n    writer.commit(cx)?;\n    \n    // Simulate crash - don't checkpoint\n    drop(writer);\n    \n    // Recover\n    let replayer = JournalReplayer::new(device)?;\n    let outcome = replayer.replay(cx)?;\n    \n    // Block 100 should have data\n    let read = device.read_block(cx, BlockNumber(100))?;\n    assert_eq!(read.as_ref(), data);\n}\n```\n\n## DEPENDENCIES\n- MVCC persistence (bd-22w.1)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-12T15:03:18.014922177Z","created_by":"ubuntu","updated_at":"2026-02-12T15:03:50.171627613Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-huh.3","depends_on_id":"bd-22w.1","type":"blocks","created_at":"2026-02-12T15:03:50.171584222Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-huh.3","depends_on_id":"bd-huh","type":"parent-child","created_at":"2026-02-12T15:03:18.014922177Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-huh.4","title":"Implement FUSE write operation handlers","description":"# Implement FUSE write operation handlers\n\n## GOAL\nAdd write operation support to ffs-fuse: write, create, mkdir, unlink, rmdir, rename, setattr, etc.\n\n## CONTEXT\nffs-fuse currently only implements read operations via FsOps trait. Need to:\n1. Extend FsOps with write methods\n2. Implement in Ext4FsOps\n3. Wire to fuser's Filesystem trait\n\n## IMPLEMENTATION APPROACH\n\n### Extend FsOps trait\n```rust\n// In ffs-core/src/ops.rs\npub trait FsOps: Send + Sync {\n    // Existing read methods...\n    \n    // NEW write methods\n    fn write(&self, cx: &Cx, ino: InodeNumber, offset: u64, data: &[u8]) -> Result<u32>;\n    fn create(&self, cx: &Cx, parent: InodeNumber, name: &OsStr, mode: u32) -> Result<InodeAttr>;\n    fn mkdir(&self, cx: &Cx, parent: InodeNumber, name: &OsStr, mode: u32) -> Result<InodeAttr>;\n    fn unlink(&self, cx: &Cx, parent: InodeNumber, name: &OsStr) -> Result<()>;\n    fn rmdir(&self, cx: &Cx, parent: InodeNumber, name: &OsStr) -> Result<()>;\n    fn rename(&self, cx: &Cx, parent: InodeNumber, name: &OsStr, newparent: InodeNumber, newname: &OsStr) -> Result<()>;\n    fn setattr(&self, cx: &Cx, ino: InodeNumber, attrs: SetAttrRequest) -> Result<InodeAttr>;\n    fn truncate(&self, cx: &Cx, ino: InodeNumber, size: u64) -> Result<()>;\n    fn symlink(&self, cx: &Cx, parent: InodeNumber, name: &OsStr, target: &Path) -> Result<InodeAttr>;\n    fn link(&self, cx: &Cx, ino: InodeNumber, newparent: InodeNumber, newname: &OsStr) -> Result<InodeAttr>;\n    fn setxattr(&self, cx: &Cx, ino: InodeNumber, name: &str, value: &[u8], flags: u32) -> Result<()>;\n    fn removexattr(&self, cx: &Cx, ino: InodeNumber, name: &str) -> Result<()>;\n}\n```\n\n### Implement in FrankenFuse\n```rust\n// In ffs-fuse/src/lib.rs\nimpl Filesystem for FrankenFuse {\n    fn write(\n        &mut self,\n        _req: &Request,\n        ino: u64,\n        _fh: u64,\n        offset: i64,\n        data: &[u8],\n        _write_flags: u32,\n        _flags: i32,\n        _lock_owner: Option<u64>,\n        reply: ReplyWrite,\n    ) {\n        let cx = Cx::background();\n        match self.ops.write(&cx, InodeNumber(ino), offset as u64, data) {\n            Ok(written) => reply.written(written),\n            Err(e) => reply.error(e.to_errno()),\n        }\n    }\n    \n    fn create(\n        &mut self,\n        _req: &Request,\n        parent: u64,\n        name: &OsStr,\n        mode: u32,\n        _umask: u32,\n        _flags: i32,\n        reply: ReplyCreate,\n    ) {\n        let cx = Cx::background();\n        match self.ops.create(&cx, InodeNumber(parent), name, mode) {\n            Ok(attr) => {\n                let fuse_attr = self.to_fuse_attr(&attr);\n                reply.created(&ATTR_TTL, &fuse_attr, 0, 0, 0);\n            }\n            Err(e) => reply.error(e.to_errno()),\n        }\n    }\n    \n    // ... similar for mkdir, unlink, rmdir, rename, setattr, etc.\n}\n```\n\n### Ext4FsOps write implementation\n```rust\nimpl FsOps for Ext4FsOps {\n    fn write(&self, cx: &Cx, ino: InodeNumber, offset: u64, data: &[u8]) -> Result<u32> {\n        // 1. Begin MVCC transaction\n        let txn = self.mvcc.begin();\n        \n        // 2. Load inode\n        let mut inode = self.read_inode(cx, ino)?;\n        \n        // 3. Determine which blocks need writing\n        let block_size = self.geometry.block_size as u64;\n        let start_block = offset / block_size;\n        let end_block = (offset + data.len() as u64 - 1) / block_size;\n        \n        // 4. For each block:\n        for logical_block in start_block..=end_block {\n            let physical = self.resolve_or_allocate_block(cx, &mut inode, logical_block)?;\n            let block_offset = (offset - logical_block * block_size) as usize;\n            let block_data = self.prepare_block_write(cx, physical, block_offset, data)?;\n            self.mvcc.write(&txn, physical, &block_data);\n        }\n        \n        // 5. Update inode size if extended\n        if offset + data.len() as u64 > inode.i_size {\n            inode.i_size = offset + data.len() as u64;\n        }\n        inode.i_mtime = current_time();\n        \n        // 6. Write inode\n        self.write_inode(cx, ino, &inode)?;\n        \n        // 7. Commit transaction\n        self.mvcc.commit(txn)?;\n        \n        Ok(data.len() as u32)\n    }\n}\n```\n\n## FILES TO MODIFY\n- crates/ffs-core/src/ops.rs (extend trait)\n- crates/ffs-core/src/ext4/ops.rs (implement write methods)\n- crates/ffs-fuse/src/lib.rs (wire to fuser)\n\n## ACCEPTANCE CRITERIA\n1. `echo \"test\" > /mnt/ffs/file` works\n2. `mkdir /mnt/ffs/dir` works\n3. `rm /mnt/ffs/file` works\n4. `mv /mnt/ffs/a /mnt/ffs/b` works\n5. File permissions work (chmod via setattr)\n6. Concurrent writes don't corrupt (MVCC)\n7. Changes persist after remount\n\n## TESTING\n```bash\n# Mount read-write\ncargo run -p ffs-cli -- mount test.ext4 /tmp/ffs --rw\n\n# Test basic operations\necho \"hello\" > /tmp/ffs/test.txt\ncat /tmp/ffs/test.txt  # Should print \"hello\"\nmkdir /tmp/ffs/subdir\nls /tmp/ffs  # Should show test.txt and subdir\n\n# Unmount and remount\nfusermount -u /tmp/ffs\ncargo run -p ffs-cli -- mount test.ext4 /tmp/ffs\ncat /tmp/ffs/test.txt  # Should still print \"hello\"\n```\n\n## DEPENDENCIES\n- B+tree insert (bd-huh.1)\n- mballoc (bd-huh.2)\n- Journal write (bd-huh.3)\n- MVCC persistence (bd-22w.1)","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-12T15:03:43.931713919Z","created_by":"ubuntu","updated_at":"2026-02-12T15:03:50.536863745Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-huh.4","depends_on_id":"bd-22w.1","type":"blocks","created_at":"2026-02-12T15:03:50.536825253Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-huh.4","depends_on_id":"bd-huh","type":"parent-child","created_at":"2026-02-12T15:03:43.931713919Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-huh.4","depends_on_id":"bd-huh.1","type":"blocks","created_at":"2026-02-12T15:03:50.263590892Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-huh.4","depends_on_id":"bd-huh.2","type":"blocks","created_at":"2026-02-12T15:03:50.356178049Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-huh.4","depends_on_id":"bd-huh.3","type":"blocks","created_at":"2026-02-12T15:03:50.446298896Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-huh.5","title":"Add E2E test suite for ext4 write path (FUSE RW + crash checks)","description":"# Add E2E test suite for ext4 write path (FUSE RW + crash checks)\n\n## GOAL\nAdd an end-to-end test script that mounts an ext4 fixture image read-write via FrankenFS and validates correct behavior for core write operations.\n\nThis is a user-facing guarantee: if we claim we can write, we must have an E2E script that proves it.\n\n## DELIVERABLES\n1. `scripts/e2e/ffs_ext4_rw_smoke.sh`\n2. Uses `scripts/e2e/lib.sh` conventions (logging/cleanup).\n3. Document prerequisites for RW mount (FUSE permissions, fixture image must be writable copy).\n\n## SCRIPT DESIGN\n- Never mutate the committed fixture image in-place.\n- Always copy the fixture to a temp working image:\n  - `cp tests/fixtures/images/ext4_medium.img <tmp>/work.img`\n- Mount `<tmp>/work.img` to `<tmp>/mnt` with a new CLI flag (to be implemented as part of write path): `ffs mount --rw`.\n- Run a bounded set of operations and verify results.\n- Unmount.\n- Re-mount the same `<tmp>/work.img` read-only and re-verify persistence.\n\n## REQUIRED OPERATIONS\n1. Create/write/overwrite:\n   - `echo hello > <mnt>/newfile.txt`\n   - `cat <mnt>/newfile.txt` == `hello`\n   - overwrite with different content, verify.\n2. mkdir/rmdir:\n   - `mkdir <mnt>/newdir`\n   - `rmdir <mnt>/newdir` (after empty)\n3. rename:\n   - `mv <mnt>/newfile.txt <mnt>/renamed.txt`\n4. unlink:\n   - `rm <mnt>/renamed.txt`\n5. metadata:\n   - `chmod 600 <mnt>/somefile` (if supported)\n   - `stat` checks: size, mode, mtime monotonicity.\n\n## CRASH / RECOVERY MINI-HARNESS (PHASED)\nWe want to validate journal replay correctness. Two phases:\n\n### Phase A (required): clean shutdown persistence\n- After unmount/remount, all writes persist.\n\n### Phase B (best-effort): simulated crash during write\n- Run `ffs mount --rw` in background.\n- Perform writes.\n- Kill the mount process abruptly.\n- Re-open the image with `ffs inspect` and/or remount and verify either:\n  - journal replay restores the committed state, or\n  - mount fails with a clear diagnostic if recovery isn‚Äôt implemented yet.\n\nThe script must produce logs sufficient to debug recovery.\n\n## LOGGING REQUIREMENTS\n- `RUST_LOG=trace RUST_BACKTRACE=1`.\n- Log file must include:\n  - before/after directory trees (bounded depth)\n  - per-operation assertions\n  - mount/unmount commands and their outputs\n\n## ACCEPTANCE CRITERIA\n1. Script does not require root except for optional fixture creation; it operates on copied images.\n2. It exits 0 on success and non-zero on failure.\n3. It never leaves a mounted FUSE filesystem behind.\n4. It prints a concise PASS/FAIL summary and points to the detailed logs.\n\n## NOTES\n- RW mount flag and correctness is implemented elsewhere in bd-huh; this bead is strictly test infrastructure.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-12T20:56:45.772907573Z","created_by":"ubuntu","updated_at":"2026-02-12T20:56:45.772907573Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-huh.5","depends_on_id":"bd-2jk.1","type":"blocks","created_at":"2026-02-12T20:56:45.772907573Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-huh.5","depends_on_id":"bd-2jk.6","type":"blocks","created_at":"2026-02-12T20:56:45.772907573Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-huh.5","depends_on_id":"bd-huh","type":"parent-child","created_at":"2026-02-12T20:56:45.772907573Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-huh.5","depends_on_id":"bd-huh.4","type":"blocks","created_at":"2026-02-12T20:56:45.772907573Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-hv6","title":"Docs: Canonicalize normative type definitions (single source + cross-links)","description":"Goal: ensure every core type has ONE normative definition and every other doc reference links to it (no divergent signatures/field types).\\n\\nScope (must be covered):\\n- ffs-types: BlockNumber, BlockSize (to add), ByteOffset (to add), InodeNumber (u64 canonical), TxnId, CommitSeq, Snapshot, GroupNumber (to add), DeviceId (to add), Generation (to add)\\n- Parsing errors: ParseError and mapping boundary to FfsError\\n- Any MVCC identifiers (TxnId/CommitSeq) must not fork into TxId/Lsn synonyms unless explicitly aliased.\\n\\nDeliverables:\\n- Choose canonical source locations (prefer ffs-types + COMPREHENSIVE_SPEC glossary).\\n- Replace all other duplicate definitions with 'see <canonical section>'.\\n\\nAcceptance:\\n- rg finds no conflicting alternative definitions of these names in docs.\\n- Any intentional alias is documented once (e.g. 'TxnId and TxId are the same; we standardize on TxnId').","status":"closed","priority":0,"issue_type":"task","created_at":"2026-02-10T03:12:39.148878165Z","created_by":"ubuntu","updated_at":"2026-02-11T02:23:47.119885446Z","closed_at":"2026-02-11T02:23:47.119866560Z","close_reason":"Canonicalized core newtype mentions across docs (removed duplicate MVCC type block in spec ¬ß1.3; updated ffs-types type lists; removed MVCC LSN synonyms in plan)","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs"],"dependencies":[{"issue_id":"bd-hv6","depends_on_id":"bd-2ds","type":"blocks","created_at":"2026-02-10T03:14:05.300116625Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-kdk","title":"btrfs: Implement single-device logical->physical mapping (sys_chunk only)","description":"Goal: given a logical bytenr, map it to (device_id, physical_bytenr) using only the sys_chunk_array bootstrap data.\n\nScope (v1):\n- single-device images only\n- only mapping via sys_chunk_array (enough to read the chunk tree/root tree)\n\nDeliverables:\n- Parse chunk items from sys_chunk_array.\n- Implement map(logical) -> physical (with bounds checking).\n\nAcceptance:\n- Fixture test: pick known logical addresses (superblock.root, superblock.chunk_root) and verify mapping lands inside the device.\n- No panics; invalid mapping returns ParseError/Format.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:18:42.802564278Z","created_by":"ubuntu","updated_at":"2026-02-10T20:06:19.809203889Z","closed_at":"2026-02-10T20:06:19.809180495Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["btrfs"],"dependencies":[{"issue_id":"bd-kdk","depends_on_id":"bd-2vt","type":"blocks","created_at":"2026-02-10T03:19:11.407714905Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-kyc","title":"Error model: add explicit incompatible-feature and block-size variants","description":"Context: PLAN_TO_PORT_FRANKENFS_TO_RUST.md ¬ß0.3 still has unchecked item for unsupported/incompatible mount validation variants.\\n\\nScope:\\n- Extend ffs-error with explicit variants for incompatible feature flags and unsupported block size.\\n- Map new variants to stable errno values and keep exhaustive tests updated.\\n- Update ffs-core parse/mount conversion to emit these variants where appropriate.\\n- Add/adjust unit tests in ffs-error and ffs-core for new mapping behavior.\\n\\nAcceptance:\\n- cargo test -p ffs-error -p ffs-core passes.\\n- New variants used by mount-validation conversion paths.\\n- No warning regressions in affected crates.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T16:22:09.369075317Z","created_by":"ubuntu","updated_at":"2026-02-11T16:28:14.612726041Z","closed_at":"2026-02-11T16:28:14.612707737Z","close_reason":"Implemented and verified (cargo check/clippy/test); landed in current main state including commit 0d8350f","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-m35","title":"CLI: Implement fsck/scrub command (read-only integrity scan)","description":"Goal: provide an operator-visible integrity scan that runs scrub and prints a report.\n\nDeliverables:\n- ffs-cli fsck/scrub <image> [--json]\n- Use ffs-repair scrub pipeline to produce ScrubReport.\n\nAcceptance:\n- On a clean fixture image: report is empty or low severity.\n- On a corrupted fixture: report includes the corrupted block and reason.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:29:40.911287475Z","created_by":"ubuntu","updated_at":"2026-02-11T01:33:39.974350937Z","closed_at":"2026-02-11T01:33:39.974230282Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli","repair"],"dependencies":[{"issue_id":"bd-m35","depends_on_id":"bd-3vn","type":"blocks","created_at":"2026-02-10T03:30:05.869432040Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-m35","depends_on_id":"bd-b80","type":"blocks","created_at":"2026-02-10T03:30:05.783412328Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-o76","title":"btrfs: Implement initial read-only tree-walk (root -> items iterator)","description":"Goal: given a btrfs superblock and a logical->physical mapper, implement a minimal tree traversal that can enumerate items in a tree (starting with root tree).\n\nDeliverables:\n- Read root node at superblock.root (using mapper + BlockDevice).\n- Descend internal nodes by key ranges until leaf.\n- Iterate leaf items and expose (BtrfsKey, payload slice) to higher layers.\n\nAcceptance:\n- Can enumerate some items from a real btrfs image (fixture) without panics.\n- Traversal is deterministic and fully bounds-checked.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-10T03:18:59.474306677Z","created_by":"ubuntu","updated_at":"2026-02-10T20:26:58.096706918Z","closed_at":"2026-02-10T20:26:58.096682512Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["btrfs"],"dependencies":[{"issue_id":"bd-o76","depends_on_id":"bd-1fo","type":"blocks","created_at":"2026-02-10T03:19:11.552636857Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-o76","depends_on_id":"bd-kdk","type":"blocks","created_at":"2026-02-10T03:19:11.482040240Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-owq","title":"ffs-error: Define canonical error taxonomy + mapping boundaries","description":"Goal: make error handling consistent across layers:\n- ffs-ondisk returns ParseError (pure parsing)\n- ffs-core/ffs-fuse/ffs-cli return FfsError (user-facing)\n\nDeliverables:\n- Document the mapping from ParseError -> FfsError (which variants, and when we preserve details).\n- Ensure ffs-error stays independent of ffs-ondisk (avoid cyclic deps).\n- Update docs to reference the canonical listing in crates/ffs-error/src/lib.rs.\n\nAcceptance:\n- No doc section lists a different FfsError variant set than the code.\n- FUSE errno mapping is complete and matches chosen policy.","status":"closed","priority":0,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:14:37.381664627Z","created_by":"ubuntu","updated_at":"2026-02-10T08:51:43.466728917Z","closed_at":"2026-02-10T08:51:43.466701065Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["foundation"]}
{"id":"bd-p1l","title":"Perf: Golden outputs + isomorphism proof protocol for optimizations","description":"Goal: enforce the extreme-software-optimization loop.\n\nDeliverables:\n- Define which commands produce golden outputs (JSON reports, fixture parse summaries).\n- Store sha256 checksums for goldens.\n- Require an isomorphism proof note for each optimization (ordering preserved, tie-breaking unchanged, etc.).\n\nAcceptance:\n- There is a repeatable command to verify goldens before/after.\n- Optimization PRs cannot \"accidentally\" change behavior without detection.","status":"closed","priority":2,"issue_type":"task","owner":"PinkCreek","created_at":"2026-02-10T03:30:48.769379006Z","created_by":"ubuntu","updated_at":"2026-02-11T02:43:55.310367049Z","closed_at":"2026-02-11T02:43:55.310340298Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["harness","perf"],"dependencies":[{"issue_id":"bd-p1l","depends_on_id":"bd-6st","type":"blocks","created_at":"2026-02-10T03:31:10.590283149Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-q0g","title":"ext4 semantics: Implement permission/uid/gid + mode mapping details [phased]","description":"Future task: ensure getattr reports correct permission bits and ownership, and FUSE enforces them where appropriate.\n\nDeliverables:\n- Map inode mode bits to POSIX file types + permissions.\n- Surface uid/gid correctly (including high bits).\n\nAcceptance:\n- getattr output matches kernel for a fixture inode set.","status":"closed","priority":3,"issue_type":"task","owner":"PinkCreek","created_at":"2026-02-10T03:27:47.212225418Z","created_by":"ubuntu","updated_at":"2026-02-11T03:22:20.184893543Z","closed_at":"2026-02-11T03:22:20.184870791Z","close_reason":"Implemented device_number/major/minor on Ext4Inode, wired rdev in inode_to_attr. uid/gid/permissions were already correct. 8 tests added.","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","semantics"],"dependencies":[{"issue_id":"bd-q0g","depends_on_id":"bd-10t","type":"blocks","created_at":"2026-02-10T03:28:01.896707087Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-q0g","depends_on_id":"bd-3qq","type":"blocks","created_at":"2026-02-10T03:28:01.815846651Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-sik","title":"ffs-types: Checked arithmetic + alignment helpers (block math, offsets)","description":"Goal: centralize all tricky integer math used by on-disk parsing and block mapping.\n\nDeliverables:\n- checked_add_bytes(offset, len) -> ByteOffset end\n- checked_mul_block(block_nr, block_size) -> ByteOffset\n- align_down/align_up helpers with overflow checking\n- conversions with explicit error paths (no silent truncation)\n\nAcceptance:\n- Unit tests cover overflow/edge cases.\n- Call sites in ffs-block/ffs-ondisk/ffs-core prefer these helpers over ad-hoc math.","status":"closed","priority":1,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:14:24.565357285Z","created_by":"ubuntu","updated_at":"2026-02-10T08:41:43.432870696Z","closed_at":"2026-02-10T08:41:43.432852562Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["foundation"]}
{"id":"bd-tnu","title":"Docs: Add AGENTS.md playbooks (cass rituals + alien artifact + optimization)","description":"Goal: make agent workflow more repeatable by adding short, high-signal playbooks to AGENTS.md.\n\nDeliverables:\n- Session-start ritual (cass-mined prompts) + command checklist.\n- Cass archaeology workflow (status/index/search/view/expand/context).\n- Alien-artifact elicitation prompt + evidence-ledger / loss-matrix reminder.\n- Extreme optimization protocol: baseline/profile/isomorphism proof + golden checksums.\n- Porting-to-rust essence extraction checklist (spec-first, conformance-first).\n\nAcceptance:\n- AGENTS.md gains a new Playbooks section with copy-paste-ready snippets.\n- No contradictory guidance vs existing rules (no file deletion, no destructive commands, use cargo, run gates).","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-10T17:26:00.865935325Z","created_by":"QuietFalcon","updated_at":"2026-02-10T17:27:13.879183709Z","closed_at":"2026-02-10T17:27:13.879154274Z","close_reason":"Add playbooks to AGENTS.md (cass workflow, alien artifact prompt, optimization loop, porting checklist)","source_repo":".","compaction_level":0,"original_size":0,"labels":["docs"]}
{"id":"bd-v1r","title":"Implement format-aware scrub validators for ext4/btrfs superblocks","description":"Scope:\\n- Replace scrub CLI placeholder-only validator stack with format-aware validators.\\n- Add ext4 superblock validator (parse + geometry + metadata checksum when enabled).\\n- Add btrfs superblock validator (parse + superblock checksum).\\n- Keep scrub engine format-agnostic with pluggable validators.\\n- Add unit tests for both validators and validator selection in CLI path where practical.\\n\\nAcceptance:\\n- cargo test for affected crates passes.\\n- No regressions in existing scrub tests.\\n- FEATURE_PARITY/ParityReport updated if capability counts change.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T16:20:52.577227565Z","created_by":"ubuntu","updated_at":"2026-02-11T16:31:12.099229919Z","closed_at":"2026-02-11T16:31:12.099209722Z","close_reason":"Implemented ext4+btrfs superblock scrub validators, wired CLI composite validator stack, updated parity counts (self-healing 3/10, overall 29/75), and validated with cargo check --all-targets, cargo clippy --all-targets -- -D warnings, cargo test --workspace, plus targeted scrub/parity tests. cargo fmt --check still reports broad pre-existing formatting drift in unrelated files.","source_repo":".","compaction_level":0,"original_size":0,"labels":["conformance","repair"]}
{"id":"bd-vgu","title":"FUSE: Add fuser dependency + skeleton Filesystem impl delegating to FsOps","description":"Goal: make ffs-fuse a thin adapter: kernel FUSE requests -> internal FsOps.\n\nDeliverables:\n- Add fuser crate dependency (phase-gated if needed).\n- Implement fuser::Filesystem with stubs that call into a boxed FsOps.\n- Ensure all errors map through FfsError::to_errno().\n\nAcceptance:\n- cargo test passes with fuser integrated.\n- The filesystem can be mounted in a no-op mode (returns ENOSYS for unimplemented ops).","status":"closed","priority":1,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:28:25.716873087Z","created_by":"ubuntu","updated_at":"2026-02-10T17:03:51.276679325Z","closed_at":"2026-02-10T17:03:51.276651664Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["fuse"],"dependencies":[{"issue_id":"bd-vgu","depends_on_id":"bd-2tq","type":"blocks","created_at":"2026-02-10T03:28:51.835670432Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-vgu","depends_on_id":"bd-3is","type":"blocks","created_at":"2026-02-10T03:28:51.923969054Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-wse","title":"ext4 semantics: Implement symlink handling (fast/slow) [phased]","description":"Future task: support symlinks.\n\nDeliverables:\n- Fast symlinks stored inline in inode i_block.\n- Slow symlinks stored as file data.\n\nAcceptance:\n- readlink works in harness/FUSE on a fixture symlink.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-10T03:26:33.602112189Z","created_by":"ubuntu","updated_at":"2026-02-10T19:52:24.738156191Z","closed_at":"2026-02-10T19:52:24.738138448Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","semantics"],"dependencies":[{"issue_id":"bd-wse","depends_on_id":"bd-10t","type":"blocks","created_at":"2026-02-10T03:28:01.570813210Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-wse","depends_on_id":"bd-2q7","type":"blocks","created_at":"2026-02-10T03:28:01.649820965Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-ye4","title":"ext4 semantics: Implement extent mapping (ffs-extent) for regular files","description":"Goal: map file logical offsets to physical blocks using the ext4 extent tree.\n\nDeliverables:\n- ffs-extent: a function that takes an inode extent root and returns an iterator of (logical_block -> physical_start,len,unwritten?).\n- Support depth>0 extent trees by reading extent blocks from disk (requires BlockDevice).\n- Provide a read_extent_at(file_block) helper for file reads.\n\nAcceptance:\n- Tests cover: leaf-only extents and index+leaf extents.\n- Verified on a fixture file with known extents (compare to debugfs stats or golden outputs).","status":"closed","priority":1,"issue_type":"task","assignee":"AzureBeaver","created_at":"2026-02-10T03:25:46.487585491Z","created_by":"ubuntu","updated_at":"2026-02-10T19:36:21.615868955Z","closed_at":"2026-02-10T19:36:21.615851281Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["ext4","semantics"],"dependencies":[{"issue_id":"bd-ye4","depends_on_id":"bd-10t","type":"blocks","created_at":"2026-02-10T03:27:33.350893907Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ye4","depends_on_id":"bd-3qq","type":"blocks","created_at":"2026-02-10T03:27:33.455657288Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-z7n","title":"Track: Foundations (ffs-types + ffs-error)","description":"Define canonical newtypes, parsing primitives, and error model that every other crate depends on.\\n\\nAcceptance: all normative types (BlockNumber, BlockSize, InodeNumber strategy, TxnId/CommitSeq/Snapshot, ByteOffset, DeviceId, etc.) exist in ffs-types with documented invariants; ffs-error has stable variant set + errno mapping used by FUSE. All other crates use these types (no local duplicates).","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-02-10T03:08:25.718123153Z","created_by":"ubuntu","updated_at":"2026-02-10T16:01:32.112602115Z","closed_at":"2026-02-10T16:01:32.112583771Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-z7n","depends_on_id":"bd-14w","type":"blocks","created_at":"2026-02-10T03:15:03.228334190Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7n","depends_on_id":"bd-1ds","type":"blocks","created_at":"2026-02-10T03:15:03.370722205Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7n","depends_on_id":"bd-2oa","type":"blocks","created_at":"2026-02-10T03:15:03.603178334Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7n","depends_on_id":"bd-3hr","type":"blocks","created_at":"2026-02-10T03:15:03.526401085Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7n","depends_on_id":"bd-owq","type":"blocks","created_at":"2026-02-10T03:15:03.446053435Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-z7n","depends_on_id":"bd-sik","type":"blocks","created_at":"2026-02-10T03:15:03.299955368Z","created_by":"ubuntu","metadata":"{}","thread_id":""}],"comments":[{"id":3,"issue_id":"bd-z7n","author":"Dicklesworthstone","text":"Why this track is first-class: filesystem bugs love unit confusion (bytes vs blocks) and inconsistent error semantics (silent truncation, wrong errno).\n\nNorth star:\n- ffs-types: strong newtypes + checked arithmetic so invariants are enforced at compile time.\n- ffs-error: a stable user-facing error set + errno mapping for FUSE.\n\nIf this track is sloppy, every other track becomes untestable and unsafe.","created_at":"2026-02-10T03:34:38Z"}]}
{"id":"bd-zge","title":"Epic: Write Path (alloc + journal + MVCC integration) [future]","description":"Future epic: implement writes in a way that preserves ext4 observable contract but uses FrankenFS internals (MVCC/COW).\n\nScope:\n- allocation (ffs-alloc)\n- journal / replay (ffs-journal)\n- inode updates, directory updates\n- fsync semantics\n\nAcceptance:\n- Controlled write workloads run without corruption and with conformance tests.","status":"closed","priority":3,"issue_type":"epic","created_at":"2026-02-10T03:27:06.048285176Z","created_by":"ubuntu","updated_at":"2026-02-11T06:35:30.950661930Z","closed_at":"2026-02-11T06:35:30.950640140Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["semantics"]}
{"id":"bd-zge.1","title":"ffs-btree: Implement generic in-memory B+tree (insert, delete, search, split, merge)","description":"Generic B+tree with configurable fanout. Operations: insert, delete, search, range-scan, split, merge. Used by extent tree and potentially directory htree. Pure data structure, no I/O. Acceptance: insert/delete/search work correctly with property tests for tree invariants (sorted keys, balanced depth, correct splits).","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T03:52:02.063269844Z","created_by":"ubuntu","updated_at":"2026-02-11T05:46:04.707803885Z","closed_at":"2026-02-11T05:46:04.707782275Z","close_reason":"Implemented ext4 extent B+tree with search, insert, delete, walk. 14 tests passing.","source_repo":".","compaction_level":0,"original_size":0,"labels":["semantics"],"dependencies":[{"issue_id":"bd-zge.1","depends_on_id":"bd-zge","type":"parent-child","created_at":"2026-02-11T03:52:02.063269844Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-zge.2","title":"ffs-alloc: Implement block/inode allocation (bitmap management, buddy allocator, mballoc)","description":"Block and inode allocation subsystem. Phase 1: bitmap read/write (block group bitmaps). Phase 2: buddy allocator (order-0 to order-13). Phase 3: mballoc goal-directed allocation. Phase 4: Orlov inode allocator. Depends on ffs-block (BlockDevice) and ffs-ondisk (Ext4GroupDesc, Ext4Superblock). Acceptance: can allocate/free blocks and inodes correctly, bitmap state stays consistent.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T03:52:07.766831467Z","created_by":"ubuntu","updated_at":"2026-02-11T05:52:48.477720896Z","closed_at":"2026-02-11T05:52:48.477702702Z","close_reason":"Implemented: bitmap ops, group stats, block alloc (goal-directed), inode alloc (Orlov), 18 tests passing","source_repo":".","compaction_level":0,"original_size":0,"labels":["semantics"],"dependencies":[{"issue_id":"bd-zge.2","depends_on_id":"bd-zge","type":"parent-child","created_at":"2026-02-11T03:52:07.766831467Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-zge.3","title":"ffs-journal: Implement JBD2 replay + native COW journal","description":"Journal subsystem. Phase 1: JBD2 journal replay at mount (descriptor/commit/revoke block parsing, forward scan, replay). Phase 2: Native COW journal using MVCC version store (commit_seq-based, flush watermark). Depends on ffs-block, ffs-mvcc, ffs-ondisk. Acceptance: JBD2 replay recovers committed transactions from journal area; native COW journal survives simulated crash+recovery.","notes":"Implemented JBD2 replay engine in crates/ffs-journal (descriptor/commit/revoke parsing, staged transaction replay on commit, revoke filtering) and native append-only COW journal (write+commit records, crash-safe recovery of committed sequences, replay onto block device). Added 6 unit tests covering committed replay, revoke behavior, uncommitted drop, COW recovery, COW replay, and tail discovery. Verified with: cargo test -p ffs-journal; cargo check --all-targets; cargo clippy --all-targets -- -D warnings; cargo test --workspace. workspace cargo fmt --check currently reports unrelated pre-existing formatting diffs outside this bead; cargo fmt -p ffs-journal --check passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T03:52:13.243461752Z","created_by":"ubuntu","updated_at":"2026-02-11T06:20:00.235462139Z","closed_at":"2026-02-11T06:20:00.235380516Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["semantics"],"dependencies":[{"issue_id":"bd-zge.3","depends_on_id":"bd-zge","type":"parent-child","created_at":"2026-02-11T03:52:13.243461752Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-zge.4","title":"ffs-extent: Implement extent tree write operations (allocate, truncate, punch hole)","description":"Extent tree mutation operations. allocate_extent() via ffs-alloc, truncate_extents() to free blocks, punch_hole() for fallocate(PUNCH_HOLE), mark_written() for unwritten extents. Depends on ffs-btree (tree structure) and ffs-alloc (block allocation). Acceptance: can grow/shrink/punch files via extent tree, tree invariants preserved.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T03:52:21.829148472Z","created_by":"ubuntu","updated_at":"2026-02-11T06:03:32.120662425Z","closed_at":"2026-02-11T06:03:32.120645744Z","close_reason":"Implemented: map, allocate, truncate, punch_hole, mark_written, GroupBlockAllocator adapter, 12 tests passing","source_repo":".","compaction_level":0,"original_size":0,"labels":["semantics"],"dependencies":[{"issue_id":"bd-zge.4","depends_on_id":"bd-zge","type":"parent-child","created_at":"2026-02-11T03:52:21.829148472Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zge.4","depends_on_id":"bd-zge.1","type":"blocks","created_at":"2026-02-11T03:52:21.829148472Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zge.4","depends_on_id":"bd-zge.2","type":"blocks","created_at":"2026-02-11T03:52:21.829148472Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-zge.5","title":"ffs-inode: Implement inode lifecycle (create, delete, update, serialize, checksum)","description":"Inode write operations. create_inode() with Orlov allocation, delete_inode() freeing blocks+inode, write_inode() serialization with CRC32C checksum, timestamp management (atime/mtime/ctime/crtime with nanoseconds). Depends on ffs-alloc (inode allocation) and ffs-extent (block management). Acceptance: create/delete/update inodes round-trips correctly with checksum verification.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T03:52:27.665573751Z","created_by":"ubuntu","updated_at":"2026-02-11T06:13:08.148014951Z","closed_at":"2026-02-11T06:13:08.147996486Z","close_reason":"verified complete: inode lifecycle APIs + checksum write/read + unit tests passing","source_repo":".","compaction_level":0,"original_size":0,"labels":["semantics"],"dependencies":[{"issue_id":"bd-zge.5","depends_on_id":"bd-zge","type":"parent-child","created_at":"2026-02-11T03:52:27.665573751Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zge.5","depends_on_id":"bd-zge.2","type":"blocks","created_at":"2026-02-11T03:52:27.665573751Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zge.5","depends_on_id":"bd-zge.4","type":"blocks","created_at":"2026-02-11T03:52:27.665573751Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-zge.6","title":"ffs-dir: Implement directory write operations (add/remove entry, htree insert/delete)","description":"Directory mutation operations. add_entry() with rec_len management, remove_entry() coalescing free space, htree insert/delete with node split/merge, mkdir initialization (. and .. entries). Depends on ffs-inode (inode creation for new dirs). Acceptance: can create/remove directory entries, htree operations preserve hash ordering.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T03:52:32.729027526Z","created_by":"ubuntu","updated_at":"2026-02-11T06:18:26.230109534Z","closed_at":"2026-02-11T06:18:26.230087674Z","close_reason":"implemented dir add/remove/init + htree insert/remove/find; validated with cargo test -p ffs-dir and cargo clippy -p ffs-dir --all-targets -- -D warnings","source_repo":".","compaction_level":0,"original_size":0,"labels":["semantics"],"dependencies":[{"issue_id":"bd-zge.6","depends_on_id":"bd-zge","type":"parent-child","created_at":"2026-02-11T03:52:32.729027526Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zge.6","depends_on_id":"bd-zge.5","type":"blocks","created_at":"2026-02-11T03:52:32.729027526Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-zge.7","title":"ffs-xattr: Implement extended attribute write operations (set/remove inline + external block)","description":"Extended attribute mutation. set_xattr() for inline (ibody) and external block xattrs, remove_xattr(), namespace permission checks, size limit enforcement. Depends on ffs-inode (inode update for ibody xattrs). Acceptance: can set/remove xattrs in both inline and external block modes, size limits enforced.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-11T03:52:37.886140968Z","created_by":"ubuntu","updated_at":"2026-02-11T06:25:23.553784754Z","closed_at":"2026-02-11T06:25:23.553765748Z","close_reason":"implemented ffs-xattr set/remove/list/get with inline+external storage routing, namespace permission checks, and size limits; validated via cargo test -p ffs-xattr and cargo clippy -p ffs-xattr --all-targets -- -D warnings","source_repo":".","compaction_level":0,"original_size":0,"labels":["semantics"],"dependencies":[{"issue_id":"bd-zge.7","depends_on_id":"bd-zge","type":"parent-child","created_at":"2026-02-11T03:52:37.886140968Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zge.7","depends_on_id":"bd-zge.5","type":"blocks","created_at":"2026-02-11T03:52:37.886140968Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-zge.8","title":"ffs-core: Integrate ffs-journal replay into ext4 mount path + crash-recovery conformance","description":"Goal: make journal replay observable to users by wiring the new ffs-journal engine into the ext4 open/mount pipeline and proving behavior with deterministic crash-recovery tests.\n\nScope:\n- Integrate replay_jbd2 invocation into ffs-core ext4 open flow behind compatibility-mode guard.\n- Define journal-region discovery from ext4 superblock metadata (journal inode/device pointers) with explicit unsupported-mode errors.\n- Surface replay metrics (ReplayStats) through engine diagnostics/logging path for auditable mount decisions.\n- Add unit tests for integration glue (invocation conditions, error mapping, no-op behavior when journal absent).\n- Add harness-level crash/recovery test flow that simulates committed and uncommitted journal transactions and verifies post-mount state.\n- Add detailed structured logging lines for replay decisions and outcomes (scanned, committed, revoked, skipped).\n\nAcceptance:\n- Mount/open of ext4 fixture with journal transactions replays committed updates and ignores uncommitted/revoked updates.\n- cargo test -p ffs-core and cargo test -p ffs-harness -- --nocapture include replay-path coverage with deterministic assertions.\n- Logs contain machine-parseable replay summary fields suitable for CI diagnostics.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-11T06:21:35.744999433Z","created_by":"ubuntu","updated_at":"2026-02-11T06:35:12.471070906Z","closed_at":"2026-02-11T06:35:12.471036722Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"labels":["conformance","semantics"],"dependencies":[{"issue_id":"bd-zge.8","depends_on_id":"bd-zge","type":"parent-child","created_at":"2026-02-11T06:21:35.744999433Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zge.8","depends_on_id":"bd-zge.3","type":"blocks","created_at":"2026-02-11T06:21:35.744999433Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
